{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "327ee811-5244-4de1-96bd-61a55d2f59c1",
   "metadata": {},
   "source": [
    "# ECE 364 Lecture 18\n",
    "## PyTorch ``nn.Module``, Optimizers, Datasets, Dataloaders (Part 2)\n",
    "\n",
    "### Learning objectives\n",
    "After today's lecture, students should be able to\n",
    "* Describe the differences between the training set, validation set, and testing set for a machine learning problem.\n",
    "* Create a PyTorch ``Dataset`` class from available data, e.g. from a toy dataset. Specifically, be able to state the necessary methods and attributes for a PyTorch ``Dataset`` class.\n",
    "* Apply the PyTorch ``Dataloader`` class to improve the flexibility and readability of a standard PyTorch training loop.\n",
    "* Extend the implementation of logistic regression from the previous lecture to multi-class logistic regression using the ``nn.Linear`` parameter class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e4c5f-9b50-41b6-b339-30755fec31e8",
   "metadata": {},
   "source": [
    "## Experimental Setup for Machine Learning Problems\n",
    "The purpose of any machine learning model is to apply the trained model on new, unseen data. In many cases, a machine learning model may be able to perform close to perfect, i.e. nearly 100% classification accuracy, on the data it is trained on. However, we need some way to evaluate the ability for the model to **generalize** to new data. The most common approach to training and testing a model is to partition a dataset into a **training set**, **validation set**, and a **test set**.\n",
    "\n",
    "For a dataset $\\mathcal{D}=\\{(x_i, y_i)\\}_{i=1}^{N}$, each of these sets are defined as:\n",
    "* **Training set**: $\\mathcal{D}_{\\textrm{train}}=\\{(x_i, y_i)\\}_{i=1}^{N_\\textrm{train}}$ is the collection of data which we train the data on.\n",
    "* **Validation set**: $\\mathcal{D}_{\\textrm{train}}=\\{(x_i, y_i)\\}_{i=1}^{N_\\textrm{val}}$ is the collection of data which the model **does not train on**, but we use to evaluate how the model generalizes to new data. We then use the validation set to tune any hyperparameters of the model or learning algorithm, e.g. learning rate, how long we train, choice of weight decay, etc.\n",
    "* **Test set**: $\\mathcal{D}_{\\textrm{train}}=\\{(x_i, y_i)\\}_{i=1}^{N_\\textrm{test}}$ is the collection of data which the model **does not train on** and which we **do not use to modify hyperparameters**. Thus, the test set is the final evaluation of model generalization and should be the primary method for comparing model performance.\n",
    "\n",
    "Note that each of these sets are disjoint and thus share no data points. The size of each partition is a choice that may depend on the application, but in general we usually reserve at least half of the data for training and roughly equal amounts of the remainder for validation and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879f47b-a16e-4996-aa79-bf5e1c6e0293",
   "metadata": {},
   "source": [
    "## PyTorch Datasets\n",
    "\n",
    "PyTorch offers an abstract base class for creating datasets that simplifies the process of building, manipulating, and sampling datasets, e.g. into training, validation, and testing sets. The ``torch.utils.data.Dataset`` class requires any new class that inherits this base class to implement three methods:\n",
    "* ``__init__``: The ``__init__`` method is the constructor for the new dataset. Unlike the ``nn.Module`` class, the base class constructor does not need to be called, i.e. we do not need to call ``super().__init__()``. The constructor is most commonly used to establish the data for the dataset or the necessary information to assign attributes that will assist the data retrieval process in the ``__getitem__`` method.\n",
    "\n",
    "* ``__len__``: The ``__len__`` method overrides the ``len()`` function in Python to determine the length of the dataset. In other words, for a dataset named ``my_dataset``. The implemented ``__len__`` function will allow ``len(my_dataset)`` to return the length of the dataset.\n",
    "\n",
    "* ``__getitem__``: The ``__getitem__`` method overloads the use of brackets to index items in a dataset. For example, a dataset named ``my_dataset`` will call the ``__getitem__`` method when we use ``my_dataset[i]`` and the index ``i`` is an input to the ``__getitem__`` method.\n",
    "\n",
    "Let's take a look at an example dataset by implementing the toy dataset from the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369b3b7-5eb2-4973-9dce-74d84bca3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TwoClassDataset(Dataset):\n",
    "    # don't forget the self identifier!\n",
    "    def __init__(self, N, sigma):\n",
    "        self.N = N # number of data points per class\n",
    "        self.sigma = sigma # standard deviation of each class cluster\n",
    "        self.plus_class = self.sigma*torch.randn(N, 2) + torch.tensor([-1, 1])\n",
    "        self.negative_class = self.sigma*torch.randn(N, 2) + torch.tensor([1, -1])\n",
    "        self.data = torch.cat((self.plus_class, self.negative_class), dim=0)\n",
    "        self.labels = torch.cat((torch.ones(self.N), torch.zeros(self.N)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y # return input and output pair\n",
    "\n",
    "N = 50\n",
    "sigma = 0.75\n",
    "dataset = TwoClassDataset(N, sigma)\n",
    "\n",
    "plus_data = dataset.plus_class\n",
    "negative_data = dataset.negative_class\n",
    "print('Dataset has {} points'.format(len(dataset)))\n",
    "idx = 2\n",
    "x, y = dataset[idx]\n",
    "print('Dataset point with index {} is at x={} and label y={}'.format(idx, x, y))\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(plus_data[:, 0].numpy(), plus_data[:, 1].numpy(), color='tomato', s=50, edgecolor='black')\n",
    "plt.scatter(negative_data[:, 0].numpy(), negative_data[:, 1].numpy(), color='cornflowerblue', s=50, edgecolor='black')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40122bba-0d16-4530-9f4a-cde97932bd70",
   "metadata": {},
   "source": [
    "Aside from making custom datasets, PyTorch and torchvision have [many pre-loaded datasets](https://pytorch.org/vision/stable/datasets.html) implemented within the same ``Dataset`` interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb25245-7c3e-4f6c-9f91-8af7547a9b71",
   "metadata": {},
   "source": [
    "## PyTorch Dataloaders\n",
    "\n",
    "With a PyTorch ``Dataset`` class in hand, we may take advantage of the ``torch.utils.data.DataLoader`` interface that will simplify the process of sampling batches of data; shuffling the dataset; partitioning into training set, validation set, testing set; and more! A ``DataLoader`` does not need to be implemented like a ``Dataset`` or ``nn.Module`` class. Instead, we only need to provide a ``Dataset`` object as input alongside several optional inputs:\n",
    "* ``batch_size``: number of examples in each batch or call to the dataloader\n",
    "\n",
    "* ``shuffle``: Boolean option to shuffle dataset each pass or **epoch** through the dataset\n",
    "\n",
    "* ``sampler``: ``Sampler`` object that specifies how data will be extracted from the dataset. For example, the ``SubsetRandomSampler`` allows us to specify indices within the larger dataset to sample at random. This is an easy way to create training, validation, and testing sets!\n",
    "\n",
    "* Plenty other options that [may be explored here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806549fd-1c05-4bfb-98a1-9ce74006b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# create indices for each split of dataset\n",
    "N_train = 60\n",
    "N_val = 20\n",
    "N_test = 20\n",
    "indices = np.arange(len(dataset))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:N_train]\n",
    "val_indices = indices[N_train:N_train+N_val]\n",
    "test_indices = indices[N_train+N_val:]\n",
    "\n",
    "# create dataloader for each split\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices))\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(val_indices))\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "# data loaders are iterable\n",
    "for x_batch, y_batch in val_loader:\n",
    "    print(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad33fd-72e5-4928-ab15-6d12e68eed9a",
   "metadata": {},
   "source": [
    "## The New and Improved Training Loop\n",
    "\n",
    "Now, let's combine these datasets and dataloaders to further simplify the training loop we used to perform the toy logistic regression problem in the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379969ba-33b3-420d-a840-2b4bd46a2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from previous lecture\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.ones(N))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1/(1+torch.exp(-(self.w@x+self.b)))\n",
    "\n",
    "# compute classification accuracy\n",
    "def model_accuracy(model, input_data, labels):\n",
    "    predictions = model(input_data.unsqueeze(-1)).squeeze(-1)\n",
    "    positive_preds = predictions >= 0.5\n",
    "    negative_preds = predictions < 0.5\n",
    "    n_correct = torch.sum(positive_preds*labels)+torch.sum(negative_preds*(1-labels))\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d077c5-9a95-4f59-a2d1-151312c2cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "# create indices for each split of dataset\n",
    "N_train = 60\n",
    "N_val = 20\n",
    "N_test = 20\n",
    "indices = np.arange(len(dataset))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[:N_train]\n",
    "val_indices = indices[N_train:N_train+N_val]\n",
    "test_indices = indices[N_train+N_val:]\n",
    "\n",
    "# create dataloader for each split\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_indices))\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(val_indices))\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "# training setup\n",
    "criterion = nn.BCELoss(reduction='mean') # binary cross-entropy loss, use mean loss\n",
    "lr = 1e-2 # learning rate\n",
    "logreg_model = LogisticRegression(2) # initialize model\n",
    "optimizer = torch.optim.SGD(logreg_model.parameters(), lr=lr, momentum=0.99, weight_decay=1e-3) # initialize optimizer\n",
    "\n",
    "n_epoch = 50 # number of passes through the training dataset\n",
    "loss_values, train_accuracies, val_accuracies = [], [], []\n",
    "for n in range(n_epoch):\n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "        # pass batch to model\n",
    "        predictions = logreg_model(x_batch.unsqueeze(-1)).squeeze(-1) # make dimensions match for loss function\n",
    "        # calculate loss\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        # backpropagate and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # logging\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += model_accuracy(logreg_model, x_batch, y_batch)\n",
    "    loss_values.append(epoch_loss/len(train_loader))\n",
    "    train_accuracies.append(epoch_acc/N_train)\n",
    "    # validation performance\n",
    "    val_acc = 0\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        # don't compute gradients since we are only evaluating the model\n",
    "        with torch.no_grad():\n",
    "            val_acc += model_accuracy(logreg_model, x_batch, y_batch)\n",
    "    val_accuracies.append(val_acc/N_val)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(131)\n",
    "plt.semilogy(loss_values)\n",
    "plt.grid(True)\n",
    "plt.title('Loss values')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(132)\n",
    "plt.plot(train_accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(133)\n",
    "plt.plot(val_accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930bf80-8b4a-4d35-a56e-061d219d1588",
   "metadata": {},
   "source": [
    "## Multi-class Logistic Regression\n",
    "\n",
    "Recall from our lecture on multi-class logistic regression that we may perform multi-class classification using logistic regression where each class has its own weight vector and bias term. More formally, for class $k$, we have weight vector $w_k\\in\\mathbb{R}^n$ and bias $b_k\\in\\mathbb{R}$. Thus, an input $x\\in\\mathbb{R}^n$ receives a \"score\" $z_k$ for class $k$ via\n",
    "$$\n",
    "z_k = w_k^\\top x + b_k.\n",
    "$$\n",
    "\n",
    "Larger scores should correspond to larger probabilities for a particular class while smaller (possibly negative) scores give smaller probabilities. For a collection of scores $z=\\{z_1, z_2, \\ldots, z_M\\}$ across $M$ classes, we can use the softmax function to normalize these scores to a probability distribution.\n",
    "\n",
    "$$\n",
    "\\textrm{softmax}(z)_k=\\mathbf{Pr}\\{\\textrm{Class }y=k|x\\} = \\frac{e^{z_k}}{\\sum_{j=1}^{M}e^{z_j}}.\n",
    "$$\n",
    "\n",
    "Instead of computing each score one-by-one, we can put all of our parameters into a weight matrix $A$ with a bias vector $b$. Thus,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= Ax+b\\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\rule[.6ex]{4ex}{0.75pt} & w_1^\\top & \\rule[.6ex]{4ex}{0.75pt}\\\\\n",
    "\\rule[.6ex]{4ex}{0.75pt} & w_2^\\top & \\rule[.6ex]{4ex}{0.75pt}\\\\\n",
    "& \\vdots & \\\\\n",
    "\\rule[.6ex]{4ex}{0.75pt} & w_M^\\top & \\rule[.6ex]{4ex}{0.75pt}\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\rule[-1ex]{0.5pt}{4ex}\\\\\n",
    "x\\\\\n",
    "\\rule[1ex]{0.5pt}{4ex}\\\\\n",
    "\\end{bmatrix}\n",
    "+\\begin{bmatrix}\n",
    "b_1\\\\\n",
    "b_2\\\\\n",
    "\\vdots\\\\\n",
    "b_M\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "z_1\\\\\n",
    "z_2\\\\\n",
    "\\vdots\\\\\n",
    "z_M\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In PyTorch, we can efficiently implement the multi-class logistic regression model using the [``nn.Linear`` class](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) which implements parameter matrices including bias terms. For the below implementation, we will also not apply the softmax function ourselves since the ``nn.CrossEntropyLoss`` class expects **logits** or scores instead of the final probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8c67c-7d98-4eed-8e86-9e0b5997c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassLogisticRegression(nn.Module):\n",
    "    def __init__(self, N, M):\n",
    "        super().__init__()\n",
    "        self.N = N # input dimension\n",
    "        self.M = M # number of classes\n",
    "        self.weight_matrix = nn.Linear(N, M, bias=True) # N input dimensions, M output dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight_matrix(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c4915-0d86-4414-8caa-7253e568ed26",
   "metadata": {},
   "source": [
    "And that's it! Again, we could compute the softmax of these logits/scores but the PyTorch implementation of cross-entropy loss asks for logits instead of probabilities. Finally, recall for model $f_\\theta(x)=z$ that cross-entropy loss is given by\n",
    "$$\n",
    "\\ell_{ce}(f_\\theta(x), y) = -\\log\\left(\\textrm{softmax}(f_\\theta(x))_y\\right)= -\\log\\left(\\frac{e^{z_y}}{\\sum_{j=1}^{M}e^{z_j}}\\right)\n",
    "$$\n",
    "for input $x$ with label $y$ (assume class number $y$ also identifies the appropriate index in $z$, for simplicity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e298e-0a48-4232-bef3-34b005a163f8",
   "metadata": {},
   "source": [
    "## Lecture Exercise: Toy Multi-class Logistic Regression\n",
    "\n",
    "Below, we provide a toy dataset for generating a toy 4-class dataset with $N=50$ samples per class and $\\sigma=0.6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa22a3a-cf93-4147-8216-f912ab9eb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "\n",
    "class FourClassDataset(Dataset):\n",
    "    def __init__(self, N, sigma):\n",
    "        self.N = N # number of data points per class\n",
    "        self.sigma = sigma # standard deviation of each class cluster\n",
    "        self.class_zero = self.sigma*torch.randn(N, 2) + torch.tensor([1, 1])\n",
    "        self.class_one = self.sigma*torch.randn(N, 2) + torch.tensor([-1, 1])\n",
    "        self.class_two = self.sigma*torch.randn(N, 2) + torch.tensor([-1, -1])\n",
    "        self.class_three = self.sigma*torch.randn(N, 2) + torch.tensor([1, -1])\n",
    "        self.data = torch.cat((self.class_zero, self.class_one, self.class_two, self.class_three), dim=0)\n",
    "        self.labels = torch.cat((torch.zeros(self.N), torch.ones(self.N),\n",
    "                                 2*torch.ones(self.N), 3*torch.ones(self.N))).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y # return input and output pair\n",
    "\n",
    "# visualize dataset\n",
    "N = 50\n",
    "sigma = 0.6\n",
    "dataset = FourClassDataset(N, sigma)\n",
    "class_zero = dataset.class_zero\n",
    "class_one = dataset.class_one\n",
    "class_two = dataset.class_two\n",
    "class_three = dataset.class_three\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(class_zero[:, 0].numpy(), class_zero[:, 1].numpy(), color='tomato', s=50, edgecolor='black', label='Class 0')\n",
    "plt.scatter(class_one[:, 0].numpy(), class_one[:, 1].numpy(), color='cornflowerblue', s=50, edgecolor='black', label='Class 1')\n",
    "plt.scatter(class_two[:, 0].numpy(), class_two[:, 1].numpy(), color='seagreen', s=50, edgecolor='black', label='Class 2')\n",
    "plt.scatter(class_three[:, 0].numpy(), class_three[:, 1].numpy(), color='violet', s=50, edgecolor='black', label='Class 3')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd2b7d-7b22-4edb-9cfe-03ced15e7ab1",
   "metadata": {},
   "source": [
    "a) Create training, validation, and testing dataloaders with a 60%:20%:20% training:validation:testing split and batch size 16.\n",
    "\n",
    "b) Fill in the training loop for training the ``MulticlassLogisticRegression`` model using the ``nn.CrossEntropyLoss`` function. We have provided the helper function for tracking model accuracy and comments below to help. **Note:** we do not need to worry about the squeezing and unsqueezing of the last dimension when passing data to the model now that we are using the ``nn.Linear`` class for our parameters.\n",
    "\n",
    "c) Experiment with training parameters, e.g. learning rate, number of epochs, batch size, momentum, weight decay, and plot the training loss, training accuracy, and validation accuracy.\n",
    "\n",
    "d) Observe the performance of this multi-class logistic regression model with noisier clusters, i.e. bigger values of $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f19d1c-3a79-416c-be95-443a5a6c5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_model_accuracy(model, input_data, labels):\n",
    "    predictions = model(input_data) # no need to squeeze/unsqueeze dimensions now!\n",
    "    predicted_classes = torch.argmax(predictions, dim=1) # find highest scoring class along the columns\n",
    "    n_correct = torch.sum(torch.eq(predicted_classes, labels))\n",
    "    return n_correct\n",
    "    \n",
    "# Part a) create DataLoaders\n",
    "\n",
    "# Part b) training loop\n",
    "# initialize MulticlassLogisticRegression model\n",
    "N = 2\n",
    "M = 4\n",
    "\n",
    "# initialize loss function and optimizer\n",
    "\n",
    "\n",
    "# logging info\n",
    "loss_values, train_accuracies, val_accuracies = [], [], []\n",
    "n_epoch = None # set this value\n",
    "for n in range(n_epoch):\n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # zero out gradients\n",
    "\n",
    "        # pass batch to model, no need to worry about using squeeze/unsqueeze now\n",
    "        \n",
    "        # calculate loss\n",
    "        \n",
    "        # backpropagate and update\n",
    "\n",
    "        # logging to update epoch_loss (add loss value) and epoch_acc (add current batch accuracy)\n",
    "        \n",
    "\n",
    "    loss_values.append(epoch_loss/len(train_loader))\n",
    "    train_accuracies.append(epoch_acc/N_train)\n",
    "    # validation performance\n",
    "    val_acc = 0\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        # don't compute gradients since we are only evaluating the model\n",
    "        with torch.no_grad():\n",
    "            # validation batch accuracy\n",
    "            \n",
    "    val_accuracies.append(val_acc/N_val)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(131)\n",
    "plt.semilogy(loss_values)\n",
    "plt.grid(True)\n",
    "plt.title('Loss values')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(132)\n",
    "plt.plot(train_accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(133)\n",
    "plt.plot(val_accuracies)\n",
    "plt.grid(True)\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
