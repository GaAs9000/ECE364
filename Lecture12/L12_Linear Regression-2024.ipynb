{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847696bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ECE 364 Lecture 12 Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597c77e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Linear Regression (LR)?  \n",
    "* Linear regression (LR) models the linear relationship between the independent (X) variable with that of the dependent variable (y).\n",
    "For example, how the likelihood of blood pressure is influenced by a person‚Äôs age and weight. This relationship can be explained using linear regression.\n",
    "* In LR, the y variable should be continuous, whereas the X variable can be continuous or categorical. If both X and y are continuous, the linear relationship can be estimated using correlation coefficient (r) or the coefficient of determination (R-Squared)\n",
    "* LR is useful if the relationships between the X and y variables are linear\n",
    "* LR is helpful to predict the value of y based on the value of the X variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa07317f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iinear Regression<br>\n",
    "* Given a dataset ${\\cal D} = \\{(x^{(i)},y^{(i)})\\}_{i=1}^N$\n",
    "* Data $x^{(i)} \\in \\mathbb{R}^d$\n",
    "* **Question:** Given new unseen data $ùë•$ how to predict its label $ùë¶$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebffd03",
   "metadata": {},
   "source": [
    "* For example: likelihood of blood pressure is influenced by a person‚Äôs age.<br>\n",
    "    This relationship can be explained using linear regression.\n",
    "* We have a data set of blood pressure vs ages from 20 patients. They can be ploted as following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04620623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# x from 20 to 65\n",
    "x = 20 + 45 * np.random.random((20, 1))\n",
    "# y = a*x + b with noise\n",
    "y = 0.4 * x + 120 + np.random.normal(size=x.shape)\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = plt.axes()\n",
    "ax.scatter(x, y)\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Blood Pressure, SBP [mm Hg]')\n",
    "\n",
    "ax.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0756f",
   "metadata": {},
   "source": [
    "* **Question:** Can we predit the pressure based on a new patient's age?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ea07e",
   "metadata": {},
   "source": [
    "* Want to predict a scalar $\\hat y$ as a function of a scalar $x$<br>\n",
    "* Model: $\\hat y$ is a linear function of $x$:<br>\n",
    "> $\\it\\hat y = wx + b$<br>\n",
    "> $\\hat y$ is the prediction<br>\n",
    "> $w$ is the weight<br>\n",
    "> $b$ is the bias<br>\n",
    "* $w$ and $b$ together are the parameters<br>\n",
    "* Settings of the parameters are called hypotheses<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "befeba55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# predict y from the data\n",
    "x_new = np.linspace(20, 65, 100)\n",
    "y_new = model.predict(x_new[:, np.newaxis])\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = plt.axes()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_new, y_new)\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Blood Pressure, SBP [mm Hg]')\n",
    "\n",
    "ax.axis('tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc4f1c",
   "metadata": {},
   "source": [
    "* How do you know your predited model is accurate? <br>\n",
    "    or how well your predited function models your dataset?"
   ]
  },
  {
   "attachments": {
    "Errors.jpeg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3b994006",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Loss:** Errors of the predition to the real data. Loss helps us to understand how much the predicted value differ from actual value\n",
    "<img src=\"attachment:Errors.jpeg\" width= \"300\"/> </div> <br>\n",
    "\n",
    "* **Loss function:** It is a function to predic the loss. It is mainly applies for a single training set.  \n",
    "     - If your predictions are totally off, your loss function will output a higher number. \n",
    "     - If they‚Äôre pretty good, it‚Äôll output a lower number. <br>\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdd15d",
   "metadata": {},
   "source": [
    "* In linear regression square loss is the most common loss function:\n",
    ">$L_s(y,\\hat y)=(y^{(i)}-\\hat y^{(i)})^{2}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878b731",
   "metadata": {},
   "source": [
    "* **Cost function:** It is a function that measures the performance of a Machine Learning model for given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number.\n",
    "* **Cost function:** loss function averaged over all training examples.<br>\n",
    "* Difference between a lost function and a cost function: A loss function/error function is for a single training example/input. A cost function, on the other hand, is the average loss over the entire training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15010548",
   "metadata": {},
   "source": [
    "* **Mean Square Error (MSE), also called Quadratic loss or L2 Loss:** Mean Square Error (MSE) is the most commonly used regression cost function. MSE is the sum of squared distances between our target variable, $y$, and predicted values, $\\hat y$.\n",
    ">$MSE=\\frac{\\sum_i^N (y^{(i)}-\\hat y^{(i)})^{2}}{N}$  \n",
    "* $\\it y-\\hat y$ is the residual (error), and we want to make this small in magnitude<br>"
   ]
  },
  {
   "attachments": {
    "Picture1.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "01b5f8c6",
   "metadata": {},
   "source": [
    "* How to optimize your linear regression model\n",
    "* Our goal is to find:   $\\underset{w,b}{min}MSE$\n",
    "* Gradient decent:   \n",
    "<img src=\"attachment:Picture1.png\" width= \"300\"/> </div> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc141132",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An example of how to perform Linear Regression with PyTorch\n",
    "* Load the dataset and create tensors\n",
    "We'll create a model that predicts crop yields for apples and oranges (target variables) by looking at the average temperature, rainfall, and humidity (input variables or features) in a region. Here's the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "543f8b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = torch.tensor ([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype=torch.float32)\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = torch.tensor ([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype=torch.float32)\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec3950",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The weights and biases $(w_{11}, w_{12},... w_{23})$ and $(b_1, b_2)$ can also be represented as matrices, initialized as random values. The first row of $w$ and the first element of $b$ are used to predict the first target variable, i.e., yield of apples, and similarly, the second for oranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "255b888e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2790,  1.5873, -0.6567],\n",
      "        [-1.2797,  0.3305,  0.7545]], requires_grad=True)\n",
      "tensor([-0.2280, -0.4864], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "attachments": {
    "WGXLFvA.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ac89a2c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Our model is simply a function that performs a matrix multiplication of the inputs and the weights $w$ (transposed) and adds the bias $b$ (replicated for each observation).\n",
    "\n",
    "<img src=\"attachment:WGXLFvA.png\" width=\"350\"/> </div><br>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1d5860e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 98.2498, -39.3193],\n",
      "        [122.8147, -39.5697],\n",
      "        [198.6546, -23.7762],\n",
      "        [ 72.1850, -88.8883],\n",
      "        [125.4356,  -4.2461]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# We can define the model as follows:\n",
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "#@ represents matrix multiplication in PyTorch, and the .t method returns the transpose of a tensor.\n",
    "# The matrix obtained by passing the input data into the model is a set of predictions for the target variables.\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)\n",
    "\n",
    "# Compare with targets\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb75297",
   "metadata": {},
   "source": [
    "* You can see that there's a huge difference between the predictions of our model, and the actual values of the target variables. Obviously, this is because we've initialized our model with random weights and biases, and we can't expect it to just work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ed151",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Loss function:** Before we improve our model, we need a way to evaluate how well our model is performing. We can compare the model's predictions with the actual targets using the following method: <br>\n",
    "\n",
    "    - Calculate the difference between the two matrices (preds and targets).\n",
    "    - Square all elements of the difference matrix to remove negative values.\n",
    "    - Calculate the average of the elements in the resulting matrix.\n",
    "    - The result is a single number, known as the mean squared error (SE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0af1f86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10022.7061, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# MSE Cost function\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "#torch.sum returns the sum of all the elements in a tensor. The .numel method of a tensor returns the number of elements in a tensor. Let's compute the mean squared error for the current predictions of our model.\n",
    "\n",
    "\n",
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7661c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Compute gradients:** As in previous lectures, with PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases because they have requires_grad set to True. We'll see how this is useful in just a moment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "729c20e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2790,  1.5873, -0.6567],\n",
      "        [-1.2797,  0.3305,  0.7545]], requires_grad=True)\n",
      "tensor([[  4097.2500,   4299.1836,   2508.0371],\n",
      "        [-11151.2549, -11589.4727,  -7215.0610]])\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "#The gradients are stored in the .grad property of the respective tensors. Note that the derivative of the loss w.r.t. the weights matrix is itself a matrix with the same dimensions.\n",
    "\n",
    "# Gradients for weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52f74c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Adjust weights and biases to reduce the loss:**\n",
    "The loss is a quadratic function of our weights and biases, and our objective is to find the set of weights where the loss is the lowest. If we plot a graph of the loss w.r.t any individual weight or bias element, it will look like the figure shown below. An important insight from calculus is that the gradient indicates the rate of change of the loss, i.e., the loss function's slope w.r.t. the weights and biases.\n",
    "\n",
    "If a gradient element is positive:\n",
    "\n",
    "* increasing the weight element's value slightly will increase the loss\n",
    "* decreasing the weight element's value slightly will decrease the loss\n",
    "\n",
    "If a gradient element is negative:\n",
    "\n",
    "* increasing the weight element's value slightly will decrease the loss\n",
    "* decreasing the weight element's value slightly will increase the loss  \n",
    "\n",
    "The increase or decrease in the loss by changing a weight element is proportional to the gradient of the loss w.r.t. that element. This observation forms the basis of the gradient descent optimization algorithm that we'll use to improve our model (by descending along the gradient).\n",
    "\n",
    "We can subtract from each weight element a small quantity proportional to the derivative of the loss w.r.t. that element to reduce the loss slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c556a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 98.2498, -39.3193],\n",
      "        [122.8147, -39.5697],\n",
      "        [198.6546, -23.7762],\n",
      "        [ 72.1850, -88.8883],\n",
      "        [125.4356,  -4.2461]], grad_fn=<AddBackward0>)\n",
      "tensor(10022.7061, grad_fn=<DivBackward0>)\n",
      "tensor([[  8194.5000,   8598.3672,   5016.0742],\n",
      "        [-22302.5098, -23178.9453, -14430.1221]])\n",
      "tensor([  94.5359, -262.3199])\n",
      "tensor([[ 0.1970,  1.5013, -0.7068],\n",
      "        [-1.0567,  0.5623,  0.8988]], requires_grad=True)\n",
      "tensor([-0.2290, -0.4838], requires_grad=True)\n",
      "tensor(4238.1924, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "# Adjust weights & reset gradients\n",
    "# We multiply the gradients with a very small number (10^-5 in this case) to ensure that we don't modify the weights by a very large amount. We want to take a small step in the downhill direction of the gradient, not a giant leap. This number is called the learning rate of the algorithm.\n",
    "# We use torch.no_grad to indicate to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases.    \n",
    "# Before we proceed, we reset the gradients to zero by invoking the .zero_() method. We need to do this because PyTorch accumulates gradients. Otherwise, the next time we invoke .backward on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results.   \n",
    "    \n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(w)\n",
    "print(b)\n",
    "\n",
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148804d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Train the model using gradient descent**\n",
    "As seen above, we reduce the loss and improve our model using the gradient descent optimization algorithm. Thus, we can train the model using the following steps:\n",
    "\n",
    "    1. Generate predictions\n",
    "    2. Calculate the loss\n",
    "    3. Compute gradients w.r.t the weights and biases\n",
    "    4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "    5. Reset the gradients to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf30fd7",
   "metadata": {},
   "source": [
    "* Linear regression using PyTorch built-ins\n",
    "We've implemented linear regression & gradient descent model using some basic tensor operations. However, since this is a common pattern in deep learning, PyTorch provides several built-in functions and classes to make it easy to create and train models with just a few lines of code.\n",
    "\n",
    "* Let's begin by importing the **torch.nn** package from PyTorch, which contains utility classes for building neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f5dc165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs= tensor([[ 73,  67,  43],\n",
      "        [ 91,  88,  64],\n",
      "        [ 87, 134,  58],\n",
      "        [102,  43,  37],\n",
      "        [ 69,  96,  70]])\n",
      "targets= tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "w= tensor([[-0.9507, -1.0005,  0.7608],\n",
      "        [ 2.3370,  0.6812,  0.5952]], requires_grad=True)\n",
      "b= tensor([-0.7575, -0.6335], requires_grad=True)\n",
      "i= 0\n",
      "w.grad= tensor([[-16883.5000, -18872.3691, -11396.0078],\n",
      "        [ 16899.7832,  16235.1504,  10362.8135]])\n",
      "b.grad= tensor([-201.4562,  195.2986])\n",
      "new w tensor([[-0.1065, -0.0569,  1.3306],\n",
      "        [ 1.4920, -0.1305,  0.0771]], requires_grad=True)\n",
      "new b tensor([-0.7474, -0.6433], requires_grad=True)\n",
      "Loss= tensor(41436.9922, grad_fn=<DivBackward0>)\n",
      "i= 1\n",
      "w.grad= tensor([[-1457.1974, -2277.9839, -1160.4766],\n",
      "        [ 2639.2029,   977.0807,   933.2172]])\n",
      "b.grad= tensor([-219.8831,  221.5971])\n",
      "new w tensor([[-0.0337,  0.0570,  1.3886],\n",
      "        [ 1.3600, -0.1794,  0.0304]], requires_grad=True)\n",
      "new b tensor([-0.7364, -0.6544], requires_grad=True)\n",
      "Loss= tensor(1969.4879, grad_fn=<DivBackward0>)\n",
      "i= 2\n",
      "w.grad= tensor([[ 145.9635, -533.5713,  -89.2905],\n",
      "        [1120.5288, -595.1720,  -49.8879]])\n",
      "b.grad= tensor([-219.2433,  230.0267])\n",
      "new w tensor([[-0.0410,  0.0837,  1.3931],\n",
      "        [ 1.3040, -0.1496,  0.0329]], requires_grad=True)\n",
      "new b tensor([-0.7254, -0.6659], requires_grad=True)\n",
      "Loss= tensor(1447.0496, grad_fn=<DivBackward0>)\n",
      "i= 3\n",
      "w.grad= tensor([[ 303.8063, -342.4664,   23.4460],\n",
      "        [ 935.2788, -736.8928, -149.7390]])\n",
      "b.grad= tensor([-216.6820,  236.3993])\n",
      "new w tensor([[-0.0561,  0.1008,  1.3919],\n",
      "        [ 1.2572, -0.1128,  0.0404]], requires_grad=True)\n",
      "new b tensor([-0.7146, -0.6777], requires_grad=True)\n",
      "Loss= tensor(1355.8491, grad_fn=<DivBackward0>)\n",
      "i= 4\n",
      "w.grad= tensor([[ 310.8085, -314.0794,   35.9162],\n",
      "        [ 890.3148, -729.7273, -157.3040]])\n",
      "b.grad= tensor([-213.9899,  242.3744])\n",
      "new w tensor([[-0.0717,  0.1165,  1.3901],\n",
      "        [ 1.2127, -0.0763,  0.0483]], requires_grad=True)\n",
      "new b tensor([-0.7039, -0.6898], requires_grad=True)\n",
      "Loss= tensor(1274.6266, grad_fn=<DivBackward0>)\n",
      "i= 5\n",
      "w.grad= tensor([[ 302.3385, -302.9722,   37.8713],\n",
      "        [ 860.8299, -707.6902, -155.3012]])\n",
      "b.grad= tensor([-211.3522,  248.1313])\n",
      "new w tensor([[-0.0868,  0.1316,  1.3882],\n",
      "        [ 1.1697, -0.0409,  0.0560]], requires_grad=True)\n",
      "new b tensor([-0.6933, -0.7022], requires_grad=True)\n",
      "Loss= tensor(1198.4792, grad_fn=<DivBackward0>)\n",
      "i= 6\n",
      "w.grad= tensor([[ 292.5432, -293.9264,   38.6981],\n",
      "        [ 833.7454, -684.7748, -152.3790]])\n",
      "b.grad= tensor([-208.7861,  253.6945])\n",
      "new w tensor([[-0.1014,  0.1463,  1.3863],\n",
      "        [ 1.1280, -0.0067,  0.0636]], requires_grad=True)\n",
      "new b tensor([-0.6829, -0.7149], requires_grad=True)\n",
      "Loss= tensor(1127.0476, grad_fn=<DivBackward0>)\n",
      "i= 7\n",
      "w.grad= tensor([[ 282.8922, -285.3438,   39.3788],\n",
      "        [ 807.6696, -662.4228, -149.4392]])\n",
      "b.grad= tensor([-206.2913,  259.0718])\n",
      "new w tensor([[-0.1156,  0.1606,  1.3843],\n",
      "        [ 1.0876,  0.0265,  0.0711]], requires_grad=True)\n",
      "new b tensor([-0.6726, -0.7279], requires_grad=True)\n",
      "Loss= tensor(1060.0381, grad_fn=<DivBackward0>)\n",
      "i= 8\n",
      "w.grad= tensor([[ 273.5317, -277.0471,   40.0181],\n",
      "        [ 782.4334, -640.7637, -146.5732]])\n",
      "b.grad= tensor([-203.8658,  264.2695])\n",
      "new w tensor([[-0.1292,  0.1744,  1.3823],\n",
      "        [ 1.0485,  0.0585,  0.0784]], requires_grad=True)\n",
      "new b tensor([-0.6624, -0.7411], requires_grad=True)\n",
      "Loss= tensor(997.1754, grad_fn=<DivBackward0>)\n",
      "i= 9\n",
      "w.grad= tensor([[ 264.4678, -269.0114,   40.6275],\n",
      "        [ 757.9965, -619.7903, -143.7877]])\n",
      "b.grad= tensor([-201.5076,  269.2933])\n",
      "new w tensor([[-0.1425,  0.1879,  1.3803],\n",
      "        [ 1.0106,  0.0895,  0.0856]], requires_grad=True)\n",
      "new b tensor([-0.6523, -0.7545], requires_grad=True)\n",
      "Loss= tensor(938.2015, grad_fn=<DivBackward0>)\n",
      "i= 10\n",
      "w.grad= tensor([[ 255.6924, -261.2267,   41.2087],\n",
      "        [ 734.3284, -599.4860, -141.0837]])\n",
      "b.grad= tensor([-199.2144,  274.1487])\n",
      "new w tensor([[-0.1553,  0.2010,  1.3782],\n",
      "        [ 0.9739,  0.1195,  0.0927]], requires_grad=True)\n",
      "new b tensor([-0.6423, -0.7682], requires_grad=True)\n",
      "Loss= tensor(882.8742, grad_fn=<DivBackward0>)\n",
      "i= 11\n",
      "w.grad= tensor([[ 247.1959, -253.6858,   41.7625],\n",
      "        [ 711.4102, -579.8245, -138.4550]])\n",
      "b.grad= tensor([-196.9844,  278.8410])\n",
      "new w tensor([[-0.1676,  0.2136,  1.3761],\n",
      "        [ 0.9383,  0.1484,  0.0996]], requires_grad=True)\n",
      "new b tensor([-0.6325, -0.7822], requires_grad=True)\n",
      "Loss= tensor(830.9670, grad_fn=<DivBackward0>)\n",
      "i= 12\n",
      "w.grad= tensor([[ 238.9712, -246.3790,   42.2909],\n",
      "        [ 689.2130, -560.7904, -135.9028]])\n",
      "b.grad= tensor([-194.8155,  283.3755])\n",
      "new w tensor([[-0.1796,  0.2260,  1.3740],\n",
      "        [ 0.9038,  0.1765,  0.1064]], requires_grad=True)\n",
      "new b tensor([-0.6228, -0.7964], requires_grad=True)\n",
      "Loss= tensor(782.2666, grad_fn=<DivBackward0>)\n",
      "i= 13\n",
      "w.grad= tensor([[ 231.0083, -239.3004,   42.7937],\n",
      "        [ 667.7175, -542.3607, -133.4224]])\n",
      "b.grad= tensor([-192.7058,  287.7572])\n",
      "new w tensor([[-0.1911,  0.2379,  1.3719],\n",
      "        [ 0.8704,  0.2036,  0.1131]], requires_grad=True)\n",
      "new b tensor([-0.6131, -0.8107], requires_grad=True)\n",
      "Loss= tensor(736.5737, grad_fn=<DivBackward0>)\n",
      "i= 14\n",
      "w.grad= tensor([[ 223.2991, -232.4426,   43.2720],\n",
      "        [ 646.9005, -524.5171, -131.0123]])\n",
      "b.grad= tensor([-190.6536,  291.9909])\n",
      "new w tensor([[-0.2023,  0.2495,  1.3697],\n",
      "        [ 0.8381,  0.2298,  0.1196]], requires_grad=True)\n",
      "new b tensor([-0.6036, -0.8253], requires_grad=True)\n",
      "Loss= tensor(693.7010, grad_fn=<DivBackward0>)\n",
      "i= 15\n",
      "w.grad= tensor([[ 215.8350, -225.7995,   43.7262],\n",
      "        [ 626.7400, -507.2413, -128.6706]])\n",
      "b.grad= tensor([-188.6570,  296.0814])\n",
      "new w tensor([[-0.2131,  0.2608,  1.3675],\n",
      "        [ 0.8068,  0.2552,  0.1261]], requires_grad=True)\n",
      "new b tensor([-0.5942, -0.8401], requires_grad=True)\n",
      "Loss= tensor(653.4733, grad_fn=<DivBackward0>)\n",
      "i= 16\n",
      "w.grad= tensor([[ 208.6105, -219.3614,   44.1587],\n",
      "        [ 607.2145, -490.5165, -126.3955]])\n",
      "b.grad= tensor([-186.7142,  300.0333])\n",
      "new w tensor([[-0.2235,  0.2718,  1.3653],\n",
      "        [ 0.7764,  0.2797,  0.1324]], requires_grad=True)\n",
      "new b tensor([-0.5848, -0.8551], requires_grad=True)\n",
      "Loss= tensor(615.7257, grad_fn=<DivBackward0>)\n",
      "i= 17\n",
      "w.grad= tensor([[ 201.6159, -213.1245,   44.5686],\n",
      "        [ 588.3063, -474.3226, -124.1837]])\n",
      "b.grad= tensor([-184.8238,  303.8509])\n",
      "new w tensor([[-0.2336,  0.2825,  1.3631],\n",
      "        [ 0.7470,  0.3034,  0.1386]], requires_grad=True)\n",
      "new b tensor([-0.5756, -0.8703], requires_grad=True)\n",
      "Loss= tensor(580.3042, grad_fn=<DivBackward0>)\n",
      "i= 18\n",
      "w.grad= tensor([[ 194.8448, -207.0813,   44.9575],\n",
      "        [ 569.9942, -458.6444, -122.0339]])\n",
      "b.grad= tensor([-182.9839,  307.5385])\n",
      "new w tensor([[-0.2433,  0.2928,  1.3608],\n",
      "        [ 0.7185,  0.3264,  0.1447]], requires_grad=True)\n",
      "new b tensor([-0.5664, -0.8857], requires_grad=True)\n",
      "Loss= tensor(547.0639, grad_fn=<DivBackward0>)\n",
      "i= 19\n",
      "w.grad= tensor([[ 188.2887, -201.2273,   45.3251],\n",
      "        [ 552.2590, -443.4663, -119.9448]])\n",
      "b.grad= tensor([-181.1932,  311.1003])\n",
      "new w tensor([[-0.2527,  0.3029,  1.3586],\n",
      "        [ 0.6909,  0.3485,  0.1507]], requires_grad=True)\n",
      "new b tensor([-0.5574, -0.9013], requires_grad=True)\n",
      "Loss= tensor(515.8694, grad_fn=<DivBackward0>)\n",
      "i= 20\n",
      "w.grad= tensor([[ 181.9424, -195.5548,   45.6730],\n",
      "        [ 535.0843, -428.7702, -117.9131]])\n",
      "b.grad= tensor([-179.4500,  314.5403])\n",
      "new w tensor([[-0.2618,  0.3127,  1.3563],\n",
      "        [ 0.6641,  0.3700,  0.1566]], requires_grad=True)\n",
      "new b tensor([-0.5484, -0.9170], requires_grad=True)\n",
      "Loss= tensor(486.5932, grad_fn=<DivBackward0>)\n",
      "i= 21\n",
      "w.grad= tensor([[ 175.7991, -190.0582,   46.0019],\n",
      "        [ 518.4506, -414.5428, -115.9383]])\n",
      "b.grad= tensor([-177.7528,  317.8624])\n",
      "new w tensor([[-0.2706,  0.3222,  1.3540],\n",
      "        [ 0.6382,  0.3907,  0.1624]], requires_grad=True)\n",
      "new b tensor([-0.5395, -0.9329], requires_grad=True)\n",
      "Loss= tensor(459.1161, grad_fn=<DivBackward0>)\n",
      "i= 22\n",
      "w.grad= tensor([[ 169.8524, -184.7319,   46.3125],\n",
      "        [ 502.3411, -400.7688, -114.0183]])\n",
      "b.grad= tensor([-176.1004,  321.0703])\n",
      "new w tensor([[-0.2791,  0.3314,  1.3517],\n",
      "        [ 0.6131,  0.4107,  0.1681]], requires_grad=True)\n",
      "new b tensor([-0.5307, -0.9489], requires_grad=True)\n",
      "Loss= tensor(433.3263, grad_fn=<DivBackward0>)\n",
      "i= 23\n",
      "w.grad= tensor([[ 164.0951, -179.5715,   46.6047],\n",
      "        [ 486.7395, -387.4338, -112.1512]])\n",
      "b.grad= tensor([-174.4912,  324.1677])\n",
      "new w tensor([[-0.2873,  0.3404,  1.3493],\n",
      "        [ 0.5887,  0.4301,  0.1737]], requires_grad=True)\n",
      "new b tensor([-0.5220, -0.9652], requires_grad=True)\n",
      "Loss= tensor(409.1186, grad_fn=<DivBackward0>)\n",
      "i= 24\n",
      "w.grad= tensor([[ 158.5228, -174.5702,   46.8802],\n",
      "        [ 471.6298, -374.5237, -110.3356]])\n",
      "b.grad= tensor([-172.9240,  327.1581])\n",
      "new w tensor([[-0.2952,  0.3491,  1.3470],\n",
      "        [ 0.5652,  0.4488,  0.1792]], requires_grad=True)\n",
      "new b tensor([-0.5133, -0.9815], requires_grad=True)\n",
      "Loss= tensor(386.3950, grad_fn=<DivBackward0>)\n",
      "i= 25\n",
      "w.grad= tensor([[ 153.1271, -169.7256,   47.1379],\n",
      "        [ 456.9969, -362.0245, -108.5692]])\n",
      "b.grad= tensor([-171.3974,  330.0450])\n",
      "new w tensor([[-0.3029,  0.3576,  1.3446],\n",
      "        [ 0.5423,  0.4669,  0.1846]], requires_grad=True)\n",
      "new b tensor([-0.5048, -0.9980], requires_grad=True)\n",
      "Loss= tensor(365.0630, grad_fn=<DivBackward0>)\n",
      "i= 26\n",
      "w.grad= tensor([[ 147.9046, -165.0306,   47.3799],\n",
      "        [ 442.8251, -349.9240, -106.8513]])\n",
      "b.grad= tensor([-169.9103,  332.8316])\n",
      "new w tensor([[-0.3103,  0.3658,  1.3423],\n",
      "        [ 0.5202,  0.4844,  0.1900]], requires_grad=True)\n",
      "new b tensor([-0.4963, -1.0147], requires_grad=True)\n",
      "Loss= tensor(345.0363, grad_fn=<DivBackward0>)\n",
      "i= 27\n",
      "w.grad= tensor([[ 142.8495, -160.4805,   47.6064],\n",
      "        [ 429.0997, -338.2096, -105.1802]])\n",
      "b.grad= tensor([-168.4613,  335.5211])\n",
      "new w tensor([[-0.3174,  0.3739,  1.3399],\n",
      "        [ 0.4987,  0.5013,  0.1952]], requires_grad=True)\n",
      "new b tensor([-0.4878, -1.0314], requires_grad=True)\n",
      "Loss= tensor(326.2336, grad_fn=<DivBackward0>)\n",
      "i= 28\n",
      "w.grad= tensor([[ 137.9559, -156.0716,   47.8176],\n",
      "        [ 415.8074, -326.8683, -103.5540]])\n",
      "b.grad= tensor([-167.0495,  338.1168])\n",
      "new w tensor([[-0.3243,  0.3817,  1.3375],\n",
      "        [ 0.4779,  0.5177,  0.2004]], requires_grad=True)\n",
      "new b tensor([-0.4795, -1.0483], requires_grad=True)\n",
      "Loss= tensor(308.5789, grad_fn=<DivBackward0>)\n",
      "i= 29\n",
      "w.grad= tensor([[ 133.2193, -151.7985,   48.0145],\n",
      "        [ 402.9333, -315.8893, -101.9720]])\n",
      "b.grad= tensor([-165.6735,  340.6214])\n",
      "new w tensor([[-0.3310,  0.3893,  1.3351],\n",
      "        [ 0.4578,  0.5335,  0.2055]], requires_grad=True)\n",
      "new b tensor([-0.4712, -1.0654], requires_grad=True)\n",
      "Loss= tensor(292.0008, grad_fn=<DivBackward0>)\n",
      "i= 30\n",
      "w.grad= tensor([[ 128.6338, -147.6583,   48.1969],\n",
      "        [ 390.4651, -305.2604, -100.4325]])\n",
      "b.grad= tensor([-164.3323,  343.0380])\n",
      "new w tensor([[-0.3374,  0.3966,  1.3327],\n",
      "        [ 0.4383,  0.5487,  0.2105]], requires_grad=True)\n",
      "new b tensor([-0.4630, -1.0825], requires_grad=True)\n",
      "Loss= tensor(276.4326, grad_fn=<DivBackward0>)\n",
      "i= 31\n",
      "w.grad= tensor([[ 124.1957, -143.6455,   48.3659],\n",
      "        [ 378.3894, -294.9711,  -98.9344]])\n",
      "b.grad= tensor([-163.0248,  345.3694])\n",
      "new w tensor([[-0.3436,  0.4038,  1.3303],\n",
      "        [ 0.4193,  0.5635,  0.2155]], requires_grad=True)\n",
      "new b tensor([-0.4548, -1.0998], requires_grad=True)\n",
      "Loss= tensor(261.8116, grad_fn=<DivBackward0>)\n",
      "i= 32\n",
      "w.grad= tensor([[ 119.8993, -139.7571,   48.5215],\n",
      "        [ 366.6943, -285.0098,  -97.4759]])\n",
      "b.grad= tensor([-161.7501,  347.6183])\n",
      "new w tensor([[-0.3496,  0.4108,  1.3278],\n",
      "        [ 0.4010,  0.5777,  0.2203]], requires_grad=True)\n",
      "new b tensor([-0.4468, -1.1172], requires_grad=True)\n",
      "Loss= tensor(248.0787, grad_fn=<DivBackward0>)\n",
      "i= 33\n",
      "w.grad= tensor([[ 115.7410, -135.9883,   48.6647],\n",
      "        [ 355.3682, -275.3662,  -96.0560]])\n",
      "b.grad= tensor([-160.5070,  349.7874])\n",
      "new w tensor([[-0.3554,  0.4176,  1.3254],\n",
      "        [ 0.3832,  0.5915,  0.2251]], requires_grad=True)\n",
      "new b tensor([-0.4387, -1.1347], requires_grad=True)\n",
      "Loss= tensor(235.1790, grad_fn=<DivBackward0>)\n",
      "i= 34\n",
      "w.grad= tensor([[ 111.7162, -132.3354,   48.7957],\n",
      "        [ 344.3978, -266.0314,  -94.6740]])\n",
      "b.grad= tensor([-159.2948,  351.8791])\n",
      "new w tensor([[-0.3610,  0.4242,  1.3230],\n",
      "        [ 0.3660,  0.6048,  0.2299]], requires_grad=True)\n",
      "new b tensor([-0.4308, -1.1523], requires_grad=True)\n",
      "Loss= tensor(223.0606, grad_fn=<DivBackward0>)\n",
      "i= 35\n",
      "w.grad= tensor([[ 107.8201, -128.7956,   48.9146],\n",
      "        [ 333.7729, -256.9950,  -93.3285]])\n",
      "b.grad= tensor([-158.1123,  353.8960])\n",
      "new w tensor([[-0.3664,  0.4307,  1.3205],\n",
      "        [ 0.3493,  0.6177,  0.2345]], requires_grad=True)\n",
      "new b tensor([-0.4229, -1.1699], requires_grad=True)\n",
      "Loss= tensor(211.6753, grad_fn=<DivBackward0>)\n",
      "i= 36\n",
      "w.grad= tensor([[ 104.0491, -125.3647,   49.0221],\n",
      "        [ 323.4824, -248.2473,  -92.0182]])\n",
      "b.grad= tensor([-156.9586,  355.8405])\n",
      "new w tensor([[-0.3716,  0.4369,  1.3181],\n",
      "        [ 0.3331,  0.6301,  0.2392]], requires_grad=True)\n",
      "new b tensor([-0.4150, -1.1877], requires_grad=True)\n",
      "Loss= tensor(200.9773, grad_fn=<DivBackward0>)\n",
      "i= 37\n",
      "w.grad= tensor([[ 100.3992, -122.0393,   49.1186],\n",
      "        [ 313.5168, -239.7782,  -90.7415]])\n",
      "b.grad= tensor([-155.8331,  357.7149])\n",
      "new w tensor([[-0.3766,  0.4430,  1.3156],\n",
      "        [ 0.3175,  0.6421,  0.2437]], requires_grad=True)\n",
      "new b tensor([-0.4072, -1.2056], requires_grad=True)\n",
      "Loss= tensor(190.9242, grad_fn=<DivBackward0>)\n",
      "i= 38\n",
      "w.grad= tensor([[  96.8662, -118.8166,   49.2043],\n",
      "        [ 303.8642, -231.5807,  -89.4983]])\n",
      "b.grad= tensor([-154.7346,  359.5214])\n",
      "new w tensor([[-0.3815,  0.4490,  1.3131],\n",
      "        [ 0.3023,  0.6536,  0.2482]], requires_grad=True)\n",
      "new b tensor([-0.3995, -1.2236], requires_grad=True)\n",
      "Loss= tensor(181.4758, grad_fn=<DivBackward0>)\n",
      "i= 39\n",
      "w.grad= tensor([[  93.4466, -115.6930,   49.2795],\n",
      "        [ 294.5154, -223.6451,  -88.2872]])\n",
      "b.grad= tensor([-153.6626,  361.2622])\n",
      "new w tensor([[-0.3861,  0.4548,  1.3107],\n",
      "        [ 0.2876,  0.6648,  0.2526]], requires_grad=True)\n",
      "new b tensor([-0.3918, -1.2417], requires_grad=True)\n",
      "Loss= tensor(172.5947, grad_fn=<DivBackward0>)\n",
      "i= 40\n",
      "w.grad= tensor([[  90.1381, -112.6638,   49.3457],\n",
      "        [ 285.4601, -215.9644,  -87.1077]])\n",
      "b.grad= tensor([-152.6160,  362.9395])\n",
      "new w tensor([[-0.3906,  0.4604,  1.3082],\n",
      "        [ 0.2733,  0.6756,  0.2569]], requires_grad=True)\n",
      "new b tensor([-0.3842, -1.2598], requires_grad=True)\n",
      "Loss= tensor(164.2457, grad_fn=<DivBackward0>)\n",
      "i= 41\n",
      "w.grad= tensor([[  86.9348, -109.7290,   49.4016],\n",
      "        [ 276.6909, -208.5279,  -85.9575]])\n",
      "b.grad= tensor([-151.5942,  364.5553])\n",
      "new w tensor([[-0.3950,  0.4659,  1.3057],\n",
      "        [ 0.2594,  0.6861,  0.2612]], requires_grad=True)\n",
      "new b tensor([-0.3766, -1.2780], requires_grad=True)\n",
      "Loss= tensor(156.3960, grad_fn=<DivBackward0>)\n",
      "i= 42\n",
      "w.grad= tensor([[  83.8352, -106.8833,   49.4485],\n",
      "        [ 268.1966, -201.3309,  -84.8374]])\n",
      "b.grad= tensor([-150.5965,  366.1115])\n",
      "new w tensor([[-0.3992,  0.4712,  1.3033],\n",
      "        [ 0.2460,  0.6961,  0.2655]], requires_grad=True)\n",
      "new b tensor([-0.3691, -1.2963], requires_grad=True)\n",
      "Loss= tensor(149.0145, grad_fn=<DivBackward0>)\n",
      "i= 43\n",
      "w.grad= tensor([[  80.8351, -104.1251,   49.4865],\n",
      "        [ 259.9703, -194.3629,  -83.7448]])\n",
      "b.grad= tensor([-149.6220,  367.6100])\n",
      "new w tensor([[-0.4032,  0.4764,  1.3008],\n",
      "        [ 0.2330,  0.7058,  0.2697]], requires_grad=True)\n",
      "new b tensor([-0.3616, -1.3147], requires_grad=True)\n",
      "Loss= tensor(142.0721, grad_fn=<DivBackward0>)\n",
      "i= 44\n",
      "w.grad= tensor([[  77.9313, -101.4517,   49.5156],\n",
      "        [ 252.0012, -187.6202,  -82.6808]])\n",
      "b.grad= tensor([-148.6702,  369.0528])\n",
      "new w tensor([[-0.4071,  0.4815,  1.2983],\n",
      "        [ 0.2204,  0.7152,  0.2738]], requires_grad=True)\n",
      "new b tensor([-0.3541, -1.3332], requires_grad=True)\n",
      "Loss= tensor(135.5419, grad_fn=<DivBackward0>)\n",
      "i= 45\n",
      "w.grad= tensor([[  75.1223,  -98.8582,   49.5374],\n",
      "        [ 244.2842, -181.0918,  -81.6425]])\n",
      "b.grad= tensor([-147.7402,  370.4415])\n",
      "new w tensor([[-0.4109,  0.4864,  1.2958],\n",
      "        [ 0.2082,  0.7243,  0.2779]], requires_grad=True)\n",
      "new b tensor([-0.3468, -1.3517], requires_grad=True)\n",
      "Loss= tensor(129.3981, grad_fn=<DivBackward0>)\n",
      "i= 46\n",
      "w.grad= tensor([[  72.4027,  -96.3456,   49.5503],\n",
      "        [ 236.8087, -174.7738,  -80.6306]])\n",
      "b.grad= tensor([-146.8315,  371.7780])\n",
      "new w tensor([[-0.4145,  0.4913,  1.2934],\n",
      "        [ 0.1964,  0.7330,  0.2819]], requires_grad=True)\n",
      "new b tensor([-0.3394, -1.3703], requires_grad=True)\n",
      "Loss= tensor(123.6171, grad_fn=<DivBackward0>)\n",
      "i= 47\n",
      "w.grad= tensor([[  69.7704,  -93.9101,   49.5552],\n",
      "        [ 229.5688, -168.6578,  -79.6435]])\n",
      "b.grad= tensor([-145.9434,  373.0639])\n",
      "new w tensor([[-0.4180,  0.4960,  1.2909],\n",
      "        [ 0.1849,  0.7414,  0.2859]], requires_grad=True)\n",
      "new b tensor([-0.3321, -1.3889], requires_grad=True)\n",
      "Loss= tensor(118.1763, grad_fn=<DivBackward0>)\n",
      "i= 48\n",
      "w.grad= tensor([[  67.2242,  -91.5472,   49.5536],\n",
      "        [ 222.5562, -162.7382,  -78.6807]])\n",
      "b.grad= tensor([-145.0753,  374.3009])\n",
      "new w tensor([[-0.4213,  0.5005,  1.2884],\n",
      "        [ 0.1738,  0.7496,  0.2898]], requires_grad=True)\n",
      "new b tensor([-0.3249, -1.4077], requires_grad=True)\n",
      "Loss= tensor(113.0546, grad_fn=<DivBackward0>)\n",
      "i= 49\n",
      "w.grad= tensor([[  64.7594,  -89.2574,   49.5442],\n",
      "        [ 215.7641, -157.0082,  -77.7414]])\n",
      "b.grad= tensor([-144.2266,  375.4905])\n",
      "new w tensor([[-0.4246,  0.5050,  1.2859],\n",
      "        [ 0.1630,  0.7574,  0.2937]], requires_grad=True)\n",
      "new b tensor([-0.3177, -1.4264], requires_grad=True)\n",
      "Loss= tensor(108.2324, grad_fn=<DivBackward0>)\n",
      "i= 50\n",
      "w.grad= tensor([[  62.3750,  -87.0365,   49.5284],\n",
      "        [ 209.1848, -151.4631,  -76.8253]])\n",
      "b.grad= tensor([-143.3967,  376.6343])\n",
      "new w tensor([[-0.4277,  0.5094,  1.2835],\n",
      "        [ 0.1525,  0.7650,  0.2976]], requires_grad=True)\n",
      "new b tensor([-0.3105, -1.4453], requires_grad=True)\n",
      "Loss= tensor(103.6912, grad_fn=<DivBackward0>)\n",
      "i= 51\n",
      "w.grad= tensor([[  60.0668,  -84.8837,   49.5054],\n",
      "        [ 202.8125, -146.0955,  -75.9311]])\n",
      "b.grad= tensor([-142.5850,  377.7337])\n",
      "new w tensor([[-0.4307,  0.5136,  1.2810],\n",
      "        [ 0.1424,  0.7723,  0.3013]], requires_grad=True)\n",
      "new b tensor([-0.3034, -1.4642], requires_grad=True)\n",
      "Loss= tensor(99.4137, grad_fn=<DivBackward0>)\n",
      "i= 52\n",
      "w.grad= tensor([[  57.8342,  -82.7954,   49.4768],\n",
      "        [ 196.6406, -140.9000,  -75.0582]])\n",
      "b.grad= tensor([-141.7911,  378.7901])\n",
      "new w tensor([[-0.4336,  0.5177,  1.2785],\n",
      "        [ 0.1326,  0.7794,  0.3051]], requires_grad=True)\n",
      "new b tensor([-0.2963, -1.4831], requires_grad=True)\n",
      "Loss= tensor(95.3834, grad_fn=<DivBackward0>)\n",
      "i= 53\n",
      "w.grad= tensor([[  55.6728,  -80.7714,   49.4411],\n",
      "        [ 190.6615, -135.8726,  -74.2068]])\n",
      "b.grad= tensor([-141.0143,  379.8050])\n",
      "new w tensor([[-0.4364,  0.5218,  1.2760],\n",
      "        [ 0.1230,  0.7862,  0.3088]], requires_grad=True)\n",
      "new b tensor([-0.2892, -1.5021], requires_grad=True)\n",
      "Loss= tensor(91.5854, grad_fn=<DivBackward0>)\n",
      "i= 54\n",
      "w.grad= tensor([[  53.5820,  -78.8082,   49.4000],\n",
      "        [ 184.8712, -131.0053,  -73.3747]])\n",
      "b.grad= tensor([-140.2542,  380.7797])\n",
      "new w tensor([[-0.4391,  0.5257,  1.2736],\n",
      "        [ 0.1138,  0.7927,  0.3125]], requires_grad=True)\n",
      "new b tensor([-0.2822, -1.5211], requires_grad=True)\n",
      "Loss= tensor(88.0051, grad_fn=<DivBackward0>)\n",
      "i= 55\n",
      "w.grad= tensor([[  51.5587,  -76.9046,   49.3529],\n",
      "        [ 179.2617, -126.2961,  -72.5630]])\n",
      "b.grad= tensor([-139.5103,  381.7155])\n",
      "new w tensor([[-0.4416,  0.5296,  1.2711],\n",
      "        [ 0.1048,  0.7990,  0.3161]], requires_grad=True)\n",
      "new b tensor([-0.2752, -1.5402], requires_grad=True)\n",
      "Loss= tensor(84.6293, grad_fn=<DivBackward0>)\n",
      "i= 56\n",
      "w.grad= tensor([[  49.6017,  -75.0576,   49.3009],\n",
      "        [ 173.8278, -121.7386,  -71.7704]])\n",
      "b.grad= tensor([-138.7820,  382.6136])\n",
      "new w tensor([[-0.4441,  0.5333,  1.2686],\n",
      "        [ 0.0961,  0.8051,  0.3197]], requires_grad=True)\n",
      "new b tensor([-0.2683, -1.5593], requires_grad=True)\n",
      "Loss= tensor(81.4453, grad_fn=<DivBackward0>)\n",
      "i= 57\n",
      "w.grad= tensor([[  47.7076,  -73.2672,   49.2431],\n",
      "        [ 168.5654, -117.3270,  -70.9956]])\n",
      "b.grad= tensor([-138.0690,  383.4753])\n",
      "new w tensor([[-0.4465,  0.5370,  1.2662],\n",
      "        [ 0.0877,  0.8110,  0.3232]], requires_grad=True)\n",
      "new b tensor([-0.2614, -1.5785], requires_grad=True)\n",
      "Loss= tensor(78.4413, grad_fn=<DivBackward0>)\n",
      "i= 58\n",
      "w.grad= tensor([[  45.8751,  -71.5304,   49.1803],\n",
      "        [ 163.4682, -113.0570,  -70.2385]])\n",
      "b.grad= tensor([-137.3708,  384.3018])\n",
      "new w tensor([[-0.4488,  0.5406,  1.2637],\n",
      "        [ 0.0795,  0.8166,  0.3268]], requires_grad=True)\n",
      "new b tensor([-0.2545, -1.5977], requires_grad=True)\n",
      "Loss= tensor(75.6063, grad_fn=<DivBackward0>)\n",
      "i= 59\n",
      "w.grad= tensor([[  44.1014,  -69.8468,   49.1122],\n",
      "        [ 158.5291, -108.9269,  -69.5000]])\n",
      "b.grad= tensor([-136.6869,  385.0941])\n",
      "new w tensor([[-0.4510,  0.5440,  1.2613],\n",
      "        [ 0.0716,  0.8221,  0.3302]], requires_grad=True)\n",
      "new b tensor([-0.2477, -1.6170], requires_grad=True)\n",
      "Loss= tensor(72.9299, grad_fn=<DivBackward0>)\n",
      "i= 60\n",
      "w.grad= tensor([[  42.3871,  -68.2118,   49.0405],\n",
      "        [ 153.7465, -104.9277,  -68.7771]])\n",
      "b.grad= tensor([-136.0170,  385.8535])\n",
      "new w tensor([[-0.4531,  0.5475,  1.2588],\n",
      "        [ 0.0639,  0.8273,  0.3337]], requires_grad=True)\n",
      "new b tensor([-0.2409, -1.6363], requires_grad=True)\n",
      "Loss= tensor(70.4025, grad_fn=<DivBackward0>)\n",
      "i= 61\n",
      "w.grad= tensor([[  40.7268,  -66.6280,   48.9629],\n",
      "        [ 149.1127, -101.0591,  -68.0712]])\n",
      "b.grad= tensor([-135.3605,  386.5808])\n",
      "new w tensor([[-0.4552,  0.5508,  1.2564],\n",
      "        [ 0.0565,  0.8324,  0.3371]], requires_grad=True)\n",
      "new b tensor([-0.2341, -1.6556], requires_grad=True)\n",
      "Loss= tensor(68.0147, grad_fn=<DivBackward0>)\n",
      "i= 62\n",
      "w.grad= tensor([[ 39.1219, -65.0901,  48.8819],\n",
      "        [144.6247, -97.3146, -67.3808]])\n",
      "b.grad= tensor([-134.7172,  387.2773])\n",
      "new w tensor([[-0.4571,  0.5540,  1.2539],\n",
      "        [ 0.0492,  0.8372,  0.3404]], requires_grad=True)\n",
      "new b tensor([-0.2274, -1.6750], requires_grad=True)\n",
      "Loss= tensor(65.7582, grad_fn=<DivBackward0>)\n",
      "i= 63\n",
      "w.grad= tensor([[ 37.5688, -63.5988,  48.7964],\n",
      "        [140.2766, -93.6918, -66.7061]])\n",
      "b.grad= tensor([-134.0866,  387.9439])\n",
      "new w tensor([[-0.4590,  0.5572,  1.2515],\n",
      "        [ 0.0422,  0.8419,  0.3438]], requires_grad=True)\n",
      "new b tensor([-0.2207, -1.6944], requires_grad=True)\n",
      "Loss= tensor(63.6248, grad_fn=<DivBackward0>)\n",
      "i= 64\n",
      "w.grad= tensor([[ 36.0665, -62.1517,  48.7070],\n",
      "        [136.0642, -90.1866, -66.0467]])\n",
      "b.grad= tensor([-133.4684,  388.5816])\n",
      "new w tensor([[-0.4608,  0.5603,  1.2490],\n",
      "        [ 0.0354,  0.8464,  0.3471]], requires_grad=True)\n",
      "new b tensor([-0.2140, -1.7138], requires_grad=True)\n",
      "Loss= tensor(61.6071, grad_fn=<DivBackward0>)\n",
      "i= 65\n",
      "w.grad= tensor([[ 34.6138, -60.7475,  48.6140],\n",
      "        [131.9854, -86.7928, -65.4005]])\n",
      "b.grad= tensor([-132.8621,  389.1913])\n",
      "new w tensor([[-0.4625,  0.5634,  1.2466],\n",
      "        [ 0.0288,  0.8508,  0.3504]], requires_grad=True)\n",
      "new b tensor([-0.2074, -1.7333], requires_grad=True)\n",
      "Loss= tensor(59.6980, grad_fn=<DivBackward0>)\n",
      "i= 66\n",
      "w.grad= tensor([[ 33.2077, -59.3859,  48.5167],\n",
      "        [128.0329, -83.5102, -64.7693]])\n",
      "b.grad= tensor([-132.2675,  389.7739])\n",
      "new w tensor([[-0.4642,  0.5663,  1.2442],\n",
      "        [ 0.0224,  0.8549,  0.3536]], requires_grad=True)\n",
      "new b tensor([-0.2007, -1.7527], requires_grad=True)\n",
      "Loss= tensor(57.8909, grad_fn=<DivBackward0>)\n",
      "i= 67\n",
      "w.grad= tensor([[ 31.8476, -58.0649,  48.4159],\n",
      "        [124.2040, -80.3342, -64.1517]])\n",
      "b.grad= tensor([-131.6842,  390.3304])\n",
      "new w tensor([[-0.4658,  0.5692,  1.2418],\n",
      "        [ 0.0162,  0.8590,  0.3568]], requires_grad=True)\n",
      "new b tensor([-0.1942, -1.7723], requires_grad=True)\n",
      "Loss= tensor(56.1796, grad_fn=<DivBackward0>)\n",
      "i= 68\n",
      "w.grad= tensor([[ 30.5323, -56.7828,  48.3118],\n",
      "        [120.4953, -77.2606, -63.5471]])\n",
      "b.grad= tensor([-131.1119,  390.8615])\n",
      "new w tensor([[-0.4673,  0.5721,  1.2393],\n",
      "        [ 0.0102,  0.8628,  0.3600]], requires_grad=True)\n",
      "new b tensor([-0.1876, -1.7918], requires_grad=True)\n",
      "Loss= tensor(54.5582, grad_fn=<DivBackward0>)\n",
      "i= 69\n",
      "w.grad= tensor([[ 29.2607, -55.5380,  48.2048],\n",
      "        [116.9020, -74.2870, -62.9556]])\n",
      "b.grad= tensor([-130.5503,  391.3680])\n",
      "new w tensor([[-0.4688,  0.5749,  1.2369],\n",
      "        [ 0.0043,  0.8665,  0.3631]], requires_grad=True)\n",
      "new b tensor([-0.1811, -1.8114], requires_grad=True)\n",
      "Loss= tensor(53.0213, grad_fn=<DivBackward0>)\n",
      "i= 70\n",
      "w.grad= tensor([[ 28.0294, -54.3316,  48.0938],\n",
      "        [113.4213, -71.4098, -62.3763]])\n",
      "b.grad= tensor([-129.9991,  391.8509])\n",
      "new w tensor([[-0.4702,  0.5776,  1.2345],\n",
      "        [-0.0013,  0.8701,  0.3662]], requires_grad=True)\n",
      "new b tensor([-0.1746, -1.8310], requires_grad=True)\n",
      "Loss= tensor(51.5637, grad_fn=<DivBackward0>)\n",
      "i= 71\n",
      "w.grad= tensor([[ 26.8397, -53.1596,  47.9804],\n",
      "        [110.0494, -68.6258, -61.8091]])\n",
      "b.grad= tensor([-129.4579,  392.3109])\n",
      "new w tensor([[-0.4715,  0.5802,  1.2321],\n",
      "        [-0.0069,  0.8735,  0.3693]], requires_grad=True)\n",
      "new b tensor([-0.1681, -1.8506], requires_grad=True)\n",
      "Loss= tensor(50.1808, grad_fn=<DivBackward0>)\n",
      "i= 72\n",
      "w.grad= tensor([[ 25.6896, -52.0214,  47.8646],\n",
      "        [106.7829, -65.9324, -61.2536]])\n",
      "b.grad= tensor([-128.9264,  392.7486])\n",
      "new w tensor([[-0.4728,  0.5828,  1.2297],\n",
      "        [-0.0122,  0.8768,  0.3724]], requires_grad=True)\n",
      "new b tensor([-0.1617, -1.8702], requires_grad=True)\n",
      "Loss= tensor(48.8679, grad_fn=<DivBackward0>)\n",
      "i= 73\n",
      "w.grad= tensor([[ 24.5759, -50.9186,  47.7449],\n",
      "        [103.6177, -63.3273, -60.7100]])\n",
      "b.grad= tensor([-128.4045,  393.1649])\n",
      "new w tensor([[-0.4740,  0.5854,  1.2273],\n",
      "        [-0.0174,  0.8800,  0.3754]], requires_grad=True)\n",
      "new b tensor([-0.1552, -1.8899], requires_grad=True)\n",
      "Loss= tensor(47.6208, grad_fn=<DivBackward0>)\n",
      "i= 74\n",
      "w.grad= tensor([[ 23.4993, -49.8476,  47.6228],\n",
      "        [100.5517, -60.8065, -60.1770]])\n",
      "b.grad= tensor([-127.8918,  393.5605])\n",
      "new w tensor([[-0.4752,  0.5879,  1.2250],\n",
      "        [-0.0224,  0.8830,  0.3784]], requires_grad=True)\n",
      "new b tensor([-0.1488, -1.9095], requires_grad=True)\n",
      "Loss= tensor(46.4356, grad_fn=<DivBackward0>)\n",
      "i= 75\n",
      "w.grad= tensor([[ 22.4592, -48.8069,  47.4990],\n",
      "        [ 97.5809, -58.3682, -59.6552]])\n",
      "b.grad= tensor([-127.3880,  393.9359])\n",
      "new w tensor([[-0.4763,  0.5903,  1.2226],\n",
      "        [-0.0273,  0.8860,  0.3814]], requires_grad=True)\n",
      "new b tensor([-0.1425, -1.9292], requires_grad=True)\n",
      "Loss= tensor(45.3086, grad_fn=<DivBackward0>)\n",
      "i= 76\n",
      "w.grad= tensor([[ 21.4520, -47.7983,  47.3717],\n",
      "        [ 94.7036, -56.0087, -59.1432]])\n",
      "b.grad= tensor([-126.8930,  394.2920])\n",
      "new w tensor([[-0.4774,  0.5927,  1.2202],\n",
      "        [-0.0320,  0.8888,  0.3844]], requires_grad=True)\n",
      "new b tensor([-0.1361, -1.9490], requires_grad=True)\n",
      "Loss= tensor(44.2362, grad_fn=<DivBackward0>)\n",
      "i= 77\n",
      "w.grad= tensor([[ 20.4793, -46.8178,  47.2428],\n",
      "        [ 91.9150, -53.7269, -58.6418]])\n",
      "b.grad= tensor([-126.4064,  394.6293])\n",
      "new w tensor([[-0.4784,  0.5950,  1.2179],\n",
      "        [-0.0366,  0.8914,  0.3873]], requires_grad=True)\n",
      "new b tensor([-0.1298, -1.9687], requires_grad=True)\n",
      "Loss= tensor(43.2152, grad_fn=<DivBackward0>)\n",
      "i= 78\n",
      "w.grad= tensor([[ 19.5386, -45.8661,  47.1116],\n",
      "        [ 89.2137, -51.5194, -58.1502]])\n",
      "b.grad= tensor([-125.9281,  394.9484])\n",
      "new w tensor([[-0.4794,  0.5973,  1.2155],\n",
      "        [-0.0411,  0.8940,  0.3902]], requires_grad=True)\n",
      "new b tensor([-0.1235, -1.9884], requires_grad=True)\n",
      "Loss= tensor(42.2426, grad_fn=<DivBackward0>)\n",
      "i= 79\n",
      "w.grad= tensor([[ 18.6288, -44.9423,  46.9780],\n",
      "        [ 86.5970, -49.3835, -57.6678]])\n",
      "b.grad= tensor([-125.4577,  395.2500])\n",
      "new w tensor([[-0.4803,  0.5996,  1.2132],\n",
      "        [-0.0454,  0.8965,  0.3931]], requires_grad=True)\n",
      "new b tensor([-0.1172, -2.0082], requires_grad=True)\n",
      "Loss= tensor(41.3154, grad_fn=<DivBackward0>)\n",
      "i= 80\n",
      "w.grad= tensor([[ 17.7495, -44.0450,  46.8427],\n",
      "        [ 84.0605, -47.3191, -57.1954]])\n",
      "b.grad= tensor([-124.9951,  395.5345])\n",
      "new w tensor([[-0.4812,  0.6018,  1.2108],\n",
      "        [-0.0496,  0.8989,  0.3960]], requires_grad=True)\n",
      "new b tensor([-0.1110, -2.0280], requires_grad=True)\n",
      "Loss= tensor(40.4310, grad_fn=<DivBackward0>)\n",
      "i= 81\n",
      "w.grad= tensor([[ 16.8991, -43.1740,  46.7052],\n",
      "        [ 81.6038, -45.3212, -56.7314]])\n",
      "b.grad= tensor([-124.5401,  395.8027])\n",
      "new w tensor([[-0.4821,  0.6039,  1.2085],\n",
      "        [-0.0537,  0.9011,  0.3988]], requires_grad=True)\n",
      "new b tensor([-0.1048, -2.0478], requires_grad=True)\n",
      "Loss= tensor(39.5868, grad_fn=<DivBackward0>)\n",
      "i= 82\n",
      "w.grad= tensor([[ 16.0766, -42.3287,  46.5655],\n",
      "        [ 79.2236, -43.3885, -56.2762]])\n",
      "b.grad= tensor([-124.0924,  396.0550])\n",
      "new w tensor([[-0.4829,  0.6061,  1.2061],\n",
      "        [-0.0576,  0.9033,  0.4016]], requires_grad=True)\n",
      "new b tensor([-0.0986, -2.0676], requires_grad=True)\n",
      "Loss= tensor(38.7805, grad_fn=<DivBackward0>)\n",
      "i= 83\n",
      "w.grad= tensor([[ 15.2827, -41.5063,  46.4250],\n",
      "        [ 76.9169, -41.5200, -55.8298]])\n",
      "b.grad= tensor([-123.6518,  396.2919])\n",
      "new w tensor([[-0.4836,  0.6081,  1.2038],\n",
      "        [-0.0615,  0.9054,  0.4044]], requires_grad=True)\n",
      "new b tensor([-0.0924, -2.0874], requires_grad=True)\n",
      "Loss= tensor(38.0097, grad_fn=<DivBackward0>)\n",
      "i= 84\n",
      "w.grad= tensor([[ 14.5145, -40.7086,  46.2820],\n",
      "        [ 74.6824, -39.7122, -55.3913]])\n",
      "b.grad= tensor([-123.2182,  396.5140])\n",
      "new w tensor([[-0.4844,  0.6102,  1.2015],\n",
      "        [-0.0652,  0.9074,  0.4072]], requires_grad=True)\n",
      "new b tensor([-0.0862, -2.1072], requires_grad=True)\n",
      "Loss= tensor(37.2725, grad_fn=<DivBackward0>)\n",
      "i= 85\n",
      "w.grad= tensor([[ 13.7727, -39.9328,  46.1382],\n",
      "        [ 72.5168, -37.9646, -54.9612]])\n",
      "b.grad= tensor([-122.7913,  396.7218])\n",
      "new w tensor([[-0.4850,  0.6122,  1.1992],\n",
      "        [-0.0689,  0.9093,  0.4099]], requires_grad=True)\n",
      "new b tensor([-0.0801, -2.1270], requires_grad=True)\n",
      "Loss= tensor(36.5669, grad_fn=<DivBackward0>)\n",
      "i= 86\n",
      "w.grad= tensor([[ 13.0550, -39.1800,  45.9922],\n",
      "        [ 70.4182, -36.2744, -54.5390]])\n",
      "b.grad= tensor([-122.3710,  396.9157])\n",
      "new w tensor([[-0.4857,  0.6141,  1.1969],\n",
      "        [-0.0724,  0.9111,  0.4126]], requires_grad=True)\n",
      "new b tensor([-0.0740, -2.1469], requires_grad=True)\n",
      "Loss= tensor(35.8910, grad_fn=<DivBackward0>)\n",
      "i= 87\n",
      "w.grad= tensor([[ 12.3616, -38.4486,  45.8449],\n",
      "        [ 68.3851, -34.6399, -54.1241]])\n",
      "b.grad= tensor([-121.9570,  397.0962])\n",
      "new w tensor([[-0.4863,  0.6160,  1.1946],\n",
      "        [-0.0758,  0.9128,  0.4154]], requires_grad=True)\n",
      "new b tensor([-0.0679, -2.1667], requires_grad=True)\n",
      "Loss= tensor(35.2430, grad_fn=<DivBackward0>)\n",
      "i= 88\n",
      "w.grad= tensor([[ 11.6912, -37.7381,  45.6962],\n",
      "        [ 66.4149, -33.0593, -53.7166]])\n",
      "b.grad= tensor([-121.5493,  397.2637])\n",
      "new w tensor([[-0.4869,  0.6179,  1.1923],\n",
      "        [-0.0791,  0.9145,  0.4180]], requires_grad=True)\n",
      "new b tensor([-0.0618, -2.1866], requires_grad=True)\n",
      "Loss= tensor(34.6215, grad_fn=<DivBackward0>)\n",
      "i= 89\n",
      "w.grad= tensor([[ 11.0444, -37.0468,  45.5467],\n",
      "        [ 64.5058, -31.5309, -53.3163]])\n",
      "b.grad= tensor([-121.1476,  397.4186])\n",
      "new w tensor([[-0.4875,  0.6198,  1.1901],\n",
      "        [-0.0823,  0.9160,  0.4207]], requires_grad=True)\n",
      "new b tensor([-0.0557, -2.2065], requires_grad=True)\n",
      "Loss= tensor(34.0248, grad_fn=<DivBackward0>)\n",
      "i= 90\n",
      "w.grad= tensor([[ 10.4186, -36.3757,  45.3956],\n",
      "        [ 62.6557, -30.0533, -52.9231]])\n",
      "b.grad= tensor([-120.7518,  397.5615])\n",
      "new w tensor([[-0.4880,  0.6216,  1.1878],\n",
      "        [-0.0855,  0.9175,  0.4234]], requires_grad=True)\n",
      "new b tensor([-0.0497, -2.2264], requires_grad=True)\n",
      "Loss= tensor(33.4516, grad_fn=<DivBackward0>)\n",
      "i= 91\n",
      "w.grad= tensor([[  9.8138, -35.7239,  45.2431],\n",
      "        [ 60.8625, -28.6249, -52.5368]])\n",
      "b.grad= tensor([-120.3617,  397.6927])\n",
      "new w tensor([[-0.4885,  0.6234,  1.1855],\n",
      "        [-0.0885,  0.9190,  0.4260]], requires_grad=True)\n",
      "new b tensor([-0.0437, -2.2462], requires_grad=True)\n",
      "Loss= tensor(32.9004, grad_fn=<DivBackward0>)\n",
      "i= 92\n",
      "w.grad= tensor([[  9.2302, -35.0896,  45.0901],\n",
      "        [ 59.1256, -27.2431, -52.1566]])\n",
      "b.grad= tensor([-119.9771,  397.8125])\n",
      "new w tensor([[-0.4889,  0.6251,  1.1833],\n",
      "        [-0.0915,  0.9203,  0.4286]], requires_grad=True)\n",
      "new b tensor([-0.0377, -2.2661], requires_grad=True)\n",
      "Loss= tensor(32.3701, grad_fn=<DivBackward0>)\n",
      "i= 93\n",
      "w.grad= tensor([[  8.6657, -34.4738,  44.9355],\n",
      "        [ 57.4416, -25.9081, -51.7832]])\n",
      "b.grad= tensor([-119.5980,  397.9214])\n",
      "new w tensor([[-0.4894,  0.6269,  1.1810],\n",
      "        [-0.0943,  0.9216,  0.4312]], requires_grad=True)\n",
      "new b tensor([-0.0317, -2.2860], requires_grad=True)\n",
      "Loss= tensor(31.8595, grad_fn=<DivBackward0>)\n",
      "i= 94\n",
      "w.grad= tensor([[  8.1215, -33.8740,  44.7807],\n",
      "        [ 55.8106, -24.6162, -51.4155]])\n",
      "b.grad= tensor([-119.2242,  398.0198])\n",
      "new w tensor([[-0.4898,  0.6286,  1.1788],\n",
      "        [-0.0971,  0.9229,  0.4337]], requires_grad=True)\n",
      "new b tensor([-0.0257, -2.3059], requires_grad=True)\n",
      "Loss= tensor(31.3674, grad_fn=<DivBackward0>)\n",
      "i= 95\n",
      "w.grad= tensor([[  7.5947, -33.2922,  44.6241],\n",
      "        [ 54.2289, -23.3687, -51.0545]])\n",
      "b.grad= tensor([-118.8554,  398.1079])\n",
      "new w tensor([[-0.4901,  0.6302,  1.1765],\n",
      "        [-0.0998,  0.9240,  0.4363]], requires_grad=True)\n",
      "new b tensor([-0.0198, -2.3258], requires_grad=True)\n",
      "Loss= tensor(30.8928, grad_fn=<DivBackward0>)\n",
      "i= 96\n",
      "w.grad= tensor([[  7.0859, -32.7264,  44.4669],\n",
      "        [ 52.6969, -22.1618, -50.6988]])\n",
      "b.grad= tensor([-118.4917,  398.1862])\n",
      "new w tensor([[-0.4905,  0.6319,  1.1743],\n",
      "        [-0.1025,  0.9251,  0.4388]], requires_grad=True)\n",
      "new b tensor([-0.0139, -2.3457], requires_grad=True)\n",
      "Loss= tensor(30.4347, grad_fn=<DivBackward0>)\n",
      "i= 97\n",
      "w.grad= tensor([[  6.5950, -32.1757,  44.3091],\n",
      "        [ 51.2105, -20.9971, -50.3499]])\n",
      "b.grad= tensor([-118.1328,  398.2549])\n",
      "new w tensor([[-0.4908,  0.6335,  1.1721],\n",
      "        [-0.1050,  0.9262,  0.4414]], requires_grad=True)\n",
      "new b tensor([-0.0080, -2.3657], requires_grad=True)\n",
      "Loss= tensor(29.9924, grad_fn=<DivBackward0>)\n",
      "i= 98\n",
      "w.grad= tensor([[  6.1209, -31.6403,  44.1505],\n",
      "        [ 49.7718, -19.8691, -50.0053]])\n",
      "b.grad= tensor([-117.7787,  398.3145])\n",
      "new w tensor([[-0.4911,  0.6351,  1.1699],\n",
      "        [-0.1075,  0.9272,  0.4439]], requires_grad=True)\n",
      "new b tensor([-2.0626e-03, -2.3856e+00], requires_grad=True)\n",
      "Loss= tensor(29.5647, grad_fn=<DivBackward0>)\n",
      "i= 99\n",
      "w.grad= tensor([[  5.6632, -31.1194,  43.9913],\n",
      "        [ 48.3765, -18.7800, -49.6667]])\n",
      "b.grad= tensor([-117.4291,  398.3651])\n",
      "new w tensor([[-0.4914,  0.6366,  1.1677],\n",
      "        [-0.1099,  0.9281,  0.4463]], requires_grad=True)\n",
      "new b tensor([ 0.0038, -2.4055], requires_grad=True)\n",
      "Loss= tensor(29.1510, grad_fn=<DivBackward0>)\n",
      "i= 100\n",
      "w.grad= tensor([[  5.2210, -30.6133,  43.8313],\n",
      "        [ 47.0242, -17.7274, -49.3333]])\n",
      "b.grad= tensor([-117.0841,  398.4071])\n",
      "new w tensor([[-0.4917,  0.6381,  1.1655],\n",
      "        [-0.1123,  0.9290,  0.4488]], requires_grad=True)\n",
      "new b tensor([ 0.0097, -2.4254], requires_grad=True)\n",
      "Loss= tensor(28.7505, grad_fn=<DivBackward0>)\n",
      "i= 101\n",
      "w.grad= tensor([[  4.7948, -30.1199,  43.6711],\n",
      "        [ 45.7135, -16.7102, -49.0051]])\n",
      "b.grad= tensor([-116.7434,  398.4408])\n",
      "new w tensor([[-0.4919,  0.6396,  1.1633],\n",
      "        [-0.1146,  0.9298,  0.4513]], requires_grad=True)\n",
      "new b tensor([ 0.0155, -2.4453], requires_grad=True)\n",
      "Loss= tensor(28.3625, grad_fn=<DivBackward0>)\n",
      "i= 102\n",
      "w.grad= tensor([[  4.3818, -29.6418,  43.5093],\n",
      "        [ 44.4428, -15.7275, -48.6819]])\n",
      "b.grad= tensor([-116.4070,  398.4666])\n",
      "new w tensor([[-0.4921,  0.6411,  1.1612],\n",
      "        [-0.1168,  0.9306,  0.4537]], requires_grad=True)\n",
      "new b tensor([ 0.0213, -2.4653], requires_grad=True)\n",
      "Loss= tensor(27.9863, grad_fn=<DivBackward0>)\n",
      "i= 103\n",
      "w.grad= tensor([[  3.9846, -29.1751,  43.3480],\n",
      "        [ 43.2112, -14.7779, -48.3636]])\n",
      "b.grad= tensor([-116.0748,  398.4845])\n",
      "new w tensor([[-0.4923,  0.6426,  1.1590],\n",
      "        [-0.1190,  0.9314,  0.4561]], requires_grad=True)\n",
      "new b tensor([ 0.0271, -2.4852], requires_grad=True)\n",
      "Loss= tensor(27.6214, grad_fn=<DivBackward0>)\n",
      "i= 104\n",
      "w.grad= tensor([[  3.6007, -28.7216,  43.1857],\n",
      "        [ 42.0178, -13.8600, -48.0498]])\n",
      "b.grad= tensor([-115.7466,  398.4950])\n",
      "new w tensor([[-0.4925,  0.6440,  1.1568],\n",
      "        [-0.1211,  0.9321,  0.4585]], requires_grad=True)\n",
      "new b tensor([ 0.0329, -2.5051], requires_grad=True)\n",
      "Loss= tensor(27.2671, grad_fn=<DivBackward0>)\n",
      "i= 105\n",
      "w.grad= tensor([[  3.2309, -28.2793,  43.0237],\n",
      "        [ 40.8601, -12.9741, -47.7410]])\n",
      "b.grad= tensor([-115.4223,  398.4983])\n",
      "new w tensor([[-0.4927,  0.6454,  1.1547],\n",
      "        [-0.1231,  0.9327,  0.4609]], requires_grad=True)\n",
      "new b tensor([ 0.0387, -2.5250], requires_grad=True)\n",
      "Loss= tensor(26.9228, grad_fn=<DivBackward0>)\n",
      "i= 106\n",
      "w.grad= tensor([[  2.8723, -27.8509,  42.8600],\n",
      "        [ 39.7384, -12.1177, -47.4365]])\n",
      "b.grad= tensor([-115.1019,  398.4946])\n",
      "new w tensor([[-0.4928,  0.6468,  1.1525],\n",
      "        [-0.1251,  0.9333,  0.4633]], requires_grad=True)\n",
      "new b tensor([ 0.0444, -2.5450], requires_grad=True)\n",
      "Loss= tensor(26.5880, grad_fn=<DivBackward0>)\n",
      "i= 107\n",
      "w.grad= tensor([[  2.5283, -27.4315,  42.6973],\n",
      "        [ 38.6511, -11.2902, -47.1363]])\n",
      "b.grad= tensor([-114.7853,  398.4842])\n",
      "new w tensor([[-0.4930,  0.6482,  1.1504],\n",
      "        [-0.1270,  0.9339,  0.4656]], requires_grad=True)\n",
      "new b tensor([ 0.0502, -2.5649], requires_grad=True)\n",
      "Loss= tensor(26.2623, grad_fn=<DivBackward0>)\n",
      "i= 108\n",
      "w.grad= tensor([[  2.1945, -27.0256,  42.5330],\n",
      "        [ 37.5969, -10.4913, -46.8403]])\n",
      "b.grad= tensor([-114.4723,  398.4673])\n",
      "new w tensor([[-0.4931,  0.6496,  1.1483],\n",
      "        [-0.1289,  0.9344,  0.4680]], requires_grad=True)\n",
      "new b tensor([ 0.0559, -2.5848], requires_grad=True)\n",
      "Loss= tensor(25.9452, grad_fn=<DivBackward0>)\n",
      "i= 109\n",
      "w.grad= tensor([[  1.8741, -26.6287,  42.3693],\n",
      "        [ 36.5743,  -9.7203, -46.5489]])\n",
      "b.grad= tensor([-114.1628,  398.4441])\n",
      "new w tensor([[-0.4932,  0.6509,  1.1462],\n",
      "        [-0.1307,  0.9349,  0.4703]], requires_grad=True)\n",
      "new b tensor([ 0.0616, -2.6047], requires_grad=True)\n",
      "Loss= tensor(25.6362, grad_fn=<DivBackward0>)\n",
      "i= 110\n",
      "w.grad= tensor([[  1.5650, -26.2421,  42.2055],\n",
      "        [ 35.5836,  -8.9748, -46.2610]])\n",
      "b.grad= tensor([-113.8569,  398.4148])\n",
      "new w tensor([[-0.4932,  0.6522,  1.1440],\n",
      "        [-0.1325,  0.9353,  0.4726]], requires_grad=True)\n",
      "new b tensor([ 0.0673, -2.6247], requires_grad=True)\n",
      "Loss= tensor(25.3351, grad_fn=<DivBackward0>)\n",
      "i= 111\n",
      "w.grad= tensor([[  1.2660, -25.8666,  42.0409],\n",
      "        [ 34.6233,  -8.2547, -45.9770]])\n",
      "b.grad= tensor([-113.5543,  398.3797])\n",
      "new w tensor([[-0.4933,  0.6535,  1.1419],\n",
      "        [-0.1342,  0.9357,  0.4749]], requires_grad=True)\n",
      "new b tensor([ 0.0730, -2.6446], requires_grad=True)\n",
      "Loss= tensor(25.0413, grad_fn=<DivBackward0>)\n",
      "i= 112\n",
      "w.grad= tensor([[  0.9790, -25.4996,  41.8770],\n",
      "        [ 33.6918,  -7.5598, -45.6970]])\n",
      "b.grad= tensor([-113.2550,  398.3390])\n",
      "new w tensor([[-0.4933,  0.6548,  1.1398],\n",
      "        [-0.1359,  0.9361,  0.4772]], requires_grad=True)\n",
      "new b tensor([ 0.0786, -2.6645], requires_grad=True)\n",
      "Loss= tensor(24.7545, grad_fn=<DivBackward0>)\n",
      "i= 113\n",
      "w.grad= tensor([[  0.7014, -25.1430,  41.7122],\n",
      "        [ 32.7882,  -6.8894, -45.4210]])\n",
      "b.grad= tensor([-112.9590,  398.2928])\n",
      "new w tensor([[-0.4934,  0.6560,  1.1378],\n",
      "        [-0.1376,  0.9365,  0.4795]], requires_grad=True)\n",
      "new b tensor([ 0.0843, -2.6844], requires_grad=True)\n",
      "Loss= tensor(24.4743, grad_fn=<DivBackward0>)\n",
      "i= 114\n",
      "w.grad= tensor([[  0.4333, -24.7962,  41.5470],\n",
      "        [ 31.9116,  -6.2427, -45.1490]])\n",
      "b.grad= tensor([-112.6661,  398.2414])\n",
      "new w tensor([[-0.4934,  0.6573,  1.1357],\n",
      "        [-0.1392,  0.9368,  0.4817]], requires_grad=True)\n",
      "new b tensor([ 0.0899, -2.7043], requires_grad=True)\n",
      "Loss= tensor(24.2006, grad_fn=<DivBackward0>)\n",
      "i= 115\n",
      "w.grad= tensor([[  0.1756, -24.4575,  41.3822],\n",
      "        [ 31.0635,  -5.6164, -44.8793]])\n",
      "b.grad= tensor([-112.3763,  398.1850])\n",
      "new w tensor([[-0.4934,  0.6585,  1.1336],\n",
      "        [-0.1407,  0.9371,  0.4840]], requires_grad=True)\n",
      "new b tensor([ 0.0955, -2.7242], requires_grad=True)\n",
      "Loss= tensor(23.9329, grad_fn=<DivBackward0>)\n",
      "i= 116\n",
      "w.grad= tensor([[ -0.0733, -24.1280,  41.2170],\n",
      "        [ 30.2394,  -5.0137, -44.6142]])\n",
      "b.grad= tensor([-112.0895,  398.1236])\n",
      "new w tensor([[-0.4934,  0.6597,  1.1316],\n",
      "        [-0.1422,  0.9373,  0.4862]], requires_grad=True)\n",
      "new b tensor([ 0.1011, -2.7441], requires_grad=True)\n",
      "Loss= tensor(23.6711, grad_fn=<DivBackward0>)\n",
      "i= 117\n",
      "w.grad= tensor([[ -0.3124, -23.8061,  41.0523],\n",
      "        [ 29.4411,  -4.4310, -44.3518]])\n",
      "b.grad= tensor([-111.8056,  398.0575])\n",
      "new w tensor([[-0.4934,  0.6609,  1.1295],\n",
      "        [-0.1437,  0.9375,  0.4884]], requires_grad=True)\n",
      "new b tensor([ 0.1067, -2.7640], requires_grad=True)\n",
      "Loss= tensor(23.4147, grad_fn=<DivBackward0>)\n",
      "i= 118\n",
      "w.grad= tensor([[ -0.5436, -23.4933,  40.8868],\n",
      "        [ 28.6661,  -3.8698, -44.0934]])\n",
      "b.grad= tensor([-111.5246,  397.9869])\n",
      "new w tensor([[-0.4934,  0.6621,  1.1275],\n",
      "        [-0.1451,  0.9377,  0.4906]], requires_grad=True)\n",
      "new b tensor([ 0.1123, -2.7839], requires_grad=True)\n",
      "Loss= tensor(23.1636, grad_fn=<DivBackward0>)\n",
      "i= 119\n",
      "w.grad= tensor([[ -0.7651, -23.1870,  40.7223],\n",
      "        [ 27.9153,  -3.3272, -43.8375]])\n",
      "b.grad= tensor([-111.2464,  397.9119])\n",
      "new w tensor([[-0.4933,  0.6632,  1.1254],\n",
      "        [-0.1465,  0.9379,  0.4928]], requires_grad=True)\n",
      "new b tensor([ 0.1179, -2.8038], requires_grad=True)\n",
      "Loss= tensor(22.9176, grad_fn=<DivBackward0>)\n",
      "i= 120\n",
      "w.grad= tensor([[ -0.9786, -22.8886,  40.5577],\n",
      "        [ 27.1868,  -2.8044, -43.5850]])\n",
      "b.grad= tensor([-110.9709,  397.8327])\n",
      "new w tensor([[-0.4933,  0.6644,  1.1234],\n",
      "        [-0.1479,  0.9380,  0.4950]], requires_grad=True)\n",
      "new b tensor([ 0.1234, -2.8237], requires_grad=True)\n",
      "Loss= tensor(22.6764, grad_fn=<DivBackward0>)\n",
      "i= 121\n",
      "w.grad= tensor([[ -1.1853, -22.5989,  40.3923],\n",
      "        [ 26.4808,  -2.2990, -43.3352]])\n",
      "b.grad= tensor([-110.6981,  397.7494])\n",
      "new w tensor([[-0.4932,  0.6655,  1.1214],\n",
      "        [-0.1492,  0.9381,  0.4972]], requires_grad=True)\n",
      "new b tensor([ 0.1290, -2.8436], requires_grad=True)\n",
      "Loss= tensor(22.4399, grad_fn=<DivBackward0>)\n",
      "i= 122\n",
      "w.grad= tensor([[ -1.3837, -22.3156,  40.2275],\n",
      "        [ 25.7948,  -1.8132, -43.0893]])\n",
      "b.grad= tensor([-110.4279,  397.6622])\n",
      "new w tensor([[-0.4931,  0.6666,  1.1194],\n",
      "        [-0.1505,  0.9382,  0.4993]], requires_grad=True)\n",
      "new b tensor([ 0.1345, -2.8635], requires_grad=True)\n",
      "Loss= tensor(22.2077, grad_fn=<DivBackward0>)\n",
      "i= 123\n",
      "w.grad= tensor([[ -1.5745, -22.0391,  40.0628],\n",
      "        [ 25.1308,  -1.3431, -42.8453]])\n",
      "b.grad= tensor([-110.1603,  397.5713])\n",
      "new w tensor([[-0.4931,  0.6677,  1.1174],\n",
      "        [-0.1518,  0.9383,  0.5014]], requires_grad=True)\n",
      "new b tensor([ 0.1400, -2.8834], requires_grad=True)\n",
      "Loss= tensor(21.9798, grad_fn=<DivBackward0>)\n",
      "i= 124\n",
      "w.grad= tensor([[ -1.7579, -21.7692,  39.8984],\n",
      "        [ 24.4861,  -0.8901, -42.6045]])\n",
      "b.grad= tensor([-109.8951,  397.4767])\n",
      "new w tensor([[-0.4930,  0.6688,  1.1154],\n",
      "        [-0.1530,  0.9383,  0.5036]], requires_grad=True)\n",
      "new b tensor([ 0.1455, -2.9032], requires_grad=True)\n",
      "Loss= tensor(21.7560, grad_fn=<DivBackward0>)\n",
      "i= 125\n",
      "w.grad= tensor([[ -1.9355, -21.5071,  39.7333],\n",
      "        [ 23.8602,  -0.4541, -42.3668]])\n",
      "b.grad= tensor([-109.6324,  397.3785])\n",
      "new w tensor([[-0.4929,  0.6699,  1.1134],\n",
      "        [-0.1542,  0.9384,  0.5057]], requires_grad=True)\n",
      "new b tensor([ 0.1510, -2.9231], requires_grad=True)\n",
      "Loss= tensor(21.5361, grad_fn=<DivBackward0>)\n",
      "i= 126\n",
      "w.grad= tensor([[-2.1052e+00, -2.1250e+01,  3.9569e+01],\n",
      "        [ 2.3254e+01, -3.3264e-02, -4.2132e+01]])\n",
      "b.grad= tensor([-109.3721,  397.2770])\n",
      "new w tensor([[-0.4928,  0.6709,  1.1114],\n",
      "        [-0.1553,  0.9384,  0.5078]], requires_grad=True)\n",
      "new b tensor([ 0.1564, -2.9430], requires_grad=True)\n",
      "Loss= tensor(21.3200, grad_fn=<DivBackward0>)\n",
      "i= 127\n",
      "w.grad= tensor([[ -2.2680, -20.9988,  39.4055],\n",
      "        [ 22.6644,   0.3716, -41.8993]])\n",
      "b.grad= tensor([-109.1140,  397.1723])\n",
      "new w tensor([[-0.4927,  0.6720,  1.1094],\n",
      "        [-0.1565,  0.9384,  0.5099]], requires_grad=True)\n",
      "new b tensor([ 0.1619, -2.9628], requires_grad=True)\n",
      "Loss= tensor(21.1074, grad_fn=<DivBackward0>)\n",
      "i= 128\n",
      "w.grad= tensor([[ -2.4260, -20.7551,  39.2411],\n",
      "        [ 22.0940,   0.7633, -41.6689]])\n",
      "b.grad= tensor([-108.8583,  397.0644])\n",
      "new w tensor([[-0.4925,  0.6730,  1.1075],\n",
      "        [-0.1576,  0.9383,  0.5120]], requires_grad=True)\n",
      "new b tensor([ 0.1673, -2.9827], requires_grad=True)\n",
      "Loss= tensor(20.8984, grad_fn=<DivBackward0>)\n",
      "i= 129\n",
      "w.grad= tensor([[ -2.5760, -20.5152,  39.0780],\n",
      "        [ 21.5396,   1.1396, -41.4416]])\n",
      "b.grad= tensor([-108.6048,  396.9535])\n",
      "new w tensor([[-0.4924,  0.6740,  1.1055],\n",
      "        [-0.1587,  0.9383,  0.5141]], requires_grad=True)\n",
      "new b tensor([ 0.1728, -3.0025], requires_grad=True)\n",
      "Loss= tensor(20.6926, grad_fn=<DivBackward0>)\n",
      "i= 130\n",
      "w.grad= tensor([[ -2.7224, -20.2835,  38.9137],\n",
      "        [ 21.0020,   1.5026, -41.2165]])\n",
      "b.grad= tensor([-108.3535,  396.8397])\n",
      "new w tensor([[-0.4923,  0.6751,  1.1036],\n",
      "        [-0.1597,  0.9382,  0.5161]], requires_grad=True)\n",
      "new b tensor([ 0.1782, -3.0224], requires_grad=True)\n",
      "Loss= tensor(20.4902, grad_fn=<DivBackward0>)\n",
      "i= 131\n",
      "w.grad= tensor([[ -2.8612, -20.0551,  38.7509],\n",
      "        [ 20.4805,   1.8524, -40.9939]])\n",
      "b.grad= tensor([-108.1043,  396.7231])\n",
      "new w tensor([[-0.4921,  0.6761,  1.1016],\n",
      "        [-0.1607,  0.9381,  0.5182]], requires_grad=True)\n",
      "new b tensor([ 0.1836, -3.0422], requires_grad=True)\n",
      "Loss= tensor(20.2908, grad_fn=<DivBackward0>)\n",
      "i= 132\n",
      "w.grad= tensor([[ -2.9947, -19.8323,  38.5882],\n",
      "        [ 19.9740,   2.1889, -40.7737]])\n",
      "b.grad= tensor([-107.8572,  396.6039])\n",
      "new w tensor([[-0.4920,  0.6771,  1.0997],\n",
      "        [-0.1617,  0.9380,  0.5202]], requires_grad=True)\n",
      "new b tensor([ 0.1890, -3.0620], requires_grad=True)\n",
      "Loss= tensor(20.0945, grad_fn=<DivBackward0>)\n",
      "i= 133\n",
      "w.grad= tensor([[ -3.1237, -19.6154,  38.4252],\n",
      "        [ 19.4823,   2.5125, -40.5561]])\n",
      "b.grad= tensor([-107.6122,  396.4820])\n",
      "new w tensor([[-0.4918,  0.6780,  1.0978],\n",
      "        [-0.1627,  0.9379,  0.5222]], requires_grad=True)\n",
      "new b tensor([ 0.1944, -3.0819], requires_grad=True)\n",
      "Loss= tensor(19.9010, grad_fn=<DivBackward0>)\n",
      "i= 134\n",
      "w.grad= tensor([[ -3.2467, -19.4026,  38.2630],\n",
      "        [ 19.0060,   2.8251, -40.3401]])\n",
      "b.grad= tensor([-107.3691,  396.3577])\n",
      "new w tensor([[-0.4917,  0.6790,  1.0959],\n",
      "        [-0.1637,  0.9377,  0.5242]], requires_grad=True)\n",
      "new b tensor([ 0.1997, -3.1017], requires_grad=True)\n",
      "Loss= tensor(19.7103, grad_fn=<DivBackward0>)\n",
      "i= 135\n",
      "w.grad= tensor([[ -3.3652, -19.1953,  38.1008],\n",
      "        [ 18.5428,   3.1251, -40.1269]])\n",
      "b.grad= tensor([-107.1280,  396.2309])\n",
      "new w tensor([[-0.4915,  0.6800,  1.0940],\n",
      "        [-0.1646,  0.9376,  0.5263]], requires_grad=True)\n",
      "new b tensor([ 0.2051, -3.1215], requires_grad=True)\n",
      "Loss= tensor(19.5224, grad_fn=<DivBackward0>)\n",
      "i= 136\n",
      "w.grad= tensor([[ -3.4791, -18.9928,  37.9389],\n",
      "        [ 18.0943,   3.4150, -39.9151]])\n",
      "b.grad= tensor([-106.8889,  396.1019])\n",
      "new w tensor([[-0.4913,  0.6809,  1.0921],\n",
      "        [-0.1655,  0.9374,  0.5283]], requires_grad=True)\n",
      "new b tensor([ 0.2104, -3.1413], requires_grad=True)\n",
      "Loss= tensor(19.3371, grad_fn=<DivBackward0>)\n",
      "i= 137\n",
      "w.grad= tensor([[ -3.5880, -18.7946,  37.7773],\n",
      "        [ 17.6579,   3.6927, -39.7062]])\n",
      "b.grad= tensor([-106.6516,  395.9707])\n",
      "new w tensor([[-0.4911,  0.6819,  1.0902],\n",
      "        [-0.1664,  0.9372,  0.5302]], requires_grad=True)\n",
      "new b tensor([ 0.2158, -3.1611], requires_grad=True)\n",
      "Loss= tensor(19.1543, grad_fn=<DivBackward0>)\n",
      "i= 138\n",
      "w.grad= tensor([[ -3.6915, -18.5998,  37.6167],\n",
      "        [ 17.2346,   3.9602, -39.4991]])\n",
      "b.grad= tensor([-106.4161,  395.8373])\n",
      "new w tensor([[-0.4910,  0.6828,  1.0883],\n",
      "        [-0.1672,  0.9370,  0.5322]], requires_grad=True)\n",
      "new b tensor([ 0.2211, -3.1809], requires_grad=True)\n",
      "Loss= tensor(18.9741, grad_fn=<DivBackward0>)\n",
      "i= 139\n",
      "w.grad= tensor([[ -3.7920, -18.4108,  37.4557],\n",
      "        [ 16.8234,   4.2171, -39.2943]])\n",
      "b.grad= tensor([-106.1825,  395.7019])\n",
      "new w tensor([[-0.4908,  0.6837,  1.0864],\n",
      "        [-0.1681,  0.9368,  0.5342]], requires_grad=True)\n",
      "new b tensor([ 0.2264, -3.2007], requires_grad=True)\n",
      "Loss= tensor(18.7962, grad_fn=<DivBackward0>)\n",
      "i= 140\n",
      "w.grad= tensor([[ -3.8875, -18.2252,  37.2953],\n",
      "        [ 16.4251,   4.4653, -39.0907]])\n",
      "b.grad= tensor([-105.9507,  395.5646])\n",
      "new w tensor([[-0.4906,  0.6846,  1.0846],\n",
      "        [-0.1689,  0.9366,  0.5361]], requires_grad=True)\n",
      "new b tensor([ 0.2317, -3.2205], requires_grad=True)\n",
      "Loss= tensor(18.6206, grad_fn=<DivBackward0>)\n",
      "i= 141\n",
      "w.grad= tensor([[ -3.9790, -18.0436,  37.1354],\n",
      "        [ 16.0375,   4.7027, -38.8896]])\n",
      "b.grad= tensor([-105.7205,  395.4254])\n",
      "new w tensor([[-0.4904,  0.6855,  1.0827],\n",
      "        [-0.1697,  0.9363,  0.5381]], requires_grad=True)\n",
      "new b tensor([ 0.2370, -3.2402], requires_grad=True)\n",
      "Loss= tensor(18.4473, grad_fn=<DivBackward0>)\n",
      "i= 142\n",
      "w.grad= tensor([[ -4.0666, -17.8660,  36.9758],\n",
      "        [ 15.6613,   4.9312, -38.6905]])\n",
      "b.grad= tensor([-105.4921,  395.2843])\n",
      "new w tensor([[-0.4902,  0.6864,  1.0808],\n",
      "        [-0.1705,  0.9361,  0.5400]], requires_grad=True)\n",
      "new b tensor([ 0.2422, -3.2600], requires_grad=True)\n",
      "Loss= tensor(18.2763, grad_fn=<DivBackward0>)\n",
      "i= 143\n",
      "w.grad= tensor([[ -4.1508, -17.6928,  36.8162],\n",
      "        [ 15.2965,   5.1510, -38.4928]])\n",
      "b.grad= tensor([-105.2654,  395.1416])\n",
      "new w tensor([[-0.4900,  0.6873,  1.0790],\n",
      "        [-0.1712,  0.9358,  0.5419]], requires_grad=True)\n",
      "new b tensor([ 0.2475, -3.2797], requires_grad=True)\n",
      "Loss= tensor(18.1073, grad_fn=<DivBackward0>)\n",
      "i= 144\n",
      "w.grad= tensor([[ -4.2302, -17.5219,  36.6578],\n",
      "        [ 14.9419,   5.3620, -38.2971]])\n",
      "b.grad= tensor([-105.0403,  394.9972])\n",
      "new w tensor([[-0.4898,  0.6882,  1.0772],\n",
      "        [-0.1720,  0.9356,  0.5438]], requires_grad=True)\n",
      "new b tensor([ 0.2528, -3.2995], requires_grad=True)\n",
      "Loss= tensor(17.9404, grad_fn=<DivBackward0>)\n",
      "i= 145\n",
      "w.grad= tensor([[ -4.3071, -17.3559,  36.4991],\n",
      "        [ 14.5973,   5.5643, -38.1034]])\n",
      "b.grad= tensor([-104.8168,  394.8512])\n",
      "new w tensor([[-0.4895,  0.6890,  1.0754],\n",
      "        [-0.1727,  0.9353,  0.5458]], requires_grad=True)\n",
      "new b tensor([ 0.2580, -3.3192], requires_grad=True)\n",
      "Loss= tensor(17.7756, grad_fn=<DivBackward0>)\n",
      "i= 146\n",
      "w.grad= tensor([[ -4.3801, -17.1928,  36.3411],\n",
      "        [ 14.2629,   5.7585, -37.9113]])\n",
      "b.grad= tensor([-104.5949,  394.7037])\n",
      "new w tensor([[-0.4893,  0.6899,  1.0735],\n",
      "        [-0.1734,  0.9350,  0.5477]], requires_grad=True)\n",
      "new b tensor([ 0.2632, -3.3390], requires_grad=True)\n",
      "Loss= tensor(17.6128, grad_fn=<DivBackward0>)\n",
      "i= 147\n",
      "w.grad= tensor([[ -4.4498, -17.0331,  36.1835],\n",
      "        [ 13.9382,   5.9454, -37.7208]])\n",
      "b.grad= tensor([-104.3745,  394.5547])\n",
      "new w tensor([[-0.4891,  0.6908,  1.0717],\n",
      "        [-0.1741,  0.9347,  0.5495]], requires_grad=True)\n",
      "new b tensor([ 0.2685, -3.3587], requires_grad=True)\n",
      "Loss= tensor(17.4519, grad_fn=<DivBackward0>)\n",
      "i= 148\n",
      "w.grad= tensor([[ -4.5160, -16.8766,  36.0266],\n",
      "        [ 13.6226,   6.1243, -37.5321]])\n",
      "b.grad= tensor([-104.1557,  394.4044])\n",
      "new w tensor([[-0.4889,  0.6916,  1.0699],\n",
      "        [-0.1748,  0.9344,  0.5514]], requires_grad=True)\n",
      "new b tensor([ 0.2737, -3.3784], requires_grad=True)\n",
      "Loss= tensor(17.2929, grad_fn=<DivBackward0>)\n",
      "i= 149\n",
      "w.grad= tensor([[ -4.5797, -16.7237,  35.8696],\n",
      "        [ 13.3163,   6.2962, -37.3447]])\n",
      "b.grad= tensor([-103.9383,  394.2527])\n",
      "new w tensor([[-0.4886,  0.6924,  1.0681],\n",
      "        [-0.1755,  0.9341,  0.5533]], requires_grad=True)\n",
      "new b tensor([ 0.2789, -3.3981], requires_grad=True)\n",
      "Loss= tensor(17.1357, grad_fn=<DivBackward0>)\n",
      "i= 150\n",
      "w.grad= tensor([[ -4.6399, -16.5736,  35.7133],\n",
      "        [ 13.0180,   6.4602, -37.1596]])\n",
      "b.grad= tensor([-103.7225,  394.0997])\n",
      "new w tensor([[-0.4884,  0.6933,  1.0663],\n",
      "        [-0.1761,  0.9338,  0.5551]], requires_grad=True)\n",
      "new b tensor([ 0.2840, -3.4178], requires_grad=True)\n",
      "Loss= tensor(16.9803, grad_fn=<DivBackward0>)\n",
      "i= 151\n",
      "w.grad= tensor([[ -4.6970, -16.4262,  35.5576],\n",
      "        [ 12.7291,   6.6182, -36.9754]])\n",
      "b.grad= tensor([-103.5080,  393.9456])\n",
      "new w tensor([[-0.4882,  0.6941,  1.0646],\n",
      "        [-0.1768,  0.9334,  0.5570]], requires_grad=True)\n",
      "new b tensor([ 0.2892, -3.4375], requires_grad=True)\n",
      "Loss= tensor(16.8267, grad_fn=<DivBackward0>)\n",
      "i= 152\n",
      "w.grad= tensor([[ -4.7512, -16.2815,  35.4026],\n",
      "        [ 12.4476,   6.7690, -36.7932]])\n",
      "b.grad= tensor([-103.2950,  393.7902])\n",
      "new w tensor([[-0.4879,  0.6949,  1.0628],\n",
      "        [-0.1774,  0.9331,  0.5588]], requires_grad=True)\n",
      "new b tensor([ 0.2944, -3.4572], requires_grad=True)\n",
      "Loss= tensor(16.6748, grad_fn=<DivBackward0>)\n",
      "i= 153\n",
      "w.grad= tensor([[ -4.8035, -16.1406,  35.2473],\n",
      "        [ 12.1742,   6.9131, -36.6125]])\n",
      "b.grad= tensor([-103.0834,  393.6338])\n",
      "new w tensor([[-0.4877,  0.6957,  1.0610],\n",
      "        [-0.1780,  0.9327,  0.5607]], requires_grad=True)\n",
      "new b tensor([ 0.2995, -3.4769], requires_grad=True)\n",
      "Loss= tensor(16.5246, grad_fn=<DivBackward0>)\n",
      "i= 154\n",
      "w.grad= tensor([[ -4.8524, -16.0017,  35.0931],\n",
      "        [ 11.9091,   7.0518, -36.4328]])\n",
      "b.grad= tensor([-102.8731,  393.4762])\n",
      "new w tensor([[-0.4875,  0.6965,  1.0593],\n",
      "        [-0.1786,  0.9324,  0.5625]], requires_grad=True)\n",
      "new b tensor([ 0.3047, -3.4966], requires_grad=True)\n",
      "Loss= tensor(16.3761, grad_fn=<DivBackward0>)\n",
      "i= 155\n",
      "w.grad= tensor([[ -4.8991, -15.8659,  34.9390],\n",
      "        [ 11.6509,   7.1841, -36.2550]])\n",
      "b.grad= tensor([-102.6642,  393.3177])\n",
      "new w tensor([[-0.4872,  0.6973,  1.0575],\n",
      "        [-0.1792,  0.9320,  0.5643]], requires_grad=True)\n",
      "new b tensor([ 0.3098, -3.5162], requires_grad=True)\n",
      "Loss= tensor(16.2291, grad_fn=<DivBackward0>)\n",
      "i= 156\n",
      "w.grad= tensor([[ -4.9432, -15.7324,  34.7856],\n",
      "        [ 11.4001,   7.3107, -36.0784]])\n",
      "b.grad= tensor([-102.4566,  393.1582])\n",
      "new w tensor([[-0.4870,  0.6981,  1.0558],\n",
      "        [-0.1797,  0.9317,  0.5661]], requires_grad=True)\n",
      "new b tensor([ 0.3149, -3.5359], requires_grad=True)\n",
      "Loss= tensor(16.0838, grad_fn=<DivBackward0>)\n",
      "i= 157\n",
      "w.grad= tensor([[ -4.9846, -15.6012,  34.6329],\n",
      "        [ 11.1559,   7.4311, -35.9036]])\n",
      "b.grad= tensor([-102.2503,  392.9978])\n",
      "new w tensor([[-0.4867,  0.6989,  1.0541],\n",
      "        [-0.1803,  0.9313,  0.5679]], requires_grad=True)\n",
      "new b tensor([ 0.3201, -3.5556], requires_grad=True)\n",
      "Loss= tensor(15.9401, grad_fn=<DivBackward0>)\n",
      "i= 158\n",
      "w.grad= tensor([[ -5.0240, -15.4726,  34.4805],\n",
      "        [ 10.9186,   7.5463, -35.7301]])\n",
      "b.grad= tensor([-102.0453,  392.8365])\n",
      "new w tensor([[-0.4865,  0.6996,  1.0523],\n",
      "        [-0.1808,  0.9309,  0.5697]], requires_grad=True)\n",
      "new b tensor([ 0.3252, -3.5752], requires_grad=True)\n",
      "Loss= tensor(15.7978, grad_fn=<DivBackward0>)\n",
      "i= 159\n",
      "w.grad= tensor([[ -5.0619, -15.3475,  34.3280],\n",
      "        [ 10.6886,   7.6571, -35.5574]])\n",
      "b.grad= tensor([-101.8416,  392.6744])\n",
      "new w tensor([[-0.4862,  0.7004,  1.0506],\n",
      "        [-0.1814,  0.9305,  0.5715]], requires_grad=True)\n",
      "new b tensor([ 0.3302, -3.5948], requires_grad=True)\n",
      "Loss= tensor(15.6571, grad_fn=<DivBackward0>)\n",
      "i= 160\n",
      "w.grad= tensor([[ -5.0961, -15.2226,  34.1772],\n",
      "        [ 10.4640,   7.7614, -35.3867]])\n",
      "b.grad= tensor([-101.6390,  392.5114])\n",
      "new w tensor([[-0.4860,  0.7012,  1.0489],\n",
      "        [-0.1819,  0.9301,  0.5732]], requires_grad=True)\n",
      "new b tensor([ 0.3353, -3.6145], requires_grad=True)\n",
      "Loss= tensor(15.5179, grad_fn=<DivBackward0>)\n",
      "i= 161\n",
      "w.grad= tensor([[ -5.1292, -15.1013,  34.0260],\n",
      "        [ 10.2464,   7.8621, -35.2168]])\n",
      "b.grad= tensor([-101.4377,  392.3478])\n",
      "new w tensor([[-0.4857,  0.7019,  1.0472],\n",
      "        [-0.1824,  0.9297,  0.5750]], requires_grad=True)\n",
      "new b tensor([ 0.3404, -3.6341], requires_grad=True)\n",
      "Loss= tensor(15.3801, grad_fn=<DivBackward0>)\n",
      "i= 162\n",
      "w.grad= tensor([[ -5.1604, -14.9821,  33.8755],\n",
      "        [ 10.0336,   7.9564, -35.0489]])\n",
      "b.grad= tensor([-101.2376,  392.1834])\n",
      "new w tensor([[-0.4854,  0.7027,  1.0455],\n",
      "        [-0.1829,  0.9294,  0.5767]], requires_grad=True)\n",
      "new b tensor([ 0.3455, -3.6537], requires_grad=True)\n",
      "Loss= tensor(15.2437, grad_fn=<DivBackward0>)\n",
      "i= 163\n",
      "w.grad= tensor([[ -5.1891, -14.8647,  33.7257],\n",
      "        [  9.8281,   8.0480, -34.8814]])\n",
      "b.grad= tensor([-101.0387,  392.0183])\n",
      "new w tensor([[-0.4852,  0.7034,  1.0438],\n",
      "        [-0.1834,  0.9289,  0.5785]], requires_grad=True)\n",
      "new b tensor([ 0.3505, -3.6733], requires_grad=True)\n",
      "Loss= tensor(15.1087, grad_fn=<DivBackward0>)\n",
      "i= 164\n",
      "w.grad= tensor([[ -5.2165, -14.7499,  33.5760],\n",
      "        [  9.6275,   8.1343, -34.7156]])\n",
      "b.grad= tensor([-100.8410,  391.8527])\n",
      "new w tensor([[-0.4849,  0.7042,  1.0422],\n",
      "        [-0.1839,  0.9285,  0.5802]], requires_grad=True)\n",
      "new b tensor([ 0.3556, -3.6929], requires_grad=True)\n",
      "Loss= tensor(14.9751, grad_fn=<DivBackward0>)\n",
      "i= 165\n",
      "w.grad= tensor([[ -5.2418, -14.6366,  33.4272],\n",
      "        [  9.4318,   8.2158, -34.5513]])\n",
      "b.grad= tensor([-100.6444,  391.6864])\n",
      "new w tensor([[-0.4847,  0.7049,  1.0405],\n",
      "        [-0.1844,  0.9281,  0.5819]], requires_grad=True)\n",
      "new b tensor([ 0.3606, -3.7125], requires_grad=True)\n",
      "Loss= tensor(14.8429, grad_fn=<DivBackward0>)\n",
      "i= 166\n",
      "w.grad= tensor([[ -5.2648, -14.5247,  33.2793],\n",
      "        [  9.2419,   8.2933, -34.3879]])\n",
      "b.grad= tensor([-100.4489,  391.5195])\n",
      "new w tensor([[-0.4844,  0.7056,  1.0388],\n",
      "        [-0.1848,  0.9277,  0.5837]], requires_grad=True)\n",
      "new b tensor([ 0.3656, -3.7320], requires_grad=True)\n",
      "Loss= tensor(14.7120, grad_fn=<DivBackward0>)\n",
      "i= 167\n",
      "w.grad= tensor([[ -5.2868, -14.4155,  33.1313],\n",
      "        [  9.0573,   8.3674, -34.2255]])\n",
      "b.grad= tensor([-100.2545,  391.3521])\n",
      "new w tensor([[-0.4841,  0.7063,  1.0372],\n",
      "        [-0.1853,  0.9273,  0.5854]], requires_grad=True)\n",
      "new b tensor([ 0.3706, -3.7516], requires_grad=True)\n",
      "Loss= tensor(14.5825, grad_fn=<DivBackward0>)\n",
      "i= 168\n",
      "w.grad= tensor([[ -5.3068, -14.3078,  32.9842],\n",
      "        [  8.8778,   8.4378, -34.0642]])\n",
      "b.grad= tensor([-100.0612,  391.1842])\n",
      "new w tensor([[-0.4839,  0.7070,  1.0355],\n",
      "        [-0.1857,  0.9269,  0.5871]], requires_grad=True)\n",
      "new b tensor([ 0.3756, -3.7712], requires_grad=True)\n",
      "Loss= tensor(14.4542, grad_fn=<DivBackward0>)\n",
      "i= 169\n",
      "w.grad= tensor([[ -5.3259, -14.2024,  32.8371],\n",
      "        [  8.7021,   8.5031, -33.9048]])\n",
      "b.grad= tensor([-99.8690, 391.0158])\n",
      "new w tensor([[-0.4836,  0.7078,  1.0339],\n",
      "        [-0.1862,  0.9265,  0.5888]], requires_grad=True)\n",
      "new b tensor([ 0.3806, -3.7907], requires_grad=True)\n",
      "Loss= tensor(14.3272, grad_fn=<DivBackward0>)\n",
      "i= 170\n",
      "w.grad= tensor([[ -5.3433, -14.0988,  32.6906],\n",
      "        [  8.5313,   8.5651, -33.7464]])\n",
      "b.grad= tensor([-99.6779, 390.8469])\n",
      "new w tensor([[-0.4833,  0.7085,  1.0322],\n",
      "        [-0.1866,  0.9260,  0.5905]], requires_grad=True)\n",
      "new b tensor([ 0.3856, -3.8103], requires_grad=True)\n",
      "Loss= tensor(14.2015, grad_fn=<DivBackward0>)\n",
      "i= 171\n",
      "w.grad= tensor([[ -5.3579, -13.9953,  32.5456],\n",
      "        [  8.3660,   8.6247, -33.5883]])\n",
      "b.grad= tensor([-99.4878, 390.6776])\n",
      "new w tensor([[-0.4831,  0.7092,  1.0306],\n",
      "        [-0.1870,  0.9256,  0.5921]], requires_grad=True)\n",
      "new b tensor([ 0.3906, -3.8298], requires_grad=True)\n",
      "Loss= tensor(14.0770, grad_fn=<DivBackward0>)\n",
      "i= 172\n",
      "w.grad= tensor([[ -5.3728, -13.8953,  32.4001],\n",
      "        [  8.2048,   8.6808, -33.4315]])\n",
      "b.grad= tensor([-99.2987, 390.5080])\n",
      "new w tensor([[-0.4828,  0.7099,  1.0290],\n",
      "        [-0.1874,  0.9252,  0.5938]], requires_grad=True)\n",
      "new b tensor([ 0.3955, -3.8493], requires_grad=True)\n",
      "Loss= tensor(13.9537, grad_fn=<DivBackward0>)\n",
      "i= 173\n",
      "w.grad= tensor([[ -5.3864, -13.7971,  32.2550],\n",
      "        [  8.0471,   8.7326, -33.2764]])\n",
      "b.grad= tensor([-99.1107, 390.3380])\n",
      "new w tensor([[-0.4825,  0.7105,  1.0274],\n",
      "        [-0.1878,  0.9247,  0.5955]], requires_grad=True)\n",
      "new b tensor([ 0.4005, -3.8688], requires_grad=True)\n",
      "Loss= tensor(13.8317, grad_fn=<DivBackward0>)\n",
      "i= 174\n",
      "w.grad= tensor([[ -5.3969, -13.6984,  32.1115],\n",
      "        [  7.8940,   8.7819, -33.1219]])\n",
      "b.grad= tensor([-98.9237, 390.1676])\n",
      "new w tensor([[-0.4823,  0.7112,  1.0258],\n",
      "        [-0.1882,  0.9243,  0.5971]], requires_grad=True)\n",
      "new b tensor([ 0.4054, -3.8883], requires_grad=True)\n",
      "Loss= tensor(13.7108, grad_fn=<DivBackward0>)\n",
      "i= 175\n",
      "w.grad= tensor([[ -5.4077, -13.6031,  31.9676],\n",
      "        [  7.7447,   8.8279, -32.9687]])\n",
      "b.grad= tensor([-98.7376, 389.9969])\n",
      "new w tensor([[-0.4820,  0.7119,  1.0242],\n",
      "        [-0.1886,  0.9238,  0.5988]], requires_grad=True)\n",
      "new b tensor([ 0.4104, -3.9078], requires_grad=True)\n",
      "Loss= tensor(13.5912, grad_fn=<DivBackward0>)\n",
      "i= 176\n",
      "w.grad= tensor([[ -5.4163, -13.5081,  31.8249],\n",
      "        [  7.6000,   8.8717, -32.8159]])\n",
      "b.grad= tensor([-98.5526, 389.8259])\n",
      "new w tensor([[-0.4817,  0.7126,  1.0226],\n",
      "        [-0.1890,  0.9234,  0.6004]], requires_grad=True)\n",
      "new b tensor([ 0.4153, -3.9273], requires_grad=True)\n",
      "Loss= tensor(13.4726, grad_fn=<DivBackward0>)\n",
      "i= 177\n",
      "w.grad= tensor([[ -5.4239, -13.4146,  31.6827],\n",
      "        [  7.4581,   8.9117, -32.6648]])\n",
      "b.grad= tensor([-98.3685, 389.6547])\n",
      "new w tensor([[-0.4814,  0.7133,  1.0210],\n",
      "        [-0.1893,  0.9230,  0.6021]], requires_grad=True)\n",
      "new b tensor([ 0.4202, -3.9468], requires_grad=True)\n",
      "Loss= tensor(13.3552, grad_fn=<DivBackward0>)\n",
      "i= 178\n",
      "w.grad= "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -5.4308, -13.3233,  31.5406],\n",
      "        [  7.3203,   8.9495, -32.5144]])\n",
      "b.grad= tensor([-98.1853, 389.4832])\n",
      "new w tensor([[-0.4812,  0.7139,  1.0194],\n",
      "        [-0.1897,  0.9225,  0.6037]], requires_grad=True)\n",
      "new b tensor([ 0.4251, -3.9663], requires_grad=True)\n",
      "Loss= tensor(13.2390, grad_fn=<DivBackward0>)\n",
      "i= 179\n",
      "w.grad= tensor([[ -5.4365, -13.2330,  31.3992],\n",
      "        [  7.1852,   8.9836, -32.3655]])\n",
      "b.grad= tensor([-98.0031, 389.3115])\n",
      "new w tensor([[-0.4809,  0.7146,  1.0178],\n",
      "        [-0.1901,  0.9221,  0.6053]], requires_grad=True)\n",
      "new b tensor([ 0.4300, -3.9858], requires_grad=True)\n",
      "Loss= tensor(13.1239, grad_fn=<DivBackward0>)\n",
      "i= 180\n",
      "w.grad= tensor([[ -5.4407, -13.1436,  31.2586],\n",
      "        [  7.0550,   9.0168, -32.2167]])\n",
      "b.grad= tensor([-97.8219, 389.1395])\n",
      "new w tensor([[-0.4806,  0.7152,  1.0163],\n",
      "        [-0.1904,  0.9216,  0.6069]], requires_grad=True)\n",
      "new b tensor([ 0.4349, -4.0052], requires_grad=True)\n",
      "Loss= tensor(13.0099, grad_fn=<DivBackward0>)\n",
      "i= 181\n",
      "w.grad= tensor([[ -5.4441, -13.0557,  31.1185],\n",
      "        [  6.9267,   9.0460, -32.0697]])\n",
      "b.grad= tensor([-97.6415, 388.9674])\n",
      "new w tensor([[-0.4804,  0.7159,  1.0147],\n",
      "        [-0.1908,  0.9212,  0.6085]], requires_grad=True)\n",
      "new b tensor([ 0.4398, -4.0247], requires_grad=True)\n",
      "Loss= tensor(12.8970, grad_fn=<DivBackward0>)\n",
      "i= 182\n",
      "w.grad= tensor([[ -5.4467, -12.9694,  30.9787],\n",
      "        [  6.8025,   9.0739, -31.9230]])\n",
      "b.grad= tensor([-97.4620, 388.7951])\n",
      "new w tensor([[-0.4801,  0.7165,  1.0132],\n",
      "        [-0.1911,  0.9207,  0.6101]], requires_grad=True)\n",
      "new b tensor([ 0.4447, -4.0441], requires_grad=True)\n",
      "Loss= tensor(12.7852, grad_fn=<DivBackward0>)\n",
      "i= 183\n",
      "w.grad= tensor([[ -5.4484, -12.8845,  30.8394],\n",
      "        [  6.6811,   9.0991, -31.7775]])\n",
      "b.grad= tensor([-97.2835, 388.6227])\n",
      "new w tensor([[-0.4798,  0.7172,  1.0116],\n",
      "        [-0.1914,  0.9202,  0.6117]], requires_grad=True)\n",
      "new b tensor([ 0.4495, -4.0635], requires_grad=True)\n",
      "Loss= tensor(12.6744, grad_fn=<DivBackward0>)\n",
      "i= 184\n",
      "w.grad= tensor([[ -5.4486, -12.7999,  30.7011],\n",
      "        [  6.5621,   9.1212, -31.6333]])\n",
      "b.grad= tensor([-97.1058, 388.4501])\n",
      "new w tensor([[-0.4795,  0.7178,  1.0101],\n",
      "        [-0.1918,  0.9198,  0.6133]], requires_grad=True)\n",
      "new b tensor([ 0.4544, -4.0830], requires_grad=True)\n",
      "Loss= tensor(12.5646, grad_fn=<DivBackward0>)\n",
      "i= 185\n",
      "w.grad= tensor([[ -5.4489, -12.7177,  30.5627],\n",
      "        [  6.4471,   9.1426, -31.4893]])\n",
      "b.grad= tensor([-96.9290, 388.2774])\n",
      "new w tensor([[-0.4793,  0.7185,  1.0086],\n",
      "        [-0.1921,  0.9193,  0.6149]], requires_grad=True)\n",
      "new b tensor([ 0.4592, -4.1024], requires_grad=True)\n",
      "Loss= tensor(12.4560, grad_fn=<DivBackward0>)\n",
      "i= 186\n",
      "w.grad= tensor([[ -5.4474, -12.6354,  30.4255],\n",
      "        [  6.3343,   9.1611, -31.3467]])\n",
      "b.grad= tensor([-96.7531, 388.1046])\n",
      "new w tensor([[-0.4790,  0.7191,  1.0071],\n",
      "        [-0.1924,  0.9189,  0.6164]], requires_grad=True)\n",
      "new b tensor([ 0.4641, -4.1218], requires_grad=True)\n",
      "Loss= tensor(12.3483, grad_fn=<DivBackward0>)\n",
      "i= 187\n",
      "w.grad= tensor([[ -5.4451, -12.5544,  30.2887],\n",
      "        [  6.2242,   9.1773, -31.2049]])\n",
      "b.grad= tensor([-96.5780, 387.9318])\n",
      "new w tensor([[-0.4787,  0.7197,  1.0055],\n",
      "        [-0.1927,  0.9184,  0.6180]], requires_grad=True)\n",
      "new b tensor([ 0.4689, -4.1412], requires_grad=True)\n",
      "Loss= tensor(12.2417, grad_fn=<DivBackward0>)\n",
      "i= 188\n",
      "w.grad= tensor([[ -5.4429, -12.4753,  30.1520],\n",
      "        [  6.1181,   9.1934, -31.0632]])\n",
      "b.grad= tensor([-96.4037, 387.7589])\n",
      "new w tensor([[-0.4785,  0.7203,  1.0040],\n",
      "        [-0.1930,  0.9180,  0.6195]], requires_grad=True)\n",
      "new b tensor([ 0.4737, -4.1606], requires_grad=True)\n",
      "Loss= tensor(12.1361, grad_fn=<DivBackward0>)\n",
      "i= 189\n",
      "w.grad= tensor([[ -5.4397, -12.3973,  30.0159],\n",
      "        [  6.0126,   9.2052, -30.9237]])\n",
      "b.grad= tensor([-96.2303, 387.5858])\n",
      "new w tensor([[-0.4782,  0.7210,  1.0025],\n",
      "        [-0.1933,  0.9175,  0.6211]], requires_grad=True)\n",
      "new b tensor([ 0.4785, -4.1799], requires_grad=True)\n",
      "Loss= tensor(12.0315, grad_fn=<DivBackward0>)\n",
      "i= 190\n",
      "w.grad= tensor([[ -5.4348, -12.3190,  29.8811],\n",
      "        [  5.9107,   9.2166, -30.7842]])\n",
      "b.grad= tensor([-96.0578, 387.4128])\n",
      "new w tensor([[-0.4779,  0.7216,  1.0010],\n",
      "        [-0.1936,  0.9170,  0.6226]], requires_grad=True)\n",
      "new b tensor([ 0.4833, -4.1993], requires_grad=True)\n",
      "Loss= tensor(11.9278, grad_fn=<DivBackward0>)\n",
      "i= 191\n",
      "w.grad= tensor([[ -5.4291, -12.2415,  29.7470],\n",
      "        [  5.8110,   9.2258, -30.6459]])\n",
      "b.grad= tensor([-95.8860, 387.2397])\n",
      "new w tensor([[-0.4776,  0.7222,  0.9995],\n",
      "        [-0.1939,  0.9166,  0.6242]], requires_grad=True)\n",
      "new b tensor([ 0.4881, -4.2187], requires_grad=True)\n",
      "Loss= tensor(11.8252, grad_fn=<DivBackward0>)\n",
      "i= 192\n",
      "w.grad= tensor([[ -5.4245, -12.1672,  29.6120],\n",
      "        [  5.7139,   9.2338, -30.5080]])\n",
      "b.grad= tensor([-95.7150, 387.0667])\n",
      "new w tensor([[-0.4774,  0.7228,  0.9981],\n",
      "        [-0.1942,  0.9161,  0.6257]], requires_grad=True)\n",
      "new b tensor([ 0.4929, -4.2380], requires_grad=True)\n",
      "Loss= tensor(11.7235, grad_fn=<DivBackward0>)\n",
      "i= 193\n",
      "w.grad= tensor([[ -5.4178, -12.0923,  29.4786],\n",
      "        [  5.6182,   9.2390, -30.3717]])\n",
      "b.grad= tensor([-95.5449, 386.8936])\n",
      "new w tensor([[-0.4771,  0.7234,  0.9966],\n",
      "        [-0.1945,  0.9156,  0.6272]], requires_grad=True)\n",
      "new b tensor([ 0.4977, -4.2574], requires_grad=True)\n",
      "Loss= tensor(11.6227, grad_fn=<DivBackward0>)\n",
      "i= 194\n",
      "w.grad= tensor([[ -5.4104, -12.0181,  29.3458],\n",
      "        [  5.5257,   9.2439, -30.2355]])\n",
      "b.grad= tensor([-95.3755, 386.7206])\n",
      "new w tensor([[-0.4768,  0.7240,  0.9951],\n",
      "        [-0.1948,  0.9152,  0.6287]], requires_grad=True)\n",
      "new b tensor([ 0.5025, -4.2767], requires_grad=True)\n",
      "Loss= tensor(11.5229, grad_fn=<DivBackward0>)\n",
      "i= 195\n",
      "w.grad= tensor([[ -5.4033, -11.9457,  29.2131],\n",
      "        [  5.4352,   9.2470, -30.1002]])\n",
      "b.grad= tensor([-95.2069, 386.5475])\n",
      "new w tensor([[-0.4766,  0.7246,  0.9937],\n",
      "        [-0.1950,  0.9147,  0.6302]], requires_grad=True)\n",
      "new b tensor([ 0.5072, -4.2960], requires_grad=True)\n",
      "Loss= tensor(11.4240, grad_fn=<DivBackward0>)\n",
      "i= 196\n",
      "w.grad= tensor([[ -5.3948, -11.8733,  29.0814],\n",
      "        [  5.3461,   9.2479, -29.9662]])\n",
      "b.grad= tensor([-95.0390, 386.3745])\n",
      "new w tensor([[-0.4763,  0.7252,  0.9922],\n",
      "        [-0.1953,  0.9143,  0.6317]], requires_grad=True)\n",
      "new b tensor([ 0.5120, -4.3153], requires_grad=True)\n",
      "Loss= tensor(11.3261, grad_fn=<DivBackward0>)\n",
      "i= 197\n",
      "w.grad= tensor([[ -5.3862, -11.8026,  28.9498],\n",
      "        [  5.2595,   9.2479, -29.8326]])\n",
      "b.grad= tensor([-94.8720, 386.2016])\n",
      "new w tensor([[-0.4760,  0.7258,  0.9908],\n",
      "        [-0.1956,  0.9138,  0.6332]], requires_grad=True)\n",
      "new b tensor([ 0.5167, -4.3347], requires_grad=True)\n",
      "Loss= tensor(11.2291, grad_fn=<DivBackward0>)\n",
      "i= 198\n",
      "w.grad= tensor([[ -5.3767, -11.7319,  28.8192],\n",
      "        [  5.1746,   9.2464, -29.7000]])\n",
      "b.grad= tensor([-94.7057, 386.0287])\n",
      "new w tensor([[-0.4757,  0.7264,  0.9893],\n",
      "        [-0.1958,  0.9133,  0.6347]], requires_grad=True)\n",
      "new b tensor([ 0.5215, -4.3540], requires_grad=True)\n",
      "Loss= tensor(11.1329, grad_fn=<DivBackward0>)\n",
      "i= 199\n",
      "w.grad= tensor([[ -5.3667, -11.6623,  28.6891],\n",
      "        [  5.0930,   9.2452, -29.5672]])\n",
      "b.grad= tensor([-94.5401, 385.8559])\n",
      "new w tensor([[-0.4755,  0.7270,  0.9879],\n",
      "        [-0.1961,  0.9129,  0.6362]], requires_grad=True)\n",
      "new b tensor([ 0.5262, -4.3733], requires_grad=True)\n",
      "Loss= tensor(11.0377, grad_fn=<DivBackward0>)\n",
      "i= 200\n",
      "w.grad= tensor([[ -5.3571, -11.5945,  28.5588],\n",
      "        [  5.0113,   9.2402, -29.4365]])\n",
      "b.grad= tensor([-94.3753, 385.6831])\n",
      "new w tensor([[-0.4752,  0.7275,  0.9865],\n",
      "        [-0.1963,  0.9124,  0.6376]], requires_grad=True)\n",
      "new b tensor([ 0.5309, -4.3925], requires_grad=True)\n",
      "Loss= tensor(10.9434, grad_fn=<DivBackward0>)\n",
      "i= 201\n",
      "w.grad= tensor([[ -5.3461, -11.5262,  28.4298],\n",
      "        [  4.9324,   9.2355, -29.3059]])\n",
      "b.grad= tensor([-94.2112, 385.5104])\n",
      "new w tensor([[-0.4749,  0.7281,  0.9850],\n",
      "        [-0.1966,  0.9120,  0.6391]], requires_grad=True)\n",
      "new b tensor([ 0.5356, -4.4118], requires_grad=True)\n",
      "Loss= tensor(10.8499, grad_fn=<DivBackward0>)\n",
      "i= 202\n",
      "w.grad= tensor([[ -5.3348, -11.4592,  28.3012],\n",
      "        [  4.8552,   9.2292, -29.1761]])\n",
      "b.grad= tensor([-94.0478, 385.3379])\n",
      "new w tensor([[-0.4747,  0.7287,  0.9836],\n",
      "        [-0.1968,  0.9115,  0.6406]], requires_grad=True)\n",
      "new b tensor([ 0.5403, -4.4311], requires_grad=True)\n",
      "Loss= tensor(10.7573, grad_fn=<DivBackward0>)\n",
      "i= 203\n",
      "w.grad= tensor([[ -5.3232, -11.3929,  28.1730],\n",
      "        [  4.7799,   9.2223, -29.0469]])\n",
      "b.grad= tensor([-93.8852, 385.1654])\n",
      "new w tensor([[-0.4744,  0.7293,  0.9822],\n",
      "        [-0.1971,  0.9110,  0.6420]], requires_grad=True)\n",
      "new b tensor([ 0.5450, -4.4503], requires_grad=True)\n",
      "Loss= tensor(10.6656, grad_fn=<DivBackward0>)\n",
      "i= 204\n",
      "w.grad= tensor([[ -5.3107, -11.3269,  28.0457],\n",
      "        [  4.7055,   9.2137, -28.9188]])\n",
      "b.grad= tensor([-93.7232, 384.9930])\n",
      "new w tensor([[-0.4741,  0.7298,  0.9808],\n",
      "        [-0.1973,  0.9106,  0.6435]], requires_grad=True)\n",
      "new b tensor([ 0.5497, -4.4696], requires_grad=True)\n",
      "Loss= tensor(10.5747, grad_fn=<DivBackward0>)\n",
      "i= 205\n",
      "w.grad= tensor([[ -5.2982, -11.2620,  27.9187],\n",
      "        [  4.6332,   9.2042, -28.7911]])\n",
      "b.grad= tensor([-93.5620, 384.8208])\n",
      "new w tensor([[-0.4739,  0.7304,  0.9794],\n",
      "        [-0.1975,  0.9101,  0.6449]], requires_grad=True)\n",
      "new b tensor([ 0.5544, -4.4888], requires_grad=True)\n",
      "Loss= tensor(10.4847, grad_fn=<DivBackward0>)\n",
      "i= 206\n",
      "w.grad= tensor([[ -5.2858, -11.1984,  27.7919],\n",
      "        [  4.5619,   9.1933, -28.6645]])\n",
      "b.grad= tensor([-93.4015, 384.6487])\n",
      "new w tensor([[-0.4736,  0.7309,  0.9780],\n",
      "        [-0.1978,  0.9096,  0.6463]], requires_grad=True)\n",
      "new b tensor([ 0.5591, -4.5081], requires_grad=True)\n",
      "Loss= tensor(10.3955, grad_fn=<DivBackward0>)\n",
      "i= 207\n",
      "w.grad= tensor([[ -5.2721, -11.1343,  27.6663],\n",
      "        [  4.4932,   9.1825, -28.5381]])\n",
      "b.grad= tensor([-93.2416, 384.4767])\n",
      "new w tensor([[-0.4734,  0.7315,  0.9766],\n",
      "        [-0.1980,  0.9092,  0.6478]], requires_grad=True)\n",
      "new b tensor([ 0.5637, -4.5273], requires_grad=True)\n",
      "Loss= tensor(10.3072, grad_fn=<DivBackward0>)\n",
      "i= 208\n",
      "w.grad= tensor([[ -5.2587, -11.0718,  27.5407],\n",
      "        [  4.4251,   9.1701, -28.4126]])\n",
      "b.grad= tensor([-93.0825, 384.3049])\n",
      "new w tensor([[-0.4731,  0.7321,  0.9753],\n",
      "        [-0.1982,  0.9087,  0.6492]], requires_grad=True)\n",
      "new b tensor([ 0.5684, -4.5465], requires_grad=True)\n",
      "Loss= tensor(10.2196, grad_fn=<DivBackward0>)\n",
      "i= 209\n",
      "w.grad= tensor([[ -5.2442, -11.0091,  27.4162],\n",
      "        [  4.3586,   9.1569, -28.2878]])\n",
      "b.grad= tensor([-92.9240, 384.1332])\n",
      "new w tensor([[-0.4728,  0.7326,  0.9739],\n",
      "        [-0.1984,  0.9083,  0.6506]], requires_grad=True)\n",
      "new b tensor([ 0.5730, -4.5657], requires_grad=True)\n",
      "Loss= tensor(10.1328, grad_fn=<DivBackward0>)\n",
      "i= 210\n",
      "w.grad= tensor([[ -5.2302, -10.9478,  27.2916],\n",
      "        [  4.2937,   9.1429, -28.1637]])\n",
      "b.grad= tensor([-92.7662, 383.9616])\n",
      "new w tensor([[-0.4726,  0.7332,  0.9725],\n",
      "        [-0.1986,  0.9078,  0.6520]], requires_grad=True)\n",
      "new b tensor([ 0.5777, -4.5849], requires_grad=True)\n",
      "Loss= tensor(10.0469, grad_fn=<DivBackward0>)\n",
      "i= 211\n",
      "w.grad= tensor([[ -5.2151, -10.8862,  27.1682],\n",
      "        [  4.2293,   9.1274, -28.0407]])\n",
      "b.grad= tensor([-92.6090, 383.7903])\n",
      "new w tensor([[-0.4723,  0.7337,  0.9712],\n",
      "        [-0.1988,  0.9074,  0.6534]], requires_grad=True)\n",
      "new b tensor([ 0.5823, -4.6041], requires_grad=True)\n",
      "Loss= tensor(9.9618, grad_fn=<DivBackward0>)\n",
      "i= 212\n",
      "w.grad= tensor([[ -5.2007, -10.8265,  27.0446],\n",
      "        [  4.1667,   9.1114, -27.9180]])\n",
      "b.grad= tensor([-92.4526, 383.6190])\n",
      "new w tensor([[-0.4720,  0.7342,  0.9698],\n",
      "        [-0.1991,  0.9069,  0.6548]], requires_grad=True)\n",
      "new b tensor([ 0.5869, -4.6233], requires_grad=True)\n",
      "Loss= tensor(9.8774, grad_fn=<DivBackward0>)\n",
      "i= 213\n",
      "w.grad= tensor([[ -5.1857, -10.7669,  26.9218],\n",
      "        [  4.1062,   9.0957, -27.7955]])\n",
      "b.grad= tensor([-92.2968, 383.4480])\n",
      "new w tensor([[-0.4718,  0.7348,  0.9685],\n",
      "        [-0.1993,  0.9064,  0.6562]], requires_grad=True)\n",
      "new b tensor([ 0.5915, -4.6424], requires_grad=True)\n",
      "Loss= tensor(9.7938, grad_fn=<DivBackward0>)\n",
      "i= 214\n",
      "w.grad= tensor([[ -5.1696, -10.7070,  26.8000],\n",
      "        [  4.0460,   9.0784, -27.6742]])\n",
      "b.grad= tensor([-92.1416, 383.2771])\n",
      "new w tensor([[-0.4715,  0.7353,  0.9671],\n",
      "        [-0.1995,  0.9060,  0.6576]], requires_grad=True)\n",
      "new b tensor([ 0.5961, -4.6616], requires_grad=True)\n",
      "Loss= tensor(9.7110, grad_fn=<DivBackward0>)\n",
      "i= 215\n",
      "w.grad= tensor([[ -5.1538, -10.6485,  26.6784],\n",
      "        [  3.9871,   9.0605, -27.5535]])\n",
      "b.grad= tensor([-91.9871, 383.1064])\n",
      "new w tensor([[-0.4713,  0.7358,  0.9658],\n",
      "        [-0.1997,  0.9055,  0.6590]], requires_grad=True)\n",
      "new b tensor([ 0.6007, -4.6808], requires_grad=True)\n",
      "Loss= tensor(9.6290, grad_fn=<DivBackward0>)\n",
      "i= 216\n",
      "w.grad= tensor([[ -5.1374, -10.5899,  26.5575],\n",
      "        [  3.9295,   9.0419, -27.4334]])\n",
      "b.grad= tensor([-91.8332, 382.9359])\n",
      "new w tensor([[-0.4710,  0.7364,  0.9645],\n",
      "        [-0.1999,  0.9051,  0.6603]], requires_grad=True)\n",
      "new b tensor([ 0.6053, -4.6999], requires_grad=True)\n",
      "Loss= tensor(9.5477, grad_fn=<DivBackward0>)\n",
      "i= 217\n",
      "w.grad= tensor([[ -5.1220, -10.5333,  26.4364],\n",
      "        [  3.8731,   9.0228, -27.3138]])\n",
      "b.grad= tensor([-91.6799, 382.7656])\n",
      "new w tensor([[-0.4708,  0.7369,  0.9632],\n",
      "        [-0.2000,  0.9046,  0.6617]], requires_grad=True)\n",
      "new b tensor([ 0.6099, -4.7191], requires_grad=True)\n",
      "Loss= tensor(9.4672, grad_fn=<DivBackward0>)\n",
      "i= 218\n",
      "w.grad= tensor([[ -5.1053, -10.4763,  26.3164],\n",
      "        [  3.8173,   9.0027, -27.1952]])\n",
      "b.grad= tensor([-91.5273, 382.5955])\n",
      "new w tensor([[-0.4705,  0.7374,  0.9618],\n",
      "        [-0.2002,  0.9042,  0.6631]], requires_grad=True)\n",
      "new b tensor([ 0.6145, -4.7382], requires_grad=True)\n",
      "Loss= tensor(9.3874, grad_fn=<DivBackward0>)\n",
      "i= 219\n",
      "w.grad= tensor([[ -5.0888, -10.4200,  26.1968],\n",
      "        [  3.7632,   8.9827, -27.0767]])\n",
      "b.grad= tensor([-91.3753, 382.4255])\n",
      "new w tensor([[-0.4702,  0.7379,  0.9605],\n",
      "        [-0.2004,  0.9037,  0.6644]], requires_grad=True)\n",
      "new b tensor([ 0.6190, -4.7573], requires_grad=True)\n",
      "Loss= tensor(9.3084, grad_fn=<DivBackward0>)\n",
      "i= 220\n",
      "w.grad= tensor([[ -5.0710, -10.3630,  26.0784],\n",
      "        [  3.7100,   8.9618, -26.9591]])\n",
      "b.grad= tensor([-91.2239, 382.2558])\n",
      "new w tensor([[-0.4700,  0.7385,  0.9592],\n",
      "        [-0.2006,  0.9033,  0.6658]], requires_grad=True)\n",
      "new b tensor([ 0.6236, -4.7764], requires_grad=True)\n",
      "Loss= tensor(9.2301, grad_fn=<DivBackward0>)\n",
      "i= 221\n",
      "w.grad= tensor([[ -5.0544, -10.3083,  25.9596],\n",
      "        [  3.6575,   8.9402, -26.8422]])\n",
      "b.grad= tensor([-91.0732, 382.0863])\n",
      "new w tensor([[-0.4697,  0.7390,  0.9579],\n",
      "        [-0.2008,  0.9028,  0.6671]], requires_grad=True)\n",
      "new b tensor([ 0.6282, -4.7955], requires_grad=True)\n",
      "Loss= tensor(9.1525, grad_fn=<DivBackward0>)\n",
      "i= 222\n",
      "w.grad= tensor([[ -5.0367, -10.2527,  25.8419],\n",
      "        [  3.6054,   8.9174, -26.7264]])\n",
      "b.grad= tensor([-90.9230, 381.9170])\n",
      "new w tensor([[-0.4695,  0.7395,  0.9566],\n",
      "        [-0.2010,  0.9024,  0.6684]], requires_grad=True)\n",
      "new b tensor([ 0.6327, -4.8146], requires_grad=True)\n",
      "Loss= tensor(9.0756, grad_fn=<DivBackward0>)\n",
      "i= 223\n",
      "w.grad= tensor([[ -5.0195, -10.1988,  25.7242],\n",
      "        [  3.5555,   8.8955, -26.6102]])\n",
      "b.grad= tensor([-90.7734, 381.7479])\n",
      "new w tensor([[-0.4692,  0.7400,  0.9553],\n",
      "        [-0.2012,  0.9020,  0.6698]], requires_grad=True)\n",
      "new b tensor([ 0.6372, -4.8337], requires_grad=True)\n",
      "Loss= tensor(8.9994, grad_fn=<DivBackward0>)\n",
      "i= 224\n",
      "w.grad= tensor([[ -5.0019, -10.1449,  25.6073],\n",
      "        [  3.5056,   8.8720, -26.4953]])\n",
      "b.grad= tensor([-90.6245, 381.5790])\n",
      "new w tensor([[-0.4690,  0.7405,  0.9541],\n",
      "        [-0.2013,  0.9015,  0.6711]], requires_grad=True)\n",
      "new b tensor([ 0.6418, -4.8528], requires_grad=True)\n",
      "Loss= tensor(8.9240, grad_fn=<DivBackward0>)\n",
      "i= 225\n",
      "w.grad= tensor([[ -4.9841, -10.0913,  25.4909],\n",
      "        [  3.4577,   8.8494, -26.3803]])\n",
      "b.grad= tensor([-90.4761, 381.4104])\n",
      "new w tensor([[-0.4687,  0.7410,  0.9528],\n",
      "        [-0.2015,  0.9011,  0.6724]], requires_grad=True)\n",
      "new b tensor([ 0.6463, -4.8719], requires_grad=True)\n",
      "Loss= tensor(8.8492, grad_fn=<DivBackward0>)\n",
      "i= 226\n",
      "w.grad= tensor([[ -4.9661, -10.0383,  25.3749],\n",
      "        [  3.4091,   8.8247, -26.2669]])\n",
      "b.grad= tensor([-90.3283, 381.2420])\n",
      "new w tensor([[-0.4685,  0.7415,  0.9515],\n",
      "        [-0.2017,  0.9006,  0.6737]], requires_grad=True)\n",
      "new b tensor([ 0.6508, -4.8909], requires_grad=True)\n",
      "Loss= tensor(8.7751, grad_fn=<DivBackward0>)\n",
      "i= 227\n",
      "w.grad= tensor([[ -4.9475,  -9.9851,  25.2599],\n",
      "        [  3.3623,   8.8004, -26.1534]])\n",
      "b.grad= tensor([-90.1812, 381.0738])\n",
      "new w tensor([[-0.4682,  0.7420,  0.9503],\n",
      "        [-0.2018,  0.9002,  0.6750]], requires_grad=True)\n",
      "new b tensor([ 0.6553, -4.9100], requires_grad=True)\n",
      "Loss= tensor(8.7017, grad_fn=<DivBackward0>)\n",
      "i= 228\n",
      "w.grad= tensor([[ -4.9296,  -9.9333,  25.1449],\n",
      "        [  3.3162,   8.7758, -26.0406]])\n",
      "b.grad= tensor([-90.0345, 380.9058])\n",
      "new w tensor([[-0.4680,  0.7425,  0.9490],\n",
      "        [-0.2020,  0.8997,  0.6763]], requires_grad=True)\n",
      "new b tensor([ 0.6598, -4.9290], requires_grad=True)\n",
      "Loss= tensor(8.6290, grad_fn=<DivBackward0>)\n",
      "i= 229\n",
      "w.grad= tensor([[ -4.9108,  -9.8811,  25.0307],\n",
      "        [  3.2707,   8.7505, -25.9286]])\n",
      "b.grad= tensor([-89.8885, 380.7380])\n",
      "new w tensor([[-0.4678,  0.7430,  0.9478],\n",
      "        [-0.2022,  0.8993,  0.6776]], requires_grad=True)\n",
      "new b tensor([ 0.6643, -4.9481], requires_grad=True)\n",
      "Loss= tensor(8.5570, grad_fn=<DivBackward0>)\n",
      "i= 230\n",
      "w.grad= tensor([[ -4.8920,  -9.8294,  24.9172],\n",
      "        [  3.2268,   8.7259, -25.8165]])\n",
      "b.grad= tensor([-89.7430, 380.5705])\n",
      "new w tensor([[-0.4675,  0.7435,  0.9465],\n",
      "        [-0.2023,  0.8989,  0.6789]], requires_grad=True)\n",
      "new b tensor([ 0.6688, -4.9671], requires_grad=True)\n",
      "Loss= tensor(8.4856, grad_fn=<DivBackward0>)\n",
      "i= 231\n",
      "w.grad= tensor([[ -4.8741,  -9.7792,  24.8034],\n",
      "        [  3.1824,   8.6993, -25.7059]])\n",
      "b.grad= tensor([-89.5982, 380.4033])\n",
      "new w tensor([[-0.4673,  0.7440,  0.9453],\n",
      "        [-0.2025,  0.8984,  0.6802]], requires_grad=True)\n",
      "new b tensor([ 0.6733, -4.9861], requires_grad=True)\n",
      "Loss= tensor(8.4149, grad_fn=<DivBackward0>)\n",
      "i= 232\n",
      "w.grad= tensor([[ -4.8557,  -9.7291,  24.6903],\n",
      "        [  3.1387,   8.6724, -25.5958]])\n",
      "b.grad= tensor([-89.4538, 380.2362])\n",
      "new w tensor([[-0.4670,  0.7445,  0.9440],\n",
      "        [-0.2026,  0.8980,  0.6815]], requires_grad=True)\n",
      "new b tensor([ 0.6778, -5.0051], requires_grad=True)\n",
      "Loss= tensor(8.3448, grad_fn=<DivBackward0>)\n",
      "i= 233\n",
      "w.grad= tensor([[ -4.8365,  -9.6786,  24.5782],\n",
      "        [  3.0966,   8.6463, -25.4857]])\n",
      "b.grad= tensor([-89.3101, 380.0694])\n",
      "new w tensor([[-0.4668,  0.7450,  0.9428],\n",
      "        [-0.2028,  0.8976,  0.6828]], requires_grad=True)\n",
      "new b tensor([ 0.6822, -5.0241], requires_grad=True)\n",
      "Loss= tensor(8.2754, grad_fn=<DivBackward0>)\n",
      "i= 234\n",
      "w.grad= tensor([[ -4.8177,  -9.6292,  24.4663],\n",
      "        [  3.0553,   8.6199, -25.3761]])\n",
      "b.grad= tensor([-89.1668, 379.9028])\n",
      "new w tensor([[-0.4665,  0.7454,  0.9416],\n",
      "        [-0.2030,  0.8971,  0.6840]], requires_grad=True)\n",
      "new b tensor([ 0.6867, -5.0431], requires_grad=True)\n",
      "Loss= tensor(8.2066, grad_fn=<DivBackward0>)\n",
      "i= 235\n",
      "w.grad= tensor([[ -4.7975,  -9.5784,  24.3559],\n",
      "        [  3.0141,   8.5927, -25.2674]])\n",
      "b.grad= tensor([-89.0242, 379.7365])\n",
      "new w tensor([[-0.4663,  0.7459,  0.9404],\n",
      "        [-0.2031,  0.8967,  0.6853]], requires_grad=True)\n",
      "new b tensor([ 0.6911, -5.0621], requires_grad=True)\n",
      "Loss= tensor(8.1385, grad_fn=<DivBackward0>)\n",
      "i= 236\n",
      "w.grad= tensor([[ -4.7793,  -9.5305,  24.2444],\n",
      "        [  2.9737,   8.5652, -25.1592]])\n",
      "b.grad= tensor([-88.8821, 379.5704])\n",
      "new w tensor([[-0.4661,  0.7464,  0.9391],\n",
      "        [-0.2033,  0.8963,  0.6865]], requires_grad=True)\n",
      "new b tensor([ 0.6956, -5.0811], requires_grad=True)\n",
      "Loss= tensor(8.0709, grad_fn=<DivBackward0>)\n",
      "i= 237\n",
      "w.grad= tensor([[ -4.7597,  -9.4815,  24.1343],\n",
      "        [  2.9341,   8.5378, -25.0515]])\n",
      "b.grad= tensor([-88.7405, 379.4046])\n",
      "new w tensor([[-0.4658,  0.7469,  0.9379],\n",
      "        [-0.2034,  0.8959,  0.6878]], requires_grad=True)\n",
      "new b tensor([ 0.7000, -5.1000], requires_grad=True)\n",
      "Loss= tensor(8.0041, grad_fn=<DivBackward0>)\n",
      "i= 238\n",
      "w.grad= tensor([[ -4.7405,  -9.4332,  24.0246],\n",
      "        [  2.8949,   8.5098, -24.9443]])\n",
      "b.grad= tensor([-88.5995, 379.2390])\n",
      "new w tensor([[-0.4656,  0.7473,  0.9367],\n",
      "        [-0.2035,  0.8954,  0.6890]], requires_grad=True)\n",
      "new b tensor([ 0.7045, -5.1190], requires_grad=True)\n",
      "Loss= tensor(7.9378, grad_fn=<DivBackward0>)\n",
      "i= 239\n",
      "w.grad= tensor([[ -4.7213,  -9.3855,  23.9151],\n",
      "        [  2.8567,   8.4821, -24.8375]])\n",
      "b.grad= tensor([-88.4590, 379.0737])\n",
      "new w tensor([[-0.4653,  0.7478,  0.9355],\n",
      "        [-0.2037,  0.8950,  0.6903]], requires_grad=True)\n",
      "new b tensor([ 0.7089, -5.1380], requires_grad=True)\n",
      "Loss= tensor(7.8721, grad_fn=<DivBackward0>)\n",
      "i= 240\n",
      "w.grad= tensor([[ -4.7017,  -9.3377,  23.8064],\n",
      "        [  2.8184,   8.4535, -24.7317]])\n",
      "b.grad= tensor([-88.3190, 378.9086])\n",
      "new w tensor([[-0.4651,  0.7483,  0.9344],\n",
      "        [-0.2038,  0.8946,  0.6915]], requires_grad=True)\n",
      "new b tensor([ 0.7133, -5.1569], requires_grad=True)\n",
      "Loss= tensor(7.8071, grad_fn=<DivBackward0>)\n",
      "i= 241\n",
      "w.grad= tensor([[ -4.6827,  -9.2910,  23.6978],\n",
      "        [  2.7807,   8.4246, -24.6263]])\n",
      "b.grad= tensor([-88.1796, 378.7438])\n",
      "new w tensor([[-0.4649,  0.7487,  0.9332],\n",
      "        [-0.2040,  0.8942,  0.6928]], requires_grad=True)\n",
      "new b tensor([ 0.7177, -5.1758], requires_grad=True)\n",
      "Loss= tensor(7.7426, grad_fn=<DivBackward0>)\n",
      "i= 242\n",
      "w.grad= tensor([[ -4.6624,  -9.2433,  23.5905],\n",
      "        [  2.7443,   8.3963, -24.5210]])\n",
      "b.grad= tensor([-88.0406, 378.5792])\n",
      "new w tensor([[-0.4646,  0.7492,  0.9320],\n",
      "        [-0.2041,  0.8937,  0.6940]], requires_grad=True)\n",
      "new b tensor([ 0.7221, -5.1948], requires_grad=True)\n",
      "Loss= tensor(7.6787, grad_fn=<DivBackward0>)\n",
      "i= 243\n",
      "w.grad= tensor([[ -4.6436,  -9.1976,  23.4826],\n",
      "        [  2.7080,   8.3673, -24.4167]])\n",
      "b.grad= tensor([-87.9023, 378.4149])\n",
      "new w tensor([[-0.4644,  0.7497,  0.9308],\n",
      "        [-0.2042,  0.8933,  0.6952]], requires_grad=True)\n",
      "new b tensor([ 0.7265, -5.2137], requires_grad=True)\n",
      "Loss= tensor(7.6154, grad_fn=<DivBackward0>)\n",
      "i= 244\n",
      "w.grad= tensor([[ -4.6239,  -9.1514,  23.3758],\n",
      "        [  2.6722,   8.3381, -24.3127]])\n",
      "b.grad= tensor([-87.7644, 378.2508])\n",
      "new w tensor([[-0.4642,  0.7501,  0.9296],\n",
      "        [-0.2044,  0.8929,  0.6964]], requires_grad=True)\n",
      "new b tensor([ 0.7309, -5.2326], requires_grad=True)\n",
      "Loss= tensor(7.5527, grad_fn=<DivBackward0>)\n",
      "i= 245\n",
      "w.grad= tensor([[ -4.6037,  -9.1048,  23.2699],\n",
      "        [  2.6374,   8.3094, -24.2089]])\n",
      "b.grad= tensor([-87.6270, 378.0870])\n",
      "new w tensor([[-0.4640,  0.7506,  0.9285],\n",
      "        [-0.2045,  0.8925,  0.6976]], requires_grad=True)\n",
      "new b tensor([ 0.7353, -5.2515], requires_grad=True)\n",
      "Loss= tensor(7.4906, grad_fn=<DivBackward0>)\n",
      "i= 246\n",
      "w.grad= tensor([[ -4.5844,  -9.0597,  23.1637],\n",
      "        [  2.6021,   8.2794, -24.1063]])\n",
      "b.grad= tensor([-87.4902, 377.9234])\n",
      "new w tensor([[-0.4637,  0.7510,  0.9273],\n",
      "        [-0.2046,  0.8921,  0.6988]], requires_grad=True)\n",
      "new b tensor([ 0.7396, -5.2704], requires_grad=True)\n",
      "Loss= tensor(7.4291, grad_fn=<DivBackward0>)\n",
      "i= 247\n",
      "w.grad= tensor([[ -4.5645,  -9.0142,  23.0584],\n",
      "        [  2.5688,   8.2510, -24.0033]])\n",
      "b.grad= tensor([-87.3539, 377.7601])\n",
      "new w tensor([[-0.4635,  0.7515,  0.9262],\n",
      "        [-0.2048,  0.8917,  0.7000]], requires_grad=True)\n",
      "new b tensor([ 0.7440, -5.2893], requires_grad=True)\n",
      "Loss= tensor(7.3681, grad_fn=<DivBackward0>)\n",
      "i= 248\n",
      "w.grad= tensor([[ -4.5452,  -8.9698,  22.9532],\n",
      "        [  2.5341,   8.2203, -23.9020]])\n",
      "b.grad= tensor([-87.2180, 377.5971])\n",
      "new w tensor([[-0.4633,  0.7519,  0.9250],\n",
      "        [-0.2049,  0.8913,  0.7012]], requires_grad=True)\n",
      "new b tensor([ 0.7484, -5.3082], requires_grad=True)\n",
      "Loss= tensor(7.3077, grad_fn=<DivBackward0>)\n",
      "i= 249\n",
      "w.grad= tensor([[ -4.5255,  -8.9253,  22.8487],\n",
      "        [  2.5007,   8.1903, -23.8007]])\n",
      "b.grad= tensor([-87.0827, 377.4343])\n",
      "new w tensor([[-0.4630,  0.7524,  0.9239],\n",
      "        [-0.2050,  0.8908,  0.7024]], requires_grad=True)\n",
      "new b tensor([ 0.7527, -5.3270], requires_grad=True)\n",
      "Loss= tensor(7.2479, grad_fn=<DivBackward0>)\n",
      "i= 250\n",
      "w.grad= tensor([[ -4.5058,  -8.8810,  22.7446],\n",
      "        [  2.4683,   8.1610, -23.6994]])\n",
      "b.grad= tensor([-86.9479, 377.2717])\n",
      "new w tensor([[-0.4628,  0.7528,  0.9227],\n",
      "        [-0.2051,  0.8904,  0.7036]], requires_grad=True)\n",
      "new b tensor([ 0.7571, -5.3459], requires_grad=True)\n",
      "Loss= tensor(7.1886, grad_fn=<DivBackward0>)\n",
      "i= 251\n",
      "w.grad= tensor([[ -4.4859,  -8.8369,  22.6412],\n",
      "        [  2.4351,   8.1300, -23.5995]])\n",
      "b.grad= tensor([-86.8136, 377.1094])\n",
      "new w tensor([[-0.4626,  0.7533,  0.9216],\n",
      "        [-0.2053,  0.8900,  0.7048]], requires_grad=True)\n",
      "new b tensor([ 0.7614, -5.3648], requires_grad=True)\n",
      "Loss= tensor(7.1298, grad_fn=<DivBackward0>)\n",
      "i= 252\n",
      "w.grad= tensor([[ -4.4658,  -8.7928,  22.5383],\n",
      "        [  2.4040,   8.1010, -23.4989]])\n",
      "b.grad= tensor([-86.6797, 376.9474])\n",
      "new w tensor([[-0.4624,  0.7537,  0.9205],\n",
      "        [-0.2054,  0.8896,  0.7060]], requires_grad=True)\n",
      "new b tensor([ 0.7657, -5.3836], requires_grad=True)\n",
      "Loss= tensor(7.0716, grad_fn=<DivBackward0>)\n",
      "i= 253\n",
      "w.grad= tensor([[ -4.4466,  -8.7501,  22.4354],\n",
      "        [  2.3716,   8.0699, -23.4000]])\n",
      "b.grad= tensor([-86.5464, 376.7856])\n",
      "new w tensor([[-0.4621,  0.7541,  0.9194],\n",
      "        [-0.2055,  0.8892,  0.7071]], requires_grad=True)\n",
      "new b tensor([ 0.7701, -5.4025], requires_grad=True)\n",
      "Loss= tensor(7.0139, grad_fn=<DivBackward0>)\n",
      "i= 254\n",
      "w.grad= tensor([[ -4.4264,  -8.7065,  22.3335],\n",
      "        [  2.3408,   8.0400, -23.3008]])\n",
      "b.grad= tensor([-86.4135, 376.6241])\n",
      "new w tensor([[-0.4619,  0.7546,  0.9182],\n",
      "        [-0.2056,  0.8888,  0.7083]], requires_grad=True)\n",
      "new b tensor([ 0.7744, -5.4213], requires_grad=True)\n",
      "Loss= tensor(6.9568, grad_fn=<DivBackward0>)\n",
      "i= 255\n",
      "w.grad= tensor([[ -4.4068,  -8.6637,  22.2318],\n",
      "        [  2.3099,   8.0094, -23.2024]])\n",
      "b.grad= tensor([-86.2811, 376.4629])\n",
      "new w tensor([[-0.4617,  0.7550,  0.9171],\n",
      "        [-0.2057,  0.8884,  0.7095]], requires_grad=True)\n",
      "new b tensor([ 0.7787, -5.4401], requires_grad=True)\n",
      "Loss= tensor(6.9001, grad_fn=<DivBackward0>)\n",
      "i= 256\n",
      "w.grad= tensor([[ -4.3867,  -8.6208,  22.1308],\n",
      "        [  2.2797,   7.9792, -23.1044]])\n",
      "b.grad= tensor([-86.1492, 376.3018])\n",
      "new w tensor([[-0.4615,  0.7554,  0.9160],\n",
      "        [-0.2058,  0.8880,  0.7106]], requires_grad=True)\n",
      "new b tensor([ 0.7830, -5.4589], requires_grad=True)\n",
      "Loss= tensor(6.8441, grad_fn=<DivBackward0>)\n",
      "i= 257\n",
      "w.grad= tensor([[ -4.3671,  -8.5786,  22.0300],\n",
      "        [  2.2502,   7.9494, -23.0065]])\n",
      "b.grad= tensor([-86.0178, 376.1411])\n",
      "new w tensor([[-0.4613,  0.7559,  0.9149],\n",
      "        [-0.2060,  0.8876,  0.7118]], requires_grad=True)\n",
      "new b tensor([ 0.7873, -5.4777], requires_grad=True)\n",
      "Loss= tensor(6.7885, grad_fn=<DivBackward0>)\n",
      "i= 258\n",
      "w.grad= tensor([[ -4.3475,  -8.5369,  21.9296],\n",
      "        [  2.2202,   7.9185, -22.9097]])\n",
      "b.grad= tensor([-85.8868, 375.9806])\n",
      "new w tensor([[-0.4610,  0.7563,  0.9138],\n",
      "        [-0.2061,  0.8872,  0.7129]], requires_grad=True)\n",
      "new b tensor([ 0.7916, -5.4965], requires_grad=True)\n",
      "Loss= tensor(6.7334, grad_fn=<DivBackward0>)\n",
      "i= 259\n",
      "w.grad= tensor([[ -4.3275,  -8.4949,  21.8299],\n",
      "        [  2.1910,   7.8878, -22.8131]])\n",
      "b.grad= tensor([-85.7564, 375.8204])\n",
      "new w tensor([[-0.4608,  0.7567,  0.9127],\n",
      "        [-0.2062,  0.8868,  0.7140]], requires_grad=True)\n",
      "new b tensor([ 0.7959, -5.5153], requires_grad=True)\n",
      "Loss= tensor(6.6789, grad_fn=<DivBackward0>)\n",
      "i= 260\n",
      "w.grad= tensor([[ -4.3077,  -8.4534,  21.7305],\n",
      "        [  2.1614,   7.8564, -22.7175]])\n",
      "b.grad= tensor([-85.6263, 375.6604])\n",
      "new w tensor([[-0.4606,  0.7571,  0.9117],\n",
      "        [-0.2063,  0.8864,  0.7152]], requires_grad=True)\n",
      "new b tensor([ 0.8002, -5.5341], requires_grad=True)\n",
      "Loss= tensor(6.6248, grad_fn=<DivBackward0>)\n",
      "i= 261\n",
      "w.grad= tensor([[ -4.2884,  -8.4126,  21.6313],\n",
      "        [  2.1337,   7.8267, -22.6213]])\n",
      "b.grad= tensor([-85.4968, 375.5007])\n",
      "new w tensor([[-0.4604,  0.7576,  0.9106],\n",
      "        [-0.2064,  0.8860,  0.7163]], requires_grad=True)\n",
      "new b tensor([ 0.8045, -5.5529], requires_grad=True)\n",
      "Loss= tensor(6.5713, grad_fn=<DivBackward0>)\n",
      "i= 262\n",
      "w.grad= tensor([[ -4.2685,  -8.3715,  21.5330],\n",
      "        [  2.1056,   7.7963, -22.5260]])\n",
      "b.grad= tensor([-85.3677, 375.3412])\n",
      "new w tensor([[-0.4602,  0.7580,  0.9095],\n",
      "        [-0.2065,  0.8857,  0.7174]], requires_grad=True)\n",
      "new b tensor([ 0.8087, -5.5716], requires_grad=True)\n",
      "Loss= tensor(6.5182, grad_fn=<DivBackward0>)\n",
      "i= 263\n",
      "w.grad= tensor([[ -4.2487,  -8.3306,  21.4350],\n",
      "        [  2.0774,   7.7651, -22.4314]])\n",
      "b.grad= tensor([-85.2391, 375.1820])\n",
      "new w tensor([[-0.4600,  0.7584,  0.9084],\n",
      "        [-0.2066,  0.8853,  0.7186]], requires_grad=True)\n",
      "new b tensor([ 0.8130, -5.5904], requires_grad=True)\n",
      "Loss= tensor(6.4656, grad_fn=<DivBackward0>)\n",
      "i= 264\n",
      "w.grad= tensor([[ -4.2292,  -8.2904,  21.3373],\n",
      "        [  2.0495,   7.7342, -22.3373]])\n",
      "b.grad= tensor([-85.1109, 375.0230])\n",
      "new w tensor([[-0.4598,  0.7588,  0.9074],\n",
      "        [-0.2067,  0.8849,  0.7197]], requires_grad=True)\n",
      "new b tensor([ 0.8172, -5.6092], requires_grad=True)\n",
      "Loss= tensor(6.4136, grad_fn=<DivBackward0>)\n",
      "i= 265\n",
      "w.grad= tensor([[ -4.2093,  -8.2498,  21.2404],\n",
      "        [  2.0228,   7.7040, -22.2432]])\n",
      "b.grad= tensor([-84.9832, 374.8643])\n",
      "new w tensor([[-0.4596,  0.7592,  0.9063],\n",
      "        [-0.2068,  0.8845,  0.7208]], requires_grad=True)\n",
      "new b tensor([ 0.8215, -5.6279], requires_grad=True)\n",
      "Loss= tensor(6.3620, grad_fn=<DivBackward0>)\n",
      "i= 266\n",
      "w.grad= tensor([[ -4.1901,  -8.2102,  21.1435],\n",
      "        [  1.9949,   7.6722, -22.1505]])\n",
      "b.grad= tensor([-84.8559, 374.7059])\n",
      "new w tensor([[-0.4593,  0.7596,  0.9052],\n",
      "        [-0.2069,  0.8841,  0.7219]], requires_grad=True)\n",
      "new b tensor([ 0.8257, -5.6466], requires_grad=True)\n",
      "Loss= tensor(6.3108, grad_fn=<DivBackward0>)\n",
      "i= 267\n",
      "w.grad= tensor([[ -4.1702,  -8.1702,  21.0475],\n",
      "        [  1.9686,   7.6418, -22.0573]])\n",
      "b.grad= tensor([-84.7291, 374.5477])\n",
      "new w tensor([[-0.4591,  0.7600,  0.9042],\n",
      "        [-0.2070,  0.8837,  0.7230]], requires_grad=True)\n",
      "new b tensor([ 0.8300, -5.6654], requires_grad=True)\n",
      "Loss= tensor(6.2602, grad_fn=<DivBackward0>)\n",
      "i= 268\n",
      "w.grad= tensor([[ -4.1504,  -8.1304,  20.9518],\n",
      "        [  1.9428,   7.6118, -21.9644]])\n",
      "b.grad= tensor([-84.6028, 374.3897])\n",
      "new w tensor([[-0.4589,  0.7605,  0.9031],\n",
      "        [-0.2071,  0.8834,  0.7241]], requires_grad=True)\n",
      "new b tensor([ 0.8342, -5.6841], requires_grad=True)\n",
      "Loss= tensor(6.2100, grad_fn=<DivBackward0>)\n",
      "i= 269\n",
      "w.grad= tensor([[ -4.1313,  -8.0917,  20.8561],\n",
      "        [  1.9160,   7.5801, -21.8729]])\n",
      "b.grad= tensor([-84.4769, 374.2320])\n",
      "new w tensor([[-0.4587,  0.7609,  0.9021],\n",
      "        [-0.2072,  0.8830,  0.7252]], requires_grad=True)\n",
      "new b tensor([ 0.8384, -5.7028], requires_grad=True)\n",
      "Loss= tensor(6.1603, grad_fn=<DivBackward0>)\n",
      "i= 270\n",
      "w.grad= tensor([[ -4.1114,  -8.0520,  20.7616],\n",
      "        [  1.8903,   7.5495, -21.7813]])\n",
      "b.grad= tensor([-84.3514, 374.0746])\n",
      "new w tensor([[-0.4585,  0.7613,  0.9011],\n",
      "        [-0.2073,  0.8826,  0.7263]], requires_grad=True)\n",
      "new b tensor([ 0.8426, -5.7215], requires_grad=True)\n",
      "Loss= tensor(6.1110, grad_fn=<DivBackward0>)\n",
      "i= 271\n",
      "w.grad= tensor([[ -4.0918,  -8.0131,  20.6672],\n",
      "        [  1.8648,   7.5189, -21.6901]])\n",
      "b.grad= tensor([-84.2263, 373.9174])\n",
      "new w tensor([[-0.4583,  0.7617,  0.9000],\n",
      "        [-0.2074,  0.8822,  0.7274]], requires_grad=True)\n",
      "new b tensor([ 0.8469, -5.7402], requires_grad=True)\n",
      "Loss= tensor(6.0623, grad_fn=<DivBackward0>)\n",
      "i= 272\n",
      "w.grad= tensor([[ -4.0725,  -7.9746,  20.5730],\n",
      "        [  1.8399,   7.4885, -21.5992]])\n",
      "b.grad= tensor([-84.1017, 373.7604])\n",
      "new w tensor([[-0.4581,  0.7621,  0.8990],\n",
      "        [-0.2075,  0.8818,  0.7284]], requires_grad=True)\n",
      "new b tensor([ 0.8511, -5.7589], requires_grad=True)\n",
      "Loss= tensor(6.0139, grad_fn=<DivBackward0>)\n",
      "i= 273\n",
      "w.grad= tensor([[ -4.0533,  -7.9366,  20.4793],\n",
      "        [  1.8145,   7.4574, -21.5091]])\n",
      "b.grad= tensor([-83.9775, 373.6037])\n",
      "new w tensor([[-0.4579,  0.7625,  0.8980],\n",
      "        [-0.2076,  0.8815,  0.7295]], requires_grad=True)\n",
      "new b tensor([ 0.8553, -5.7776], requires_grad=True)\n",
      "Loss= tensor(5.9660, grad_fn=<DivBackward0>)\n",
      "i= 274\n",
      "w.grad= tensor([[ -4.0338,  -7.8982,  20.3862],\n",
      "        [  1.7899,   7.4269, -21.4192]])\n",
      "b.grad= tensor([-83.8538, 373.4473])\n",
      "new w tensor([[-0.4577,  0.7628,  0.8970],\n",
      "        [-0.2077,  0.8811,  0.7306]], requires_grad=True)\n",
      "new b tensor([ 0.8594, -5.7962], requires_grad=True)\n",
      "Loss= tensor(5.9185, grad_fn=<DivBackward0>)\n",
      "i= 275\n",
      "w.grad= tensor([[ -4.0147,  -7.8605,  20.2934],\n",
      "        [  1.7651,   7.3959, -21.3300]])\n",
      "b.grad= tensor([-83.7305, 373.2910])\n",
      "new w tensor([[-0.4575,  0.7632,  0.8959],\n",
      "        [-0.2077,  0.8807,  0.7317]], requires_grad=True)\n",
      "new b tensor([ 0.8636, -5.8149], requires_grad=True)\n",
      "Loss= tensor(5.8715, grad_fn=<DivBackward0>)\n",
      "i= 276\n",
      "w.grad= tensor([[ -3.9956,  -7.8230,  20.2009],\n",
      "        [  1.7415,   7.3661, -21.2405]])\n",
      "b.grad= tensor([-83.6076, 373.1351])\n",
      "new w tensor([[-0.4573,  0.7636,  0.8949],\n",
      "        [-0.2078,  0.8804,  0.7327]], requires_grad=True)\n",
      "new b tensor([ 0.8678, -5.8336], requires_grad=True)\n",
      "Loss= tensor(5.8249, grad_fn=<DivBackward0>)\n",
      "i= 277\n",
      "w.grad= tensor([[ -3.9766,  -7.7858,  20.1088],\n",
      "        [  1.7174,   7.3354, -21.1520]])\n",
      "b.grad= tensor([-83.4851, 372.9794])\n",
      "new w tensor([[-0.4571,  0.7640,  0.8939],\n",
      "        [-0.2079,  0.8800,  0.7338]], requires_grad=True)\n",
      "new b tensor([ 0.8720, -5.8522], requires_grad=True)\n",
      "Loss= tensor(5.7787, grad_fn=<DivBackward0>)\n",
      "i= 278\n",
      "w.grad= tensor([[ -3.9569,  -7.7480,  20.0176],\n",
      "        [  1.6934,   7.3047, -21.0640]])\n",
      "b.grad= tensor([-83.3631, 372.8239])\n",
      "new w tensor([[-0.4569,  0.7644,  0.8929],\n",
      "        [-0.2080,  0.8796,  0.7348]], requires_grad=True)\n",
      "new b tensor([ 0.8762, -5.8708], requires_grad=True)\n",
      "Loss= tensor(5.7330, grad_fn=<DivBackward0>)\n",
      "i= 279\n",
      "w.grad= tensor([[ -3.9380,  -7.7111,  19.9265],\n",
      "        [  1.6702,   7.2747, -20.9760]])\n",
      "b.grad= tensor([-83.2415, 372.6687])\n",
      "new w tensor([[-0.4567,  0.7648,  0.8919],\n",
      "        [-0.2081,  0.8793,  0.7359]], requires_grad=True)\n",
      "new b tensor([ 0.8803, -5.8895], requires_grad=True)\n",
      "Loss= tensor(5.6877, grad_fn=<DivBackward0>)\n",
      "i= 280\n",
      "w.grad= tensor([[ -3.9188,  -7.6741,  19.8359],\n",
      "        [  1.6463,   7.2435, -20.8891]])\n",
      "b.grad= tensor([-83.1203, 372.5137])\n",
      "new w tensor([[-0.4565,  0.7652,  0.8909],\n",
      "        [-0.2082,  0.8789,  0.7369]], requires_grad=True)\n",
      "new b tensor([ 0.8845, -5.9081], requires_grad=True)\n",
      "Loss= tensor(5.6428, grad_fn=<DivBackward0>)\n",
      "i= 281\n",
      "w.grad= tensor([[ -3.9001,  -7.6378,  19.7455],\n",
      "        [  1.6236,   7.2136, -20.8020]])\n",
      "b.grad= tensor([-82.9995, 372.3590])\n",
      "new w tensor([[-0.4563,  0.7656,  0.8899],\n",
      "        [-0.2083,  0.8785,  0.7380]], requires_grad=True)\n",
      "new b tensor([ 0.8886, -5.9267], requires_grad=True)\n",
      "Loss= tensor(5.5984, grad_fn=<DivBackward0>)\n",
      "i= 282\n",
      "w.grad= tensor([[ -3.8809,  -7.6011,  19.6558],\n",
      "        [  1.6006,   7.1832, -20.7155]])\n",
      "b.grad= tensor([-82.8791, 372.2045])\n",
      "new w tensor([[-0.4561,  0.7659,  0.8890],\n",
      "        [-0.2083,  0.8782,  0.7390]], requires_grad=True)\n",
      "new b tensor([ 0.8928, -5.9453], requires_grad=True)\n",
      "Loss= tensor(5.5543, grad_fn=<DivBackward0>)\n",
      "i= 283\n",
      "w.grad= tensor([[ -3.8621,  -7.5651,  19.5662],\n",
      "        [  1.5774,   7.1525, -20.6298]])\n",
      "b.grad= tensor([-82.7591, 372.0502])\n",
      "new w tensor([[-0.4559,  0.7663,  0.8880],\n",
      "        [-0.2084,  0.8778,  0.7400]], requires_grad=True)\n",
      "new b tensor([ 0.8969, -5.9639], requires_grad=True)\n",
      "Loss= tensor(5.5107, grad_fn=<DivBackward0>)\n",
      "i= 284\n",
      "w.grad= tensor([[ -3.8428,  -7.5287,  19.4775],\n",
      "        [  1.5556,   7.1228, -20.5437]])\n",
      "b.grad= tensor([-82.6396, 371.8962])\n",
      "new w tensor([[-0.4557,  0.7667,  0.8870],\n",
      "        [-0.2085,  0.8775,  0.7411]], requires_grad=True)\n",
      "new b tensor([ 0.9010, -5.9825], requires_grad=True)\n",
      "Loss= tensor(5.4674, grad_fn=<DivBackward0>)\n",
      "i= 285\n",
      "w.grad= tensor([[ -3.8241,  -7.4928,  19.3889],\n",
      "        [  1.5321,   7.0913, -20.4593]])\n",
      "b.grad= tensor([-82.5204, 371.7424])\n",
      "new w tensor([[-0.4555,  0.7671,  0.8860],\n",
      "        [-0.2086,  0.8771,  0.7421]], requires_grad=True)\n",
      "new b tensor([ 0.9052, -6.0011], requires_grad=True)\n",
      "Loss= tensor(5.4246, grad_fn=<DivBackward0>)\n",
      "i= 286\n",
      "w.grad= tensor([[ -3.8054,  -7.4573,  19.3007],\n",
      "        [  1.5112,   7.0624, -20.3737]])\n",
      "b.grad= tensor([-82.4017, 371.5888])\n",
      "new w tensor([[-0.4554,  0.7674,  0.8851],\n",
      "        [-0.2086,  0.8768,  0.7431]], requires_grad=True)\n",
      "new b tensor([ 0.9093, -6.0197], requires_grad=True)\n",
      "Loss= tensor(5.3821, grad_fn=<DivBackward0>)\n",
      "i= 287\n",
      "w.grad= tensor([[ -3.7871,  -7.4222,  19.2127],\n",
      "        [  1.4890,   7.0320, -20.2894]])\n",
      "b.grad= tensor([-82.2833, 371.4355])\n",
      "new w tensor([[-0.4552,  0.7678,  0.8841],\n",
      "        [-0.2087,  0.8764,  0.7441]], requires_grad=True)\n",
      "new b tensor([ 0.9134, -6.0383], requires_grad=True)\n",
      "Loss= tensor(5.3401, grad_fn=<DivBackward0>)\n",
      "i= 288\n",
      "w.grad= tensor([[ -3.7683,  -7.3868,  19.1253],\n",
      "        [  1.4677,   7.0026, -20.2050]])\n",
      "b.grad= tensor([-82.1654, 371.2824])\n",
      "new w tensor([[-0.4550,  0.7682,  0.8832],\n",
      "        [-0.2088,  0.8761,  0.7451]], requires_grad=True)\n",
      "new b tensor([ 0.9175, -6.0568], requires_grad=True)\n",
      "Loss= tensor(5.2984, grad_fn=<DivBackward0>)\n",
      "i= 289\n",
      "w.grad= tensor([[ -3.7494,  -7.3514,  19.0386],\n",
      "        [  1.4457,   6.9721, -20.1217]])\n",
      "b.grad= tensor([-82.0478, 371.1296])\n",
      "new w tensor([[-0.4548,  0.7686,  0.8822],\n",
      "        [-0.2089,  0.8757,  0.7461]], requires_grad=True)\n",
      "new b tensor([ 0.9216, -6.0754], requires_grad=True)\n",
      "Loss= tensor(5.2571, grad_fn=<DivBackward0>)\n",
      "i= 290\n",
      "w.grad= tensor([[ -3.7313,  -7.3169,  18.9517],\n",
      "        [  1.4241,   6.9419, -20.0387]])\n",
      "b.grad= tensor([-81.9307, 370.9770])\n",
      "new w tensor([[-0.4546,  0.7689,  0.8813],\n",
      "        [-0.2089,  0.8754,  0.7471]], requires_grad=True)\n",
      "new b tensor([ 0.9257, -6.0939], requires_grad=True)\n",
      "Loss= tensor(5.2162, grad_fn=<DivBackward0>)\n",
      "i= 291\n",
      "w.grad= tensor([[ -3.7128,  -7.2822,  18.8656],\n",
      "        [  1.4036,   6.9129, -19.9553]])\n",
      "b.grad= tensor([-81.8139, 370.8246])\n",
      "new w tensor([[-0.4544,  0.7693,  0.8803],\n",
      "        [-0.2090,  0.8750,  0.7481]], requires_grad=True)\n",
      "new b tensor([ 0.9298, -6.1125], requires_grad=True)\n",
      "Loss= tensor(5.1757, grad_fn=<DivBackward0>)\n",
      "i= 292\n",
      "w.grad= tensor([[ -3.6939,  -7.2474,  18.7801],\n",
      "        [  1.3824,   6.8828, -19.8731]])\n",
      "b.grad= tensor([-81.6975, 370.6724])\n",
      "new w tensor([[-0.4542,  0.7696,  0.8794],\n",
      "        [-0.2091,  0.8747,  0.7491]], requires_grad=True)\n",
      "new b tensor([ 0.9339, -6.1310], requires_grad=True)\n",
      "Loss= tensor(5.1356, grad_fn=<DivBackward0>)\n",
      "i= 293\n",
      "w.grad= tensor([[ -3.6754,  -7.2127,  18.6949],\n",
      "        [  1.3618,   6.8534, -19.7909]])\n",
      "b.grad= tensor([-81.5815, 370.5205])\n",
      "new w tensor([[-0.4541,  0.7700,  0.8784],\n",
      "        [-0.2091,  0.8743,  0.7501]], requires_grad=True)\n",
      "new b tensor([ 0.9380, -6.1495], requires_grad=True)\n",
      "Loss= tensor(5.0958, grad_fn=<DivBackward0>)\n",
      "i= 294\n",
      "w.grad= tensor([[ -3.6579,  -7.1795,  18.6093],\n",
      "        [  1.3408,   6.8236, -19.7094]])\n",
      "b.grad= tensor([-81.4659, 370.3688])\n",
      "new w tensor([[-0.4539,  0.7704,  0.8775],\n",
      "        [-0.2092,  0.8740,  0.7511]], requires_grad=True)\n",
      "new b tensor([ 0.9420, -6.1681], requires_grad=True)\n",
      "Loss= tensor(5.0564, grad_fn=<DivBackward0>)\n",
      "i= 295\n",
      "w.grad= tensor([[ -3.6392,  -7.1451,  18.5250],\n",
      "        [  1.3204,   6.7940, -19.6281]])\n",
      "b.grad= tensor([-81.3507, 370.2173])\n",
      "new w tensor([[-0.4537,  0.7707,  0.8766],\n",
      "        [-0.2093,  0.8737,  0.7521]], requires_grad=True)\n",
      "new b tensor([ 0.9461, -6.1866], requires_grad=True)\n",
      "Loss= tensor(5.0174, grad_fn=<DivBackward0>)\n",
      "i= 296\n",
      "w.grad= tensor([[ -3.6204,  -7.1106,  18.4413],\n",
      "        [  1.3006,   6.7652, -19.5468]])\n",
      "b.grad= tensor([-81.2359, 370.0661])\n",
      "new w tensor([[-0.4535,  0.7711,  0.8757],\n",
      "        [-0.2093,  0.8733,  0.7531]], requires_grad=True)\n",
      "new b tensor([ 0.9502, -6.2051], requires_grad=True)\n",
      "Loss= tensor(4.9787, grad_fn=<DivBackward0>)\n",
      "i= 297\n",
      "w.grad= tensor([[ -3.6032,  -7.0780,  18.3568],\n",
      "        [  1.2801,   6.7355, -19.4666]])\n",
      "b.grad= tensor([-81.1214, 369.9151])\n",
      "new w tensor([[-0.4533,  0.7714,  0.8747],\n",
      "        [-0.2094,  0.8730,  0.7540]], requires_grad=True)\n",
      "new b tensor([ 0.9542, -6.2236], requires_grad=True)\n",
      "Loss= tensor(4.9404, grad_fn=<DivBackward0>)\n",
      "i= 298\n",
      "w.grad= tensor([[ -3.5850,  -7.0444,  18.2735],\n",
      "        [  1.2604,   6.7065, -19.3862]])\n",
      "b.grad= tensor([-81.0073, 369.7643])\n",
      "new w tensor([[-0.4531,  0.7718,  0.8738],\n",
      "        [-0.2095,  0.8726,  0.7550]], requires_grad=True)\n",
      "new b tensor([ 0.9583, -6.2421], requires_grad=True)\n",
      "Loss= tensor(4.9024, grad_fn=<DivBackward0>)\n",
      "i= 299\n",
      "w.grad= tensor([[ -3.5673,  -7.0115,  18.1903],\n",
      "        [  1.2400,   6.6767, -19.3067]])\n",
      "b.grad= tensor([-80.8936, 369.6137])\n",
      "new w tensor([[-0.4530,  0.7721,  0.8729],\n",
      "        [-0.2095,  0.8723,  0.7560]], requires_grad=True)\n",
      "new b tensor([ 0.9623, -6.2605], requires_grad=True)\n",
      "Loss= tensor(4.8648, grad_fn=<DivBackward0>)\n",
      "i= 300\n",
      "w.grad= tensor([[ -3.5485,  -6.9775,  18.1082],\n",
      "        [  1.2209,   6.6484, -19.2270]])\n",
      "b.grad= tensor([-80.7803, 369.4633])\n",
      "new w tensor([[-0.4528,  0.7725,  0.8720],\n",
      "        [-0.2096,  0.8720,  0.7569]], requires_grad=True)\n",
      "new b tensor([ 0.9664, -6.2790], requires_grad=True)\n",
      "Loss= tensor(4.8276, grad_fn=<DivBackward0>)\n",
      "i= 301\n",
      "w.grad= tensor([[ -3.5315,  -6.9455,  18.0253],\n",
      "        [  1.2012,   6.6191, -19.1481]])\n",
      "b.grad= tensor([-80.6673, 369.3132])\n",
      "new w tensor([[-0.4526,  0.7728,  0.8711],\n",
      "        [-0.2096,  0.8716,  0.7579]], requires_grad=True)\n",
      "new b tensor([ 0.9704, -6.2975], requires_grad=True)\n",
      "Loss= tensor(4.7907, grad_fn=<DivBackward0>)\n",
      "i= 302\n",
      "w.grad= tensor([[ -3.5133,  -6.9124,  17.9437],\n",
      "        [  1.1820,   6.5903, -19.0693]])\n",
      "b.grad= tensor([-80.5548, 369.1633])\n",
      "new w tensor([[-0.4524,  0.7732,  0.8702],\n",
      "        [-0.2097,  0.8713,  0.7588]], requires_grad=True)\n",
      "new b tensor([ 0.9744, -6.3159], requires_grad=True)\n",
      "Loss= tensor(4.7541, grad_fn=<DivBackward0>)\n",
      "i= 303\n",
      "w.grad= tensor([[ -3.4951,  -6.8793,  17.8624],\n",
      "        [  1.1621,   6.5607, -18.9914]])\n",
      "b.grad= tensor([-80.4425, 369.0135])\n",
      "new w tensor([[-0.4523,  0.7735,  0.8693],\n",
      "        [-0.2098,  0.8710,  0.7598]], requires_grad=True)\n",
      "new b tensor([ 0.9784, -6.3344], requires_grad=True)\n",
      "Loss= tensor(4.7179, grad_fn=<DivBackward0>)\n",
      "i= 304\n",
      "w.grad= tensor([[ -3.4776,  -6.8471,  17.7812],\n",
      "        [  1.1437,   6.5326, -18.9131]])\n",
      "b.grad= tensor([-80.3307, 368.8641])\n",
      "new w tensor([[-0.4521,  0.7739,  0.8684],\n",
      "        [-0.2098,  0.8707,  0.7607]], requires_grad=True)\n",
      "new b tensor([ 0.9825, -6.3528], requires_grad=True)\n",
      "Loss= tensor(4.6820, grad_fn=<DivBackward0>)\n",
      "i= 305\n",
      "w.grad= tensor([[ -3.4605,  -6.8155,  17.7001],\n",
      "        [  1.1247,   6.5039, -18.8355]])\n",
      "b.grad= tensor([-80.2192, 368.7148])\n",
      "new w tensor([[-0.4519,  0.7742,  0.8676],\n",
      "        [-0.2099,  0.8703,  0.7617]], requires_grad=True)\n",
      "new b tensor([ 0.9865, -6.3713], requires_grad=True)\n",
      "Loss= tensor(4.6464, grad_fn=<DivBackward0>)\n",
      "i= 306\n",
      "w.grad= tensor([[ -3.4431,  -6.7835,  17.6196],\n",
      "        [  1.1054,   6.4748, -18.7587]])\n",
      "b.grad= tensor([-80.1081, 368.5657])\n",
      "new w tensor([[-0.4517,  0.7745,  0.8667],\n",
      "        [-0.2099,  0.8700,  0.7626]], requires_grad=True)\n",
      "new b tensor([ 0.9905, -6.3897], requires_grad=True)\n",
      "Loss= tensor(4.6112, grad_fn=<DivBackward0>)\n",
      "i= 307\n",
      "w.grad= tensor([[ -3.4251,  -6.7511,  17.5399],\n",
      "        [  1.0882,   6.4479, -18.6808]])\n",
      "b.grad= tensor([-79.9973, 368.4169])\n",
      "new w tensor([[-0.4516,  0.7749,  0.8658],\n",
      "        [-0.2100,  0.8697,  0.7635]], requires_grad=True)\n",
      "new b tensor([ 0.9945, -6.4081], requires_grad=True)\n",
      "Loss= tensor(4.5763, grad_fn=<DivBackward0>)\n",
      "i= 308\n",
      "w.grad= tensor([[ -3.4084,  -6.7202,  17.4597],\n",
      "        [  1.0686,   6.4185, -18.6051]])\n",
      "b.grad= tensor([-79.8869, 368.2682])\n",
      "new w tensor([[-0.4514,  0.7752,  0.8649],\n",
      "        [-0.2100,  0.8694,  0.7645]], requires_grad=True)\n",
      "new b tensor([ 0.9985, -6.4265], requires_grad=True)\n",
      "Loss= tensor(4.5418, grad_fn=<DivBackward0>)\n",
      "i= 309\n",
      "w.grad= tensor([[ -3.3907,  -6.6881,  17.3808],\n",
      "        [  1.0507,   6.3906, -18.5287]])\n",
      "b.grad= tensor([-79.7768, 368.1198])\n",
      "new w tensor([[-0.4512,  0.7755,  0.8641],\n",
      "        [-0.2101,  0.8690,  0.7654]], requires_grad=True)\n",
      "new b tensor([ 1.0025, -6.4449], requires_grad=True)\n",
      "Loss= tensor(4.5075, grad_fn=<DivBackward0>)\n",
      "i= 310\n",
      "w.grad= tensor([[ -3.3731,  -6.6563,  17.3020],\n",
      "        [  1.0318,   6.3617, -18.4534]])\n",
      "b.grad= tensor([-79.6671, 367.9716])\n",
      "new w tensor([[-0.4511,  0.7759,  0.8632],\n",
      "        [-0.2101,  0.8687,  0.7663]], requires_grad=True)\n",
      "new b tensor([ 1.0064, -6.4633], requires_grad=True)\n",
      "Loss= tensor(4.4736, grad_fn=<DivBackward0>)\n",
      "i= 311\n",
      "w.grad= tensor([[ -3.3560,  -6.6253,  17.2234],\n",
      "        [  1.0145,   6.3347, -18.3773]])\n",
      "b.grad= tensor([-79.5578, 367.8236])\n",
      "new w tensor([[-0.4509,  0.7762,  0.8623],\n",
      "        [-0.2102,  0.8684,  0.7672]], requires_grad=True)\n",
      "new b tensor([ 1.0104, -6.4817], requires_grad=True)\n",
      "Loss= tensor(4.4400, grad_fn=<DivBackward0>)\n",
      "i= 312\n",
      "w.grad= tensor([[ -3.3391,  -6.5944,  17.1450],\n",
      "        [  0.9961,   6.3063, -18.3025]])\n",
      "b.grad= tensor([-79.4488, 367.6758])\n",
      "new w tensor([[-0.4507,  0.7765,  0.8615],\n",
      "        [-0.2102,  0.8681,  0.7682]], requires_grad=True)\n",
      "new b tensor([ 1.0144, -6.5001], requires_grad=True)\n",
      "Loss= tensor(4.4067, grad_fn=<DivBackward0>)\n",
      "i= 313\n",
      "w.grad= tensor([[ -3.3215,  -6.5629,  17.0675],\n",
      "        [  0.9785,   6.2787, -18.2276]])\n",
      "b.grad= tensor([-79.3402, 367.5282])\n",
      "new w tensor([[-0.4506,  0.7769,  0.8606],\n",
      "        [-0.2103,  0.8678,  0.7691]], requires_grad=True)\n",
      "new b tensor([ 1.0184, -6.5185], requires_grad=True)\n",
      "Loss= tensor(4.3737, grad_fn=<DivBackward0>)\n",
      "i= 314\n",
      "w.grad= tensor([[ -3.3045,  -6.5322,  16.9900],\n",
      "        [  0.9603,   6.2503, -18.1536]])\n",
      "b.grad= tensor([-79.2319, 367.3808])\n",
      "new w tensor([[-0.4504,  0.7772,  0.8598],\n",
      "        [-0.2103,  0.8675,  0.7700]], requires_grad=True)\n",
      "new b tensor([ 1.0223, -6.5368], requires_grad=True)\n",
      "Loss= tensor(4.3410, grad_fn=<DivBackward0>)\n",
      "i= 315\n",
      "w.grad= tensor([[ -3.2878,  -6.5019,  16.9127],\n",
      "        [  0.9429,   6.2228, -18.0795]])\n",
      "b.grad= tensor([-79.1239, 367.2336])\n",
      "new w tensor([[-0.4502,  0.7775,  0.8589],\n",
      "        [-0.2104,  0.8672,  0.7709]], requires_grad=True)\n",
      "new b tensor([ 1.0263, -6.5552], requires_grad=True)\n",
      "Loss= tensor(4.3086, grad_fn=<DivBackward0>)\n",
      "i= 316\n",
      "w.grad= tensor([[ -3.2711,  -6.4715,  16.8359],\n",
      "        [  0.9257,   6.1957, -18.0055]])\n",
      "b.grad= tensor([-79.0163, 367.0866])\n",
      "new w tensor([[-0.4501,  0.7778,  0.8581],\n",
      "        [-0.2104,  0.8669,  0.7718]], requires_grad=True)\n",
      "new b tensor([ 1.0302, -6.5736], requires_grad=True)\n",
      "Loss= tensor(4.2766, grad_fn=<DivBackward0>)\n",
      "i= 317\n",
      "w.grad= tensor([[ -3.2538,  -6.4407,  16.7598],\n",
      "        [  0.9081,   6.1679, -17.9323]])\n",
      "b.grad= tensor([-78.9090, 366.9398])\n",
      "new w tensor([[-0.4499,  0.7782,  0.8572],\n",
      "        [-0.2105,  0.8665,  0.7727]], requires_grad=True)\n",
      "new b tensor([ 1.0342, -6.5919], requires_grad=True)\n",
      "Loss= tensor(4.2448, grad_fn=<DivBackward0>)\n",
      "i= 318\n",
      "w.grad= tensor([[ -3.2377,  -6.4114,  16.6832],\n",
      "        [  0.8911,   6.1409, -17.8592]])\n",
      "b.grad= tensor([-78.8021, 366.7932])\n",
      "new w tensor([[-0.4497,  0.7785,  0.8564],\n",
      "        [-0.2105,  0.8662,  0.7736]], requires_grad=True)\n",
      "new b tensor([ 1.0381, -6.6102], requires_grad=True)\n",
      "Loss= tensor(4.2133, grad_fn=<DivBackward0>)\n",
      "i= 319\n",
      "w.grad= tensor([[ -3.2208,  -6.3810,  16.6077],\n",
      "        [  0.8734,   6.1128, -17.7870]])\n",
      "b.grad= tensor([-78.6955, 366.6468])\n",
      "new w tensor([[-0.4496,  0.7788,  0.8556],\n",
      "        [-0.2106,  0.8659,  0.7745]], requires_grad=True)\n",
      "new b tensor([ 1.0420, -6.6286], requires_grad=True)\n",
      "Loss= tensor(4.1821, grad_fn=<DivBackward0>)\n",
      "i= 320\n",
      "w.grad= tensor([[ -3.2039,  -6.3509,  16.5325],\n",
      "        [  0.8568,   6.0862, -17.7143]])\n",
      "b.grad= tensor([-78.5893, 366.5005])\n",
      "new w tensor([[-0.4494,  0.7791,  0.8547],\n",
      "        [-0.2106,  0.8656,  0.7753]], requires_grad=True)\n",
      "new b tensor([ 1.0460, -6.6469], requires_grad=True)\n",
      "Loss= tensor(4.1512, grad_fn=<DivBackward0>)\n",
      "i= 321\n",
      "w.grad= tensor([[ -3.1871,  -6.3209,  16.4577],\n",
      "        [  0.8404,   6.0595, -17.6420]])\n",
      "b.grad= tensor([-78.4833, 366.3546])\n",
      "new w tensor([[-0.4493,  0.7794,  0.8539],\n",
      "        [-0.2107,  0.8653,  0.7762]], requires_grad=True)\n",
      "new b tensor([ 1.0499, -6.6652], requires_grad=True)\n",
      "Loss= tensor(4.1206, grad_fn=<DivBackward0>)\n",
      "i= 322\n",
      "w.grad= tensor([[ -3.1720,  -6.2929,  16.3821],\n",
      "        [  0.8233,   6.0324, -17.5705]])\n",
      "b.grad= tensor([-78.3778, 366.2087])\n",
      "new w tensor([[-0.4491,  0.7798,  0.8531],\n",
      "        [-0.2107,  0.8650,  0.7771]], requires_grad=True)\n",
      "new b tensor([ 1.0538, -6.6835], requires_grad=True)\n",
      "Loss= tensor(4.0903, grad_fn=<DivBackward0>)\n",
      "i= 323\n",
      "w.grad= tensor([[ -3.1539,  -6.2616,  16.3089],\n",
      "        [  0.8064,   6.0052, -17.4994]])\n",
      "b.grad= tensor([-78.2725, 366.0631])\n",
      "new w tensor([[-0.4489,  0.7801,  0.8523],\n",
      "        [-0.2107,  0.8647,  0.7780]], requires_grad=True)\n",
      "new b tensor([ 1.0577, -6.7018], requires_grad=True)\n",
      "Loss= tensor(4.0602, grad_fn=<DivBackward0>)\n",
      "i= 324\n",
      "w.grad= tensor([[ -3.1383,  -6.2331,  16.2344],\n",
      "        [  0.7895,   5.9781, -17.4286]])\n",
      "b.grad= tensor([-78.1676, 365.9177])\n",
      "new w tensor([[-0.4488,  0.7804,  0.8515],\n",
      "        [-0.2108,  0.8644,  0.7789]], requires_grad=True)\n",
      "new b tensor([ 1.0616, -6.7201], requires_grad=True)\n",
      "Loss= tensor(4.0305, grad_fn=<DivBackward0>)\n",
      "i= 325\n",
      "w.grad= tensor([[ -3.1220,  -6.2040,  16.1608],\n",
      "        [  0.7732,   5.9516, -17.3578]])\n",
      "b.grad= tensor([-78.0630, 365.7724])\n",
      "new w tensor([[-0.4486,  0.7807,  0.8507],\n",
      "        [-0.2108,  0.8641,  0.7797]], requires_grad=True)\n",
      "new b tensor([ 1.0655, -6.7384], requires_grad=True)\n",
      "Loss= tensor(4.0010, grad_fn=<DivBackward0>)\n",
      "i= 326\n",
      "w.grad= tensor([[ -3.1059,  -6.1750,  16.0875],\n",
      "        [  0.7566,   5.9248, -17.2876]])\n",
      "b.grad= tensor([-77.9587, 365.6273])\n",
      "new w tensor([[-0.4485,  0.7810,  0.8499],\n",
      "        [-0.2109,  0.8638,  0.7806]], requires_grad=True)\n",
      "new b tensor([ 1.0694, -6.7567], requires_grad=True)\n",
      "Loss= tensor(3.9718, grad_fn=<DivBackward0>)\n",
      "i= 327\n",
      "w.grad= tensor([[ -3.0898,  -6.1463,  16.0145],\n",
      "        [  0.7409,   5.8988, -17.2173]])\n",
      "b.grad= tensor([-77.8548, 365.4825])\n",
      "new w tensor([[-0.4483,  0.7813,  0.8491],\n",
      "        [-0.2109,  0.8635,  0.7814]], requires_grad=True)\n",
      "new b tensor([ 1.0733, -6.7750], requires_grad=True)\n",
      "Loss= tensor(3.9429, grad_fn=<DivBackward0>)\n",
      "i= 328\n",
      "w.grad= tensor([[ -3.0734,  -6.1172,  15.9421],\n",
      "        [  0.7249,   5.8727, -17.1475]])\n",
      "b.grad= tensor([-77.7512, 365.3378])\n",
      "new w tensor([[-0.4482,  0.7816,  0.8483],\n",
      "        [-0.2109,  0.8632,  0.7823]], requires_grad=True)\n",
      "new b tensor([ 1.0772, -6.7932], requires_grad=True)\n",
      "Loss= tensor(3.9142, grad_fn=<DivBackward0>)\n",
      "i= 329\n",
      "w.grad= tensor([[ -3.0578,  -6.0891,  15.8695],\n",
      "        [  0.7086,   5.8461, -17.0783]])\n",
      "b.grad= tensor([-77.6479, 365.1933])\n",
      "new w tensor([[-0.4480,  0.7819,  0.8475],\n",
      "        [-0.2110,  0.8629,  0.7832]], requires_grad=True)\n",
      "new b tensor([ 1.0811, -6.8115], requires_grad=True)\n",
      "Loss= tensor(3.8858, grad_fn=<DivBackward0>)\n",
      "i= 330\n",
      "w.grad= tensor([[ -3.0415,  -6.0604,  15.7977],\n",
      "        [  0.6922,   5.8195, -17.0096]])\n",
      "b.grad= tensor([-77.5449, 365.0490])\n",
      "new w tensor([[-0.4479,  0.7822,  0.8467],\n",
      "        [-0.2110,  0.8627,  0.7840]], requires_grad=True)\n",
      "new b tensor([ 1.0850, -6.8298], requires_grad=True)\n",
      "Loss= tensor(3.8577, grad_fn=<DivBackward0>)\n",
      "i= 331\n",
      "w.grad= tensor([[ -3.0255,  -6.0318,  15.7263],\n",
      "        [  0.6768,   5.7939, -16.9405]])\n",
      "b.grad= tensor([-77.4422, 364.9048])\n",
      "new w tensor([[-0.4477,  0.7825,  0.8459],\n",
      "        [-0.2110,  0.8624,  0.7849]], requires_grad=True)\n",
      "new b tensor([ 1.0889, -6.8480], requires_grad=True)\n",
      "Loss= tensor(3.8298, grad_fn=<DivBackward0>)\n",
      "i= 332\n",
      "w.grad= tensor([[ -3.0099,  -6.0039,  15.6549],\n",
      "        [  0.6612,   5.7682, -16.8720]])\n",
      "b.grad= tensor([-77.3399, 364.7609])\n",
      "new w tensor([[-0.4476,  0.7828,  0.8451],\n",
      "        [-0.2111,  0.8621,  0.7857]], requires_grad=True)\n",
      "new b tensor([ 1.0927, -6.8662], requires_grad=True)\n",
      "Loss= tensor(3.8022, grad_fn=<DivBackward0>)\n",
      "i= 333\n",
      "w.grad= tensor([[ -2.9942,  -5.9760,  15.5839],\n",
      "        [  0.6449,   5.7416, -16.8044]])\n",
      "b.grad= tensor([-77.2379, 364.6171])\n",
      "new w tensor([[-0.4474,  0.7831,  0.8443],\n",
      "        [-0.2111,  0.8618,  0.7865]], requires_grad=True)\n",
      "new b tensor([ 1.0966, -6.8845], requires_grad=True)\n",
      "Loss= tensor(3.7749, grad_fn=<DivBackward0>)\n",
      "i= 334\n",
      "w.grad= tensor([[ -2.9789,  -5.9486,  15.5130],\n",
      "        [  0.6302,   5.7167, -16.7361]])\n",
      "b.grad= tensor([-77.1361, 364.4734])\n",
      "new w tensor([[-0.4473,  0.7834,  0.8436],\n",
      "        [-0.2111,  0.8615,  0.7874]], requires_grad=True)\n",
      "new b tensor([ 1.1004, -6.9027], requires_grad=True)\n",
      "Loss= tensor(3.7478, grad_fn=<DivBackward0>)\n",
      "i= 335\n",
      "w.grad= tensor([[ -2.9629,  -5.9205,  15.4429],\n",
      "        [  0.6146,   5.6909, -16.6687]])\n",
      "b.grad= tensor([-77.0347, 364.3300])\n",
      "new w tensor([[-0.4471,  0.7837,  0.8428],\n",
      "        [-0.2112,  0.8612,  0.7882]], requires_grad=True)\n",
      "new b tensor([ 1.1043, -6.9209], requires_grad=True)\n",
      "Loss= tensor(3.7210, grad_fn=<DivBackward0>)\n",
      "i= 336\n",
      "w.grad= tensor([[ -2.9475,  -5.8929,  15.3729],\n",
      "        [  0.5987,   5.6649, -16.6019]])\n",
      "b.grad= tensor([-76.9337, 364.1868])\n",
      "new w tensor([[-0.4470,  0.7840,  0.8420],\n",
      "        [-0.2112,  0.8609,  0.7890]], requires_grad=True)\n",
      "new b tensor([ 1.1081, -6.9391], requires_grad=True)\n",
      "Loss= tensor(3.6944, grad_fn=<DivBackward0>)\n",
      "i= 337\n",
      "w.grad= tensor([[ -2.9323,  -5.8658,  15.3031],\n",
      "        [  0.5840,   5.6403, -16.5347]])\n",
      "b.grad= tensor([-76.8329, 364.0437])\n",
      "new w tensor([[-0.4468,  0.7843,  0.8413],\n",
      "        [-0.2112,  0.8607,  0.7899]], requires_grad=True)\n",
      "new b tensor([ 1.1120, -6.9573], requires_grad=True)\n",
      "Loss= tensor(3.6681, grad_fn=<DivBackward0>)\n",
      "i= 338\n",
      "w.grad= tensor([[ -2.9159,  -5.8374,  15.2344],\n",
      "        [  0.5687,   5.6147, -16.4682]])\n",
      "b.grad= tensor([-76.7324, 363.9008])\n",
      "new w tensor([[-0.4467,  0.7846,  0.8405],\n",
      "        [-0.2112,  0.8604,  0.7907]], requires_grad=True)\n",
      "new b tensor([ 1.1158, -6.9755], requires_grad=True)\n",
      "Loss= tensor(3.6420, grad_fn=<DivBackward0>)\n",
      "i= 339\n",
      "w.grad= tensor([[ -2.9009,  -5.8104,  15.1652],\n",
      "        [  0.5532,   5.5890, -16.4024]])\n",
      "b.grad= tensor([-76.6322, 363.7581])\n",
      "new w tensor([[-0.4465,  0.7849,  0.8397],\n",
      "        [-0.2113,  0.8601,  0.7915]], requires_grad=True)\n",
      "new b tensor([ 1.1196, -6.9937], requires_grad=True)\n",
      "Loss= tensor(3.6161, grad_fn=<DivBackward0>)\n",
      "i= 340\n",
      "w.grad= tensor([[ -2.8864,  -5.7844,  15.0959],\n",
      "        [  0.5381,   5.5637, -16.3365]])\n",
      "b.grad= tensor([-76.5323, 363.6155])\n",
      "new w tensor([[-0.4464,  0.7852,  0.8390],\n",
      "        [-0.2113,  0.8598,  0.7923]], requires_grad=True)\n",
      "new b tensor([ 1.1235, -7.0119], requires_grad=True)\n",
      "Loss= tensor(3.5905, grad_fn=<DivBackward0>)\n",
      "i= 341\n",
      "w.grad= tensor([[ -2.8702,  -5.7563,  15.0282],\n",
      "        [  0.5235,   5.5391, -16.2707]])\n",
      "b.grad= tensor([-76.4328, 363.4731])\n",
      "new w tensor([[-0.4462,  0.7855,  0.8382],\n",
      "        [-0.2113,  0.8595,  0.7931]], requires_grad=True)\n",
      "new b tensor([ 1.1273, -7.0301], requires_grad=True)\n",
      "Loss= tensor(3.5652, grad_fn=<DivBackward0>)\n",
      "i= 342\n",
      "w.grad= tensor([[ -2.8558,  -5.7302,  14.9598],\n",
      "        [  0.5089,   5.5145, -16.2053]])\n",
      "b.grad= tensor([-76.3335, 363.3308])\n",
      "new w tensor([[-0.4461,  0.7857,  0.8375],\n",
      "        [-0.2113,  0.8593,  0.7940]], requires_grad=True)\n",
      "new b tensor([ 1.1311, -7.0482], requires_grad=True)\n",
      "Loss= tensor(3.5401, grad_fn=<DivBackward0>)\n",
      "i= 343\n",
      "w.grad= tensor([[ -2.8406,  -5.7035,  14.8921],\n",
      "        [  0.4937,   5.4891, -16.1406]])\n",
      "b.grad= tensor([-76.2345, 363.1888])\n",
      "new w tensor([[-0.4460,  0.7860,  0.8367],\n",
      "        [-0.2114,  0.8590,  0.7948]], requires_grad=True)\n",
      "new b tensor([ 1.1349, -7.0664], requires_grad=True)\n",
      "Loss= tensor(3.5152, grad_fn=<DivBackward0>)\n",
      "i= 344\n",
      "w.grad= tensor([[ -2.8256,  -5.6770,  14.8246],\n",
      "        [  0.4788,   5.4640, -16.0761]])\n",
      "b.grad= tensor([-76.1359, 363.0469])\n",
      "new w tensor([[-0.4458,  0.7863,  0.8360],\n",
      "        [-0.2114,  0.8587,  0.7956]], requires_grad=True)\n",
      "new b tensor([ 1.1387, -7.0845], requires_grad=True)\n",
      "Loss= tensor(3.4905, grad_fn=<DivBackward0>)\n",
      "i= 345\n",
      "w.grad= tensor([[ -2.8105,  -5.6504,  14.7576],\n",
      "        [  0.4648,   5.4401, -16.0114]])\n",
      "b.grad= tensor([-76.0375, 362.9051])\n",
      "new w tensor([[-0.4457,  0.7866,  0.8353],\n",
      "        [-0.2114,  0.8584,  0.7964]], requires_grad=True)\n",
      "new b tensor([ 1.1425, -7.1027], requires_grad=True)\n",
      "Loss= tensor(3.4661, grad_fn=<DivBackward0>)\n",
      "i= 346\n",
      "w.grad= tensor([[ -2.7958,  -5.6243,  14.6907],\n",
      "        [  0.4499,   5.4151, -15.9475]])\n",
      "b.grad= tensor([-75.9394, 362.7635])\n",
      "new w tensor([[-0.4455,  0.7869,  0.8345],\n",
      "        [-0.2114,  0.8582,  0.7972]], requires_grad=True)\n",
      "new b tensor([ 1.1463, -7.1208], requires_grad=True)\n",
      "Loss= tensor(3.4419, grad_fn=<DivBackward0>)\n",
      "i= 347\n",
      "w.grad= tensor([[ -2.7807,  -5.5978,  14.6244],\n",
      "        [  0.4358,   5.3910, -15.8835]])\n",
      "b.grad= tensor([-75.8416, 362.6221])\n",
      "new w tensor([[-0.4454,  0.7872,  0.8338],\n",
      "        [-0.2115,  0.8579,  0.7980]], requires_grad=True)\n",
      "new b tensor([ 1.1501, -7.1390], requires_grad=True)\n",
      "Loss= tensor(3.4180, grad_fn=<DivBackward0>)\n",
      "i= 348\n",
      "w.grad= tensor([[ -2.7661,  -5.5720,  14.5581],\n",
      "        [  0.4216,   5.3669, -15.8200]])\n",
      "b.grad= tensor([-75.7441, 362.4809])\n",
      "new w tensor([[-0.4453,  0.7874,  0.8331],\n",
      "        [-0.2115,  0.8576,  0.7987]], requires_grad=True)\n",
      "new b tensor([ 1.1539, -7.1571], requires_grad=True)\n",
      "Loss= tensor(3.3942, grad_fn=<DivBackward0>)\n",
      "i= 349\n",
      "w.grad= tensor([[ -2.7512,  -5.5457,  14.4925],\n",
      "        [  0.4073,   5.3427, -15.7568]])\n",
      "b.grad= tensor([-75.6469, 362.3398])\n",
      "new w tensor([[-0.4451,  0.7877,  0.8323],\n",
      "        [-0.2115,  0.8574,  0.7995]], requires_grad=True)\n",
      "new b tensor([ 1.1577, -7.1752], requires_grad=True)\n",
      "Loss= tensor(3.3707, grad_fn=<DivBackward0>)\n",
      "i= 350\n",
      "w.grad= tensor([[ -2.7365,  -5.5197,  14.4270],\n",
      "        [  0.3927,   5.3180, -15.6942]])\n",
      "b.grad= tensor([-75.5500, 362.1988])\n",
      "new w tensor([[-0.4450,  0.7880,  0.8316],\n",
      "        [-0.2115,  0.8571,  0.8003]], requires_grad=True)\n",
      "new b tensor([ 1.1615, -7.1933], requires_grad=True)\n",
      "Loss= tensor(3.3474, grad_fn=<DivBackward0>)\n",
      "i= 351\n",
      "w.grad= tensor([[ -2.7227,  -5.4951,  14.3611],\n",
      "        [  0.3792,   5.2947, -15.6312]])\n",
      "b.grad= tensor([-75.4534, 362.0580])\n",
      "new w tensor([[-0.4449,  0.7883,  0.8309],\n",
      "        [-0.2115,  0.8568,  0.8011]], requires_grad=True)\n",
      "new b tensor([ 1.1652, -7.2114], requires_grad=True)\n",
      "Loss= tensor(3.3244, grad_fn=<DivBackward0>)\n",
      "i= 352\n",
      "w.grad= tensor([[ -2.7074,  -5.4686,  14.2967],\n",
      "        [  0.3647,   5.2702, -15.5692]])\n",
      "b.grad= tensor([-75.3571, 361.9174])\n",
      "new w tensor([[-0.4447,  0.7885,  0.8302],\n",
      "        [-0.2116,  0.8566,  0.8019]], requires_grad=True)\n",
      "new b tensor([ 1.1690, -7.2295], requires_grad=True)\n",
      "Loss= tensor(3.3015, grad_fn=<DivBackward0>)\n",
      "i= 353\n",
      "w.grad= tensor([[ -2.6937,  -5.4439,  14.2316],\n",
      "        [  0.3507,   5.2462, -15.5072]])\n",
      "b.grad= tensor([-75.2610, 361.7769])\n",
      "new w tensor([[-0.4446,  0.7888,  0.8295],\n",
      "        [-0.2116,  0.8563,  0.8027]], requires_grad=True)\n",
      "new b tensor([ 1.1728, -7.2476], requires_grad=True)\n",
      "Loss= tensor(3.2789, grad_fn=<DivBackward0>)\n",
      "i= 354\n",
      "w.grad= tensor([[ -2.6790,  -5.4183,  14.1674],\n",
      "        [  0.3375,   5.2234, -15.4450]])\n",
      "b.grad= tensor([-75.1652, 361.6366])\n",
      "new w tensor([[-0.4444,  0.7891,  0.8288],\n",
      "        [-0.2116,  0.8561,  0.8034]], requires_grad=True)\n",
      "new b tensor([ 1.1765, -7.2657], requires_grad=True)\n",
      "Loss= tensor(3.2565, grad_fn=<DivBackward0>)\n",
      "i= 355\n",
      "w.grad= tensor([[ -2.6644,  -5.3928,  14.1035],\n",
      "        [  0.3234,   5.1994, -15.3839]])\n",
      "b.grad= tensor([-75.0697, 361.4964])\n",
      "new w tensor([[-0.4443,  0.7894,  0.8281],\n",
      "        [-0.2116,  0.8558,  0.8042]], requires_grad=True)\n",
      "new b tensor([ 1.1803, -7.2837], requires_grad=True)\n",
      "Loss= tensor(3.2343, grad_fn=<DivBackward0>)\n",
      "i= 356\n",
      "w.grad= tensor([[ -2.6504,  -5.3680,  14.0396],\n",
      "        [  0.3092,   5.1752, -15.3230]])\n",
      "b.grad= tensor([-74.9745, 361.3564])\n",
      "new w tensor([[-0.4442,  0.7896,  0.8274],\n",
      "        [-0.2116,  0.8555,  0.8050]], requires_grad=True)\n",
      "new b tensor([ 1.1840, -7.3018], requires_grad=True)\n",
      "Loss= tensor(3.2122, grad_fn=<DivBackward0>)\n",
      "i= 357\n",
      "w.grad= tensor([[ -2.6363,  -5.3430,  13.9761],\n",
      "        [  0.2967,   5.1531, -15.2614]])\n",
      "b.grad= tensor([-74.8796, 361.2165])\n",
      "new w tensor([[-0.4441,  0.7899,  0.8267],\n",
      "        [-0.2116,  0.8553,  0.8057]], requires_grad=True)\n",
      "new b tensor([ 1.1878, -7.3199], requires_grad=True)\n",
      "Loss= tensor(3.1905, grad_fn=<DivBackward0>)\n",
      "i= 358\n",
      "w.grad= tensor([[ -2.6221,  -5.3182,  13.9129],\n",
      "        [  0.2822,   5.1286, -15.2014]])\n",
      "b.grad= tensor([-74.7849, 361.0768])\n",
      "new w tensor([[-0.4439,  0.7902,  0.8260],\n",
      "        [-0.2117,  0.8550,  0.8065]], requires_grad=True)\n",
      "new b tensor([ 1.1915, -7.3379], requires_grad=True)\n",
      "Loss= tensor(3.1689, grad_fn=<DivBackward0>)\n",
      "i= 359\n",
      "w.grad= tensor([[ -2.6085,  -5.2939,  13.8498],\n",
      "        [  0.2700,   5.1069, -15.1402]])\n",
      "b.grad= tensor([-74.6906, 360.9372])\n",
      "new w tensor([[-0.4438,  0.7904,  0.8253],\n",
      "        [-0.2117,  0.8548,  0.8072]], requires_grad=True)\n",
      "new b tensor([ 1.1952, -7.3560], requires_grad=True)\n",
      "Loss= tensor(3.1475, grad_fn=<DivBackward0>)\n",
      "i= 360\n",
      "w.grad= tensor([[ -2.5940,  -5.2686,  13.7876],\n",
      "        [  0.2561,   5.0833, -15.0806]])\n",
      "b.grad= tensor([-74.5964, 360.7978])\n",
      "new w tensor([[-0.4437,  0.7907,  0.8246],\n",
      "        [-0.2117,  0.8545,  0.8080]], requires_grad=True)\n",
      "new b tensor([ 1.1990, -7.3740], requires_grad=True)\n",
      "Loss= tensor(3.1263, grad_fn=<DivBackward0>)\n",
      "i= 361\n",
      "w.grad= tensor([[ -2.5807,  -5.2449,  13.7248],\n",
      "        [  0.2423,   5.0597, -15.0210]])\n",
      "b.grad= tensor([-74.5026, 360.6584])\n",
      "new w tensor([[-0.4435,  0.7909,  0.8239],\n",
      "        [-0.2117,  0.8543,  0.8087]], requires_grad=True)\n",
      "new b tensor([ 1.2027, -7.3921], requires_grad=True)\n",
      "Loss= tensor(3.1053, grad_fn=<DivBackward0>)\n",
      "i= 362\n",
      "w.grad= tensor([[ -2.5657,  -5.2193,  13.6636],\n",
      "        [  0.2298,   5.0377, -14.9610]])\n",
      "b.grad= tensor([-74.4090, 360.5193])\n",
      "new w tensor([[-0.4434,  0.7912,  0.8232],\n",
      "        [-0.2117,  0.8540,  0.8095]], requires_grad=True)\n",
      "new b tensor([ 1.2064, -7.4101], requires_grad=True)\n",
      "Loss= tensor(3.0845, grad_fn=<DivBackward0>)\n",
      "i= 363\n",
      "w.grad= tensor([[ -2.5532,  -5.1964,  13.6010],\n",
      "        [  0.2159,   5.0140, -14.9024]])\n",
      "b.grad= tensor([-74.3158, 360.3803])\n",
      "new w tensor([[-0.4433,  0.7915,  0.8225],\n",
      "        [-0.2117,  0.8538,  0.8102]], requires_grad=True)\n",
      "new b tensor([ 1.2101, -7.4281], requires_grad=True)\n",
      "Loss= tensor(3.0640, grad_fn=<DivBackward0>)\n",
      "i= 364\n",
      "w.grad= tensor([[ -2.5387,  -5.1716,  13.5400],\n",
      "        [  0.2032,   4.9917, -14.8431]])\n",
      "b.grad= tensor([-74.2227, 360.2414])\n",
      "new w tensor([[-0.4431,  0.7917,  0.8219],\n",
      "        [-0.2117,  0.8535,  0.8110]], requires_grad=True)\n",
      "new b tensor([ 1.2139, -7.4461], requires_grad=True)\n",
      "Loss= tensor(3.0436, grad_fn=<DivBackward0>)\n",
      "i= 365\n",
      "w.grad= tensor([[ -2.5252,  -5.1476,  13.4788],\n",
      "        [  0.1904,   4.9694, -14.7843]])\n",
      "b.grad= tensor([-74.1300, 360.1027])\n",
      "new w tensor([[-0.4430,  0.7920,  0.8212],\n",
      "        [-0.2117,  0.8533,  0.8117]], requires_grad=True)\n",
      "new b tensor([ 1.2176, -7.4641], requires_grad=True)\n",
      "Loss= tensor(3.0234, grad_fn=<DivBackward0>)\n",
      "i= 366\n",
      "w.grad= tensor([[ -2.5118,  -5.1239,  13.4178],\n",
      "        [  0.1774,   4.9469, -14.7259]])\n",
      "b.grad= tensor([-74.0375, 359.9641])\n",
      "new w tensor([[-0.4429,  0.7922,  0.8205],\n",
      "        [-0.2117,  0.8530,  0.8125]], requires_grad=True)\n",
      "new b tensor([ 1.2213, -7.4821], requires_grad=True)\n",
      "Loss= tensor(3.0034, grad_fn=<DivBackward0>)\n",
      "i= 367\n",
      "w.grad= tensor([[ -2.4985,  -5.1004,  13.3570],\n",
      "        [  0.1641,   4.9239, -14.6680]])\n",
      "b.grad= tensor([-73.9453, 359.8257])\n",
      "new w tensor([[-0.4428,  0.7925,  0.8198],\n",
      "        [-0.2118,  0.8528,  0.8132]], requires_grad=True)\n",
      "new b tensor([ 1.2250, -7.5001], requires_grad=True)\n",
      "Loss= tensor(2.9835, grad_fn=<DivBackward0>)\n",
      "i= 368\n",
      "w.grad= tensor([[ -2.4847,  -5.0764,  13.2969],\n",
      "        [  0.1518,   4.9021, -14.6099]])\n",
      "b.grad= tensor([-73.8533, 359.6873])\n",
      "new w tensor([[-0.4426,  0.7927,  0.8192],\n",
      "        [-0.2118,  0.8525,  0.8139]], requires_grad=True)\n",
      "new b tensor([ 1.2286, -7.5181], requires_grad=True)\n",
      "Loss= tensor(2.9639, grad_fn=<DivBackward0>)\n",
      "i= 369\n",
      "w.grad= tensor([[ -2.4714,  -5.0531,  13.2367],\n",
      "        [  0.1388,   4.8797, -14.5525]])\n",
      "b.grad= tensor([-73.7616, 359.5492])\n",
      "new w tensor([[-0.4425,  0.7930,  0.8185],\n",
      "        [-0.2118,  0.8523,  0.8146]], requires_grad=True)\n",
      "new b tensor([ 1.2323, -7.5361], requires_grad=True)\n",
      "Loss= tensor(2.9445, grad_fn=<DivBackward0>)\n",
      "i= 370\n",
      "w.grad= tensor([[ -2.4578,  -5.0293,  13.1771],\n",
      "        [  0.1262,   4.8577, -14.4951]])\n",
      "b.grad= tensor([-73.6702, 359.4111])\n",
      "new w tensor([[-0.4424,  0.7933,  0.8179],\n",
      "        [-0.2118,  0.8520,  0.8154]], requires_grad=True)\n",
      "new b tensor([ 1.2360, -7.5540], requires_grad=True)\n",
      "Loss= tensor(2.9252, grad_fn=<DivBackward0>)\n",
      "i= 371\n",
      "w.grad= tensor([[ -2.4443,  -5.0058,  13.1178],\n",
      "        [  0.1135,   4.8356, -14.4381]])\n",
      "b.grad= tensor([-73.5790, 359.2732])\n",
      "new w tensor([[-0.4423,  0.7935,  0.8172],\n",
      "        [-0.2118,  0.8518,  0.8161]], requires_grad=True)\n",
      "new b tensor([ 1.2397, -7.5720], requires_grad=True)\n",
      "Loss= tensor(2.9062, grad_fn=<DivBackward0>)\n",
      "i= 372\n",
      "w.grad= tensor([[ -2.4315,  -4.9829,  13.0583],\n",
      "        [  0.1015,   4.8142, -14.3809]])\n",
      "b.grad= tensor([-73.4881, 359.1354])\n",
      "new w tensor([[-0.4422,  0.7938,  0.8165],\n",
      "        [-0.2118,  0.8515,  0.8168]], requires_grad=True)\n",
      "new b tensor([ 1.2434, -7.5900], requires_grad=True)\n",
      "Loss= tensor(2.8873, grad_fn=<DivBackward0>)\n",
      "i= 373\n",
      "w.grad= tensor([[ -2.4187,  -4.9603,  12.9991],\n",
      "        [  0.0895,   4.7932, -14.3240]])\n",
      "b.grad= tensor([-73.3974, 358.9977])\n",
      "new w tensor([[-0.4420,  0.7940,  0.8159],\n",
      "        [-0.2118,  0.8513,  0.8175]], requires_grad=True)\n",
      "new b tensor([ 1.2470, -7.6079], requires_grad=True)\n",
      "Loss= tensor(2.8686, grad_fn=<DivBackward0>)\n",
      "i= 374\n",
      "w.grad= tensor([[ -2.4050,  -4.9367,  12.9408],\n",
      "        [  0.0759,   4.7700, -14.2685]])\n",
      "b.grad= tensor([-73.3070, 358.8602])\n",
      "new w tensor([[-0.4419,  0.7942,  0.8153],\n",
      "        [-0.2118,  0.8511,  0.8182]], requires_grad=True)\n",
      "new b tensor([ 1.2507, -7.6258], requires_grad=True)\n",
      "Loss= tensor(2.8500, grad_fn=<DivBackward0>)\n",
      "i= 375\n",
      "w.grad= tensor([[ -2.3920,  -4.9138,  12.8824],\n",
      "        [  0.0641,   4.7490, -14.2121]])\n",
      "b.grad= tensor([-73.2169, 358.7228])\n",
      "new w tensor([[-0.4418,  0.7945,  0.8146],\n",
      "        [-0.2118,  0.8508,  0.8190]], requires_grad=True)\n",
      "new b tensor([ 1.2544, -7.6438], requires_grad=True)\n",
      "Loss= tensor(2.8317, grad_fn=<DivBackward0>)\n",
      "i= 376\n",
      "w.grad= tensor([[ -2.3789,  -4.8909,  12.8244],\n",
      "        [  0.0518,   4.7274, -14.1564]])\n",
      "b.grad= tensor([-73.1270, 358.5855])\n",
      "new w tensor([[-0.4417,  0.7947,  0.8140],\n",
      "        [-0.2118,  0.8506,  0.8197]], requires_grad=True)\n",
      "new b tensor([ 1.2580, -7.6617], requires_grad=True)\n",
      "Loss= tensor(2.8135, grad_fn=<DivBackward0>)\n",
      "i= 377\n",
      "w.grad= tensor([[ -2.3665,  -4.8687,  12.7662],\n",
      "        [  0.0391,   4.7051, -14.1013]])\n",
      "b.grad= tensor([-73.0373, 358.4484])\n",
      "new w tensor([[-0.4416,  0.7950,  0.8133],\n",
      "        [-0.2118,  0.8504,  0.8204]], requires_grad=True)\n",
      "new b tensor([ 1.2617, -7.6796], requires_grad=True)\n",
      "Loss= tensor(2.7955, grad_fn=<DivBackward0>)\n",
      "i= 378\n",
      "w.grad= tensor([[ -2.3535,  -4.8461,  12.7087],\n",
      "        [  0.0277,   4.6848, -14.0455]])\n",
      "b.grad= tensor([-72.9479, 358.3114])\n",
      "new w tensor([[-0.4414,  0.7952,  0.8127],\n",
      "        [-0.2118,  0.8501,  0.8211]], requires_grad=True)\n",
      "new b tensor([ 1.2653, -7.6976], requires_grad=True)\n",
      "Loss= tensor(2.7777, grad_fn=<DivBackward0>)\n",
      "i= 379\n",
      "w.grad= tensor([[ -2.3404,  -4.8232,  12.6515],\n",
      "        [  0.0149,   4.6628, -13.9910]])\n",
      "b.grad= tensor([-72.8588, 358.1745])\n",
      "new w tensor([[-0.4413,  0.7955,  0.8121],\n",
      "        [-0.2118,  0.8499,  0.8218]], requires_grad=True)\n",
      "new b tensor([ 1.2690, -7.7155], requires_grad=True)\n",
      "Loss= tensor(2.7600, grad_fn=<DivBackward0>)\n",
      "i= 380\n",
      "w.grad= tensor([[-2.3280e+00, -4.8014e+00,  1.2594e+01],\n",
      "        [ 3.8471e-03,  4.6428e+00, -1.3936e+01]])\n",
      "b.grad= tensor([-72.7699, 358.0377])\n",
      "new w tensor([[-0.4412,  0.7957,  0.8114],\n",
      "        [-0.2118,  0.8497,  0.8225]], requires_grad=True)\n",
      "new b tensor([ 1.2726, -7.7334], requires_grad=True)\n",
      "Loss= tensor(2.7425, grad_fn=<DivBackward0>)\n",
      "i= 381\n",
      "w.grad= tensor([[-2.3148e+00, -4.7784e+00,  1.2538e+01],\n",
      "        [-8.7357e-03,  4.6209e+00, -1.3882e+01]])\n",
      "b.grad= tensor([-72.6812, 357.9010])\n",
      "new w tensor([[-0.4411,  0.7959,  0.8108],\n",
      "        [-0.2118,  0.8494,  0.8232]], requires_grad=True)\n",
      "new b tensor([ 1.2762, -7.7513], requires_grad=True)\n",
      "Loss= tensor(2.7252, grad_fn=<DivBackward0>)\n",
      "i= 382\n",
      "w.grad= tensor([[ -2.3028,  -4.7570,  12.4808],\n",
      "        [ -0.0204,   4.6003, -13.8271]])\n",
      "b.grad= tensor([-72.5928, 357.7645])\n",
      "new w tensor([[-0.4410,  0.7962,  0.8102],\n",
      "        [-0.2118,  0.8492,  0.8239]], requires_grad=True)\n",
      "new b tensor([ 1.2799, -7.7691], requires_grad=True)\n",
      "Loss= tensor(2.7080, grad_fn=<DivBackward0>)\n",
      "i= 383\n",
      "w.grad= tensor([[ -2.2901,  -4.7349,  12.4246],\n",
      "        [ -0.0324,   4.5791, -13.7733]])\n",
      "b.grad= tensor([-72.5047, 357.6281])\n",
      "new w tensor([[-0.4409,  0.7964,  0.8096],\n",
      "        [-0.2118,  0.8490,  0.8245]], requires_grad=True)\n",
      "new b tensor([ 1.2835, -7.7870], requires_grad=True)\n",
      "Loss= tensor(2.6910, grad_fn=<DivBackward0>)\n",
      "i= 384\n",
      "w.grad= tensor([[ -2.2776,  -4.7130,  12.3686],\n",
      "        [ -0.0439,   4.5586, -13.7194]])\n",
      "b.grad= tensor([-72.4168, 357.4918])\n",
      "new w tensor([[-0.4407,  0.7967,  0.8089],\n",
      "        [-0.2118,  0.8487,  0.8252]], requires_grad=True)\n",
      "new b tensor([ 1.2871, -7.8049], requires_grad=True)\n",
      "Loss= tensor(2.6742, grad_fn=<DivBackward0>)\n",
      "i= 385\n",
      "w.grad= tensor([[ -2.2647,  -4.6906,  12.3132],\n",
      "        [ -0.0554,   4.5381, -13.6658]])\n",
      "b.grad= tensor([-72.3291, 357.3556])\n",
      "new w tensor([[-0.4406,  0.7969,  0.8083],\n",
      "        [-0.2118,  0.8485,  0.8259]], requires_grad=True)\n",
      "new b tensor([ 1.2907, -7.8228], requires_grad=True)\n",
      "Loss= tensor(2.6575, grad_fn=<DivBackward0>)\n",
      "i= 386\n",
      "w.grad= tensor([[ -2.2521,  -4.6686,  12.2578],\n",
      "        [ -0.0668,   4.5178, -13.6124]])\n",
      "b.grad= tensor([-72.2417, 357.2195])\n",
      "new w tensor([[-0.4405,  0.7971,  0.8077],\n",
      "        [-0.2118,  0.8483,  0.8266]], requires_grad=True)\n",
      "new b tensor([ 1.2943, -7.8406], requires_grad=True)\n",
      "Loss= tensor(2.6410, grad_fn=<DivBackward0>)\n",
      "i= 387\n",
      "w.grad= tensor([[ -2.2407,  -4.6480,  12.2020],\n",
      "        [ -0.0789,   4.4966, -13.5597]])\n",
      "b.grad= tensor([-72.1545, 357.0835])\n",
      "new w tensor([[-0.4404,  0.7974,  0.8071],\n",
      "        [-0.2118,  0.8481,  0.8273]], requires_grad=True)\n",
      "new b tensor([ 1.2980, -7.8585], requires_grad=True)\n",
      "Loss= tensor(2.6246, grad_fn=<DivBackward0>)\n",
      "i= 388\n",
      "w.grad= tensor([[ -2.2280,  -4.6260,  12.1473],\n",
      "        [ -0.0901,   4.4767, -13.5067]])\n",
      "b.grad= tensor([-72.0675, 356.9477])\n",
      "new w tensor([[-0.4403,  0.7976,  0.8065],\n",
      "        [-0.2118,  0.8478,  0.8279]], requires_grad=True)\n",
      "new b tensor([ 1.3016, -7.8763], requires_grad=True)\n",
      "Loss= tensor(2.6084, grad_fn=<DivBackward0>)\n",
      "i= 389\n",
      "w.grad= tensor([[ -2.2153,  -4.6040,  12.0930],\n",
      "        [ -0.1015,   4.4565, -13.4542]])\n",
      "b.grad= tensor([-71.9808, 356.8120])\n",
      "new w tensor([[-0.4402,  0.7978,  0.8059],\n",
      "        [-0.2118,  0.8476,  0.8286]], requires_grad=True)\n",
      "new b tensor([ 1.3052, -7.8942], requires_grad=True)\n",
      "Loss= tensor(2.5924, grad_fn=<DivBackward0>)\n",
      "i= 390\n",
      "w.grad= tensor([[ -2.2037,  -4.5834,  12.0380],\n",
      "        [ -0.1128,   4.4363, -13.4018]])\n",
      "b.grad= tensor([-71.8943, 356.6763])\n",
      "new w tensor([[-0.4401,  0.7980,  0.8053],\n",
      "        [-0.2118,  0.8474,  0.8293]], requires_grad=True)\n",
      "new b tensor([ 1.3088, -7.9120], requires_grad=True)\n",
      "Loss= tensor(2.5765, grad_fn=<DivBackward0>)\n",
      "i= 391\n",
      "w.grad= tensor([[ -2.1913,  -4.5619,  11.9840],\n",
      "        [ -0.1240,   4.4165, -13.3497]])\n",
      "b.grad= tensor([-71.8081, 356.5408])\n",
      "new w tensor([[-0.4400,  0.7983,  0.8047],\n",
      "        [-0.2118,  0.8472,  0.8300]], requires_grad=True)\n",
      "new b tensor([ 1.3123, -7.9298], requires_grad=True)\n",
      "Loss= tensor(2.5608, grad_fn=<DivBackward0>)\n",
      "i= 392\n",
      "w.grad= tensor([[ -2.1791,  -4.5406,  11.9301],\n",
      "        [ -0.1357,   4.3958, -13.2983]])\n",
      "b.grad= tensor([-71.7220, 356.4054])\n",
      "new w tensor([[-0.4399,  0.7985,  0.8041],\n",
      "        [-0.2118,  0.8470,  0.8306]], requires_grad=True)\n",
      "new b tensor([ 1.3159, -7.9477], requires_grad=True)\n",
      "Loss= tensor(2.5452, grad_fn=<DivBackward0>)\n",
      "i= 393\n",
      "w.grad= tensor([[ -2.1675,  -4.5200,  11.8761],\n",
      "        [ -0.1464,   4.3766, -13.2463]])\n",
      "b.grad= tensor([-71.6363, 356.2701])\n",
      "new w tensor([[-0.4398,  0.7987,  0.8035],\n",
      "        [-0.2118,  0.8467,  0.8313]], requires_grad=True)\n",
      "new b tensor([ 1.3195, -7.9655], requires_grad=True)\n",
      "Loss= tensor(2.5297, grad_fn=<DivBackward0>)\n",
      "i= 394\n",
      "w.grad= tensor([[ -2.1546,  -4.4980,  11.8233],\n",
      "        [ -0.1583,   4.3559, -13.1955]])\n",
      "b.grad= tensor([-71.5507, 356.1349])\n",
      "new w tensor([[-0.4396,  0.7989,  0.8029],\n",
      "        [-0.2117,  0.8465,  0.8319]], requires_grad=True)\n",
      "new b tensor([ 1.3231, -7.9833], requires_grad=True)\n",
      "Loss= tensor(2.5144, grad_fn=<DivBackward0>)\n",
      "i= 395\n",
      "w.grad= tensor([[ -2.1435,  -4.4781,  11.7695],\n",
      "        [ -0.1689,   4.3367, -13.1441]])\n",
      "b.grad= tensor([-71.4654, 355.9999])\n",
      "new w tensor([[-0.4395,  0.7992,  0.8023],\n",
      "        [-0.2117,  0.8463,  0.8326]], requires_grad=True)\n",
      "new b tensor([ 1.3267, -8.0011], requires_grad=True)\n",
      "Loss= tensor(2.4993, grad_fn=<DivBackward0>)\n",
      "i= 396\n",
      "w.grad= tensor([[ -2.1316,  -4.4571,  11.7167],\n",
      "        [ -0.1800,   4.3170, -13.0932]])\n",
      "b.grad= tensor([-71.3803, 355.8649])\n",
      "new w tensor([[-0.4394,  0.7994,  0.8017],\n",
      "        [-0.2117,  0.8461,  0.8333]], requires_grad=True)\n",
      "new b tensor([ 1.3302, -8.0189], requires_grad=True)\n",
      "Loss= tensor(2.4843, grad_fn=<DivBackward0>)\n",
      "i= 397\n",
      "w.grad= tensor([[ -2.1197,  -4.4364,  11.6640],\n",
      "        [ -0.1910,   4.2971, -13.0427]])\n",
      "b.grad= tensor([-71.2954, 355.7300])\n",
      "new w tensor([[-0.4393,  0.7996,  0.8011],\n",
      "        [-0.2117,  0.8459,  0.8339]], requires_grad=True)\n",
      "new b tensor([ 1.3338, -8.0367], requires_grad=True)\n",
      "Loss= tensor(2.4695, grad_fn=<DivBackward0>)\n",
      "i= 398\n",
      "w.grad= tensor([[ -2.1079,  -4.4158,  11.6115],\n",
      "        [ -0.2019,   4.2778, -12.9923]])\n",
      "b.grad= tensor([-71.2108, 355.5952])\n",
      "new w tensor([[-0.4392,  0.7998,  0.8006],\n",
      "        [-0.2117,  0.8457,  0.8346]], requires_grad=True)\n",
      "new b tensor([ 1.3374, -8.0544], requires_grad=True)\n",
      "Loss= tensor(2.4548, grad_fn=<DivBackward0>)\n",
      "i= 399\n",
      "w.grad= tensor([[ -2.0966,  -4.3956,  11.5591],\n",
      "        [ -0.2123,   4.2589, -12.9418]])\n",
      "b.grad= tensor([-71.1264, 355.4606])\n",
      "new w tensor([[-0.4391,  0.8001,  0.8000],\n",
      "        [-0.2117,  0.8454,  0.8352]], requires_grad=True)\n",
      "new b tensor([ 1.3409, -8.0722], requires_grad=True)\n",
      "Loss= tensor(2.4402, grad_fn=<DivBackward0>)\n",
      "i= 400\n",
      "w.grad= tensor([[ -2.0847,  -4.3749,  11.5074],\n",
      "        [ -0.2233,   4.2393, -12.8920]])\n",
      "b.grad= tensor([-71.0422, 355.3260])\n",
      "new w tensor([[-0.4390,  0.8003,  0.7994],\n",
      "        [-0.2117,  0.8452,  0.8358]], requires_grad=True)\n",
      "new b tensor([ 1.3445, -8.0900], requires_grad=True)\n",
      "Loss= tensor(2.4258, grad_fn=<DivBackward0>)\n",
      "i= 401\n",
      "w.grad= tensor([[ -2.0737,  -4.3554,  11.4551],\n",
      "        [ -0.2341,   4.2202, -12.8424]])\n",
      "b.grad= tensor([-70.9582, 355.1915])\n",
      "new w tensor([[-0.4389,  0.8005,  0.7988],\n",
      "        [-0.2117,  0.8450,  0.8365]], requires_grad=True)\n",
      "new b tensor([ 1.3480, -8.1077], requires_grad=True)\n",
      "Loss= tensor(2.4115, grad_fn=<DivBackward0>)\n",
      "i= 402\n",
      "w.grad= tensor([[ -2.0616,  -4.3345,  11.4040],\n",
      "        [ -0.2449,   4.2007, -12.7930]])\n",
      "b.grad= tensor([-70.8745, 355.0572])\n",
      "new w tensor([[-0.4388,  0.8007,  0.7983],\n",
      "        [-0.2117,  0.8448,  0.8371]], requires_grad=True)\n",
      "new b tensor([ 1.3516, -8.1255], requires_grad=True)\n",
      "Loss= tensor(2.3973, grad_fn=<DivBackward0>)\n",
      "i= 403\n",
      "w.grad= tensor([[ -2.0500,  -4.3142,  11.3529],\n",
      "        [ -0.2552,   4.1820, -12.7436]])\n",
      "b.grad= tensor([-70.7909, 354.9229])\n",
      "new w tensor([[-0.4387,  0.8009,  0.7977],\n",
      "        [-0.2117,  0.8446,  0.8378]], requires_grad=True)\n",
      "new b tensor([ 1.3551, -8.1432], requires_grad=True)\n",
      "Loss= tensor(2.3833, grad_fn=<DivBackward0>)\n",
      "i= 404\n",
      "w.grad= tensor([[ -2.0392,  -4.2950,  11.3014],\n",
      "        [ -0.2665,   4.1622, -12.6951]])\n",
      "b.grad= tensor([-70.7076, 354.7887])\n",
      "new w tensor([[-0.4386,  0.8011,  0.7971],\n",
      "        [-0.2116,  0.8444,  0.8384]], requires_grad=True)\n",
      "new b tensor([ 1.3586, -8.1610], requires_grad=True)\n",
      "Loss= tensor(2.3695, grad_fn=<DivBackward0>)\n",
      "i= 405\n",
      "w.grad= tensor([[ -2.0277,  -4.2750,  11.2507],\n",
      "        [ -0.2765,   4.1440, -12.6459]])\n",
      "b.grad= tensor([-70.6245, 354.6546])\n",
      "new w tensor([[-0.4385,  0.8014,  0.7966],\n",
      "        [-0.2116,  0.8442,  0.8390]], requires_grad=True)\n",
      "new b tensor([ 1.3622, -8.1787], requires_grad=True)\n",
      "Loss= tensor(2.3557, grad_fn=<DivBackward0>)\n",
      "i= 406\n",
      "w.grad= tensor([[ -2.0165,  -4.2552,  11.2001],\n",
      "        [ -0.2866,   4.1256, -12.5971]])\n",
      "b.grad= tensor([-70.5417, 354.5206])\n",
      "new w tensor([[-0.4384,  0.8016,  0.7960],\n",
      "        [-0.2116,  0.8440,  0.8397]], requires_grad=True)\n",
      "new b tensor([ 1.3657, -8.1964], requires_grad=True)\n",
      "Loss= tensor(2.3421, grad_fn=<DivBackward0>)\n",
      "i= 407\n",
      "w.grad= tensor([[ -2.0051,  -4.2354,  11.1498],\n",
      "        [ -0.2971,   4.1069, -12.5488]])\n",
      "b.grad= tensor([-70.4590, 354.3867])\n",
      "new w tensor([[-0.4383,  0.8018,  0.7955],\n",
      "        [-0.2116,  0.8438,  0.8403]], requires_grad=True)\n",
      "new b tensor([ 1.3692, -8.2141], requires_grad=True)\n",
      "Loss= tensor(2.3287, grad_fn=<DivBackward0>)\n",
      "i= 408\n",
      "w.grad= tensor([[ -1.9938,  -4.2156,  11.0999],\n",
      "        [ -0.3077,   4.0879, -12.5009]])\n",
      "b.grad= tensor([-70.3766, 354.2529])\n",
      "new w tensor([[-0.4382,  0.8020,  0.7949],\n",
      "        [-0.2116,  0.8436,  0.8409]], requires_grad=True)\n",
      "new b tensor([ 1.3727, -8.2319], requires_grad=True)\n",
      "Loss= tensor(2.3153, grad_fn=<DivBackward0>)\n",
      "i= 409\n",
      "w.grad= tensor([[ -1.9831,  -4.1966,  11.0496],\n",
      "        [ -0.3182,   4.0692, -12.4531]])\n",
      "b.grad= tensor([-70.2944, 354.1192])\n",
      "new w tensor([[-0.4381,  0.8022,  0.7944],\n",
      "        [-0.2116,  0.8434,  0.8415]], requires_grad=True)\n",
      "new b tensor([ 1.3762, -8.2496], requires_grad=True)\n",
      "Loss= tensor(2.3021, grad_fn=<DivBackward0>)\n",
      "i= 410\n",
      "w.grad= tensor([[ -1.9718,  -4.1769,  11.0002],\n",
      "        [ -0.3281,   4.0510, -12.4051]])\n",
      "b.grad= tensor([-70.2124, 353.9856])\n",
      "new w tensor([[-0.4380,  0.8024,  0.7938],\n",
      "        [-0.2115,  0.8432,  0.8422]], requires_grad=True)\n",
      "new b tensor([ 1.3798, -8.2673], requires_grad=True)\n",
      "Loss= tensor(2.2890, grad_fn=<DivBackward0>)\n",
      "i= 411\n",
      "w.grad= tensor([[ -1.9607,  -4.1576,  10.9508],\n",
      "        [ -0.3383,   4.0327, -12.3577]])\n",
      "b.grad= tensor([-70.1306, 353.8520])\n",
      "new w tensor([[-0.4379,  0.8026,  0.7933],\n",
      "        [-0.2115,  0.8430,  0.8428]], requires_grad=True)\n",
      "new b tensor([ 1.3833, -8.2850], requires_grad=True)\n",
      "Loss= tensor(2.2761, grad_fn=<DivBackward0>)\n",
      "i= 412\n",
      "w.grad= tensor([[ -1.9493,  -4.1378,  10.9019],\n",
      "        [ -0.3487,   4.0142, -12.3106]])\n",
      "b.grad= tensor([-70.0490, 353.7186])\n",
      "new w tensor([[-0.4378,  0.8028,  0.7927],\n",
      "        [-0.2115,  0.8428,  0.8434]], requires_grad=True)\n",
      "new b tensor([ 1.3868, -8.3026], requires_grad=True)\n",
      "Loss= tensor(2.2632, grad_fn=<DivBackward0>)\n",
      "i= 413\n",
      "w.grad= tensor([[ -1.9389,  -4.1193,  10.8526],\n",
      "        [ -0.3582,   3.9966, -12.2633]])\n",
      "b.grad= tensor([-69.9676, 353.5852])\n",
      "new w tensor([[-0.4377,  0.8030,  0.7922],\n",
      "        [-0.2115,  0.8426,  0.8440]], requires_grad=True)\n",
      "new b tensor([ 1.3903, -8.3203], requires_grad=True)\n",
      "Loss= tensor(2.2506, grad_fn=<DivBackward0>)\n",
      "i= 414\n",
      "w.grad= tensor([[ -1.9278,  -4.0999,  10.8040],\n",
      "        [ -0.3687,   3.9779, -12.2167]])\n",
      "b.grad= tensor([-69.8864, 353.4519])\n",
      "new w tensor([[-0.4376,  0.8032,  0.7916],\n",
      "        [-0.2115,  0.8424,  0.8446]], requires_grad=True)\n",
      "new b tensor([ 1.3938, -8.3380], requires_grad=True)\n",
      "Loss= tensor(2.2380, grad_fn=<DivBackward0>)\n",
      "i= 415\n",
      "w.grad= tensor([[ -1.9175,  -4.0815,  10.7551],\n",
      "        [ -0.3789,   3.9596, -12.1703]])\n",
      "b.grad= tensor([-69.8055, 353.3188])\n",
      "new w tensor([[-0.4375,  0.8034,  0.7911],\n",
      "        [-0.2115,  0.8422,  0.8452]], requires_grad=True)\n",
      "new b tensor([ 1.3972, -8.3557], requires_grad=True)\n",
      "Loss= tensor(2.2255, grad_fn=<DivBackward0>)\n",
      "i= 416\n",
      "w.grad= tensor([[ -1.9063,  -4.0622,  10.7072],\n",
      "        [ -0.3886,   3.9418, -12.1238]])\n",
      "b.grad= tensor([-69.7247, 353.1856])\n",
      "new w tensor([[-0.4374,  0.8036,  0.7906],\n",
      "        [-0.2114,  0.8420,  0.8458]], requires_grad=True)\n",
      "new b tensor([ 1.4007, -8.3733], requires_grad=True)\n",
      "Loss= tensor(2.2132, grad_fn=<DivBackward0>)\n",
      "i= 417\n",
      "w.grad= tensor([[ -1.8953,  -4.0431,  10.6593],\n",
      "        [ -0.3985,   3.9240, -12.0776]])\n",
      "b.grad= tensor([-69.6442, 353.0526])\n",
      "new w tensor([[-0.4373,  0.8038,  0.7900],\n",
      "        [-0.2114,  0.8418,  0.8464]], requires_grad=True)\n",
      "new b tensor([ 1.4042, -8.3910], requires_grad=True)\n",
      "Loss= tensor(2.2010, grad_fn=<DivBackward0>)\n",
      "i= 418\n",
      "w.grad= tensor([[ -1.8851,  -4.0249,  10.6112],\n",
      "        [ -0.4085,   3.9060, -12.0318]])\n",
      "b.grad= tensor([-69.5638, 352.9197])\n",
      "new w tensor([[-0.4372,  0.8040,  0.7895],\n",
      "        [-0.2114,  0.8416,  0.8470]], requires_grad=True)\n",
      "new b tensor([ 1.4077, -8.4086], requires_grad=True)\n",
      "Loss= tensor(2.1889, grad_fn=<DivBackward0>)\n",
      "i= 419\n",
      "w.grad= tensor([[ -1.8743,  -4.0060,  10.5637],\n",
      "        [ -0.4173,   3.8894, -11.9854]])\n",
      "b.grad= tensor([-69.4837, 352.7868])\n",
      "new w tensor([[-0.4371,  0.8042,  0.7890],\n",
      "        [-0.2114,  0.8414,  0.8476]], requires_grad=True)\n",
      "new b tensor([ 1.4112, -8.4263], requires_grad=True)\n",
      "Loss= tensor(2.1769, grad_fn=<DivBackward0>)\n",
      "i= 420\n",
      "w.grad= tensor([[ -1.8643,  -3.9883,  10.5158],\n",
      "        [ -0.4280,   3.8707, -11.9406]])\n",
      "b.grad= tensor([-69.4038, 352.6541])\n",
      "new w tensor([[-0.4370,  0.8044,  0.7884],\n",
      "        [-0.2114,  0.8412,  0.8482]], requires_grad=True)\n",
      "new b tensor([ 1.4146, -8.4439], requires_grad=True)\n",
      "Loss= tensor(2.1651, grad_fn=<DivBackward0>)\n",
      "i= 421\n",
      "w.grad= tensor([[ -1.8529,  -3.9688,  10.4693],\n",
      "        [ -0.4377,   3.8531, -11.8952]])\n",
      "b.grad= tensor([-69.3240, 352.5214])\n",
      "new w tensor([[-0.4369,  0.8046,  0.7879],\n",
      "        [-0.2113,  0.8410,  0.8488]], requires_grad=True)\n",
      "new b tensor([ 1.4181, -8.4615], requires_grad=True)\n",
      "Loss= tensor(2.1533, grad_fn=<DivBackward0>)\n",
      "i= 422\n",
      "w.grad= tensor([[ -1.8425,  -3.9505,  10.4223],\n",
      "        [ -0.4465,   3.8366, -11.8495]])\n",
      "b.grad= tensor([-69.2445, 352.3888])\n",
      "new w tensor([[-0.4369,  0.8048,  0.7874],\n",
      "        [-0.2113,  0.8408,  0.8494]], requires_grad=True)\n",
      "new b tensor([ 1.4216, -8.4791], requires_grad=True)\n",
      "Loss= tensor(2.1417, grad_fn=<DivBackward0>)\n",
      "i= 423\n",
      "w.grad= tensor([[ -1.8329,  -3.9331,  10.3750],\n",
      "        [ -0.4565,   3.8188, -11.8049]])\n",
      "b.grad= tensor([-69.1652, 352.2562])\n",
      "new w tensor([[-0.4368,  0.8050,  0.7869],\n",
      "        [-0.2113,  0.8406,  0.8500]], requires_grad=True)\n",
      "new b tensor([ 1.4250, -8.4967], requires_grad=True)\n",
      "Loss= tensor(2.1302, grad_fn=<DivBackward0>)\n",
      "i= 424\n",
      "w.grad= tensor([[ -1.8219,  -3.9142,  10.3290],\n",
      "        [ -0.4656,   3.8019, -11.7599]])\n",
      "b.grad= tensor([-69.0860, 352.1238])\n",
      "new w tensor([[-0.4367,  0.8052,  0.7864],\n",
      "        [-0.2113,  0.8404,  0.8506]], requires_grad=True)\n",
      "new b tensor([ 1.4285, -8.5144], requires_grad=True)\n",
      "Loss= tensor(2.1188, grad_fn=<DivBackward0>)\n",
      "i= 425\n",
      "w.grad= tensor([[ -1.8122,  -3.8969,  10.2821],\n",
      "        [ -0.4751,   3.7846, -11.7154]])\n",
      "b.grad= tensor([-69.0071, 351.9914])\n",
      "new w tensor([[-0.4366,  0.8054,  0.7858],\n",
      "        [-0.2112,  0.8402,  0.8512]], requires_grad=True)\n",
      "new b tensor([ 1.4319, -8.5320], requires_grad=True)\n",
      "Loss= tensor(2.1075, grad_fn=<DivBackward0>)\n",
      "i= 426\n",
      "w.grad= tensor([[ -1.8010,  -3.8778,  10.2367],\n",
      "        [ -0.4843,   3.7678, -11.6710]])\n",
      "b.grad= tensor([-68.9284, 351.8591])\n",
      "new w tensor([[-0.4365,  0.8056,  0.7853],\n",
      "        [-0.2112,  0.8400,  0.8518]], requires_grad=True)\n",
      "new b tensor([ 1.4354, -8.5495], requires_grad=True)\n",
      "Loss= tensor(2.0963, grad_fn=<DivBackward0>)\n",
      "i= 427\n",
      "w.grad= tensor([[ -1.7913,  -3.8605,  10.1904],\n",
      "        [ -0.4944,   3.7498, -11.6274]])\n",
      "b.grad= tensor([-68.8498, 351.7269])\n",
      "new w tensor([[-0.4364,  0.8058,  0.7848],\n",
      "        [-0.2112,  0.8399,  0.8523]], requires_grad=True)\n",
      "new b tensor([ 1.4388, -8.5671], requires_grad=True)\n",
      "Loss= tensor(2.0853, grad_fn=<DivBackward0>)\n",
      "i= 428\n",
      "w.grad= tensor([[ -1.7810,  -3.8425,  10.1449],\n",
      "        [ -0.5039,   3.7326, -11.5836]])\n",
      "b.grad= tensor([-68.7715, 351.5947])\n",
      "new w tensor([[-0.4363,  0.8060,  0.7843],\n",
      "        [-0.2112,  0.8397,  0.8529]], requires_grad=True)\n",
      "new b tensor([ 1.4423, -8.5847], requires_grad=True)\n",
      "Loss= tensor(2.0743, grad_fn=<DivBackward0>)\n",
      "i= 429\n",
      "w.grad= tensor([[ -1.7712,  -3.8252,  10.0992],\n",
      "        [ -0.5127,   3.7163, -11.5396]])\n",
      "b.grad= tensor([-68.6933, 351.4626])\n",
      "new w tensor([[-0.4362,  0.8062,  0.7838],\n",
      "        [-0.2111,  0.8395,  0.8535]], requires_grad=True)\n",
      "new b tensor([ 1.4457, -8.6023], requires_grad=True)\n",
      "Loss= tensor(2.0635, grad_fn=<DivBackward0>)\n",
      "i= 430\n",
      "w.grad= tensor([[ -1.7604,  -3.8067,  10.0544],\n",
      "        [ -0.5216,   3.7000, -11.4959]])\n",
      "b.grad= tensor([-68.6153, 351.3306])\n",
      "new w tensor([[-0.4361,  0.8064,  0.7833],\n",
      "        [-0.2111,  0.8393,  0.8541]], requires_grad=True)\n",
      "new b tensor([ 1.4491, -8.6199], requires_grad=True)\n",
      "Loss= tensor(2.0528, grad_fn=<DivBackward0>)\n",
      "i= 431\n",
      "w.grad= tensor([[ -1.7510,  -3.7897,  10.0090],\n",
      "        [ -0.5311,   3.6828, -11.4528]])\n",
      "b.grad= tensor([-68.5376, 351.1987])\n",
      "new w tensor([[-0.4361,  0.8066,  0.7828],\n",
      "        [-0.2111,  0.8391,  0.8547]], requires_grad=True)\n",
      "new b tensor([ 1.4525, -8.6374], requires_grad=True)\n",
      "Loss= tensor(2.0421, grad_fn=<DivBackward0>)\n",
      "i= 432\n",
      "w.grad= tensor([[ -1.7408,  -3.7720,   9.9643],\n",
      "        [ -0.5403,   3.6660, -11.4098]])\n",
      "b.grad= tensor([-68.4600, 351.0668])\n",
      "new w tensor([[-0.4360,  0.8068,  0.7823],\n",
      "        [-0.2111,  0.8389,  0.8552]], requires_grad=True)\n",
      "new b tensor([ 1.4560, -8.6550], requires_grad=True)\n",
      "Loss= tensor(2.0316, grad_fn=<DivBackward0>)\n",
      "i= 433\n",
      "w.grad= tensor([[ -1.7307,  -3.7544,   9.9198],\n",
      "        [ -0.5491,   3.6497, -11.3667]])\n",
      "b.grad= tensor([-68.3826, 350.9350])\n",
      "new w tensor([[-0.4359,  0.8070,  0.7818],\n",
      "        [-0.2110,  0.8388,  0.8558]], requires_grad=True)\n",
      "new b tensor([ 1.4594, -8.6725], requires_grad=True)\n",
      "Loss= tensor(2.0212, grad_fn=<DivBackward0>)\n",
      "i= 434\n",
      "w.grad= tensor([[ -1.7210,  -3.7373,   9.8752],\n",
      "        [ -0.5585,   3.6329, -11.3242]])\n",
      "b.grad= tensor([-68.3054, 350.8033])\n",
      "new w tensor([[-0.4358,  0.8071,  0.7813],\n",
      "        [-0.2110,  0.8386,  0.8564]], requires_grad=True)\n",
      "new b tensor([ 1.4628, -8.6901], requires_grad=True)\n",
      "Loss= tensor(2.0109, grad_fn=<DivBackward0>)\n",
      "i= 435\n",
      "w.grad= tensor([[ -1.7111,  -3.7199,   9.8311],\n",
      "        [ -0.5673,   3.6167, -11.2816]])\n",
      "b.grad= tensor([-68.2284, 350.6716])\n",
      "new w tensor([[-0.4357,  0.8073,  0.7808],\n",
      "        [-0.2110,  0.8384,  0.8569]], requires_grad=True)\n",
      "new b tensor([ 1.4662, -8.7076], requires_grad=True)\n",
      "Loss= tensor(2.0007, grad_fn=<DivBackward0>)\n",
      "i= 436\n",
      "w.grad= tensor([[ -1.7017,  -3.7032,   9.7868],\n",
      "        [ -0.5760,   3.6006, -11.2391]])\n",
      "b.grad= tensor([-68.1515, 350.5400])\n",
      "new w tensor([[-0.4356,  0.8075,  0.7803],\n",
      "        [-0.2110,  0.8382,  0.8575]], requires_grad=True)\n",
      "new b tensor([ 1.4696, -8.7251], requires_grad=True)\n",
      "Loss= tensor(1.9906, grad_fn=<DivBackward0>)\n",
      "i= 437\n",
      "w.grad= tensor([[ -1.6917,  -3.6858,   9.7431],\n",
      "        [ -0.5850,   3.5840, -11.1971]])\n",
      "b.grad= tensor([-68.0749, 350.4085])\n",
      "new w tensor([[-0.4355,  0.8077,  0.7798],\n",
      "        [-0.2109,  0.8380,  0.8580]], requires_grad=True)\n",
      "new b tensor([ 1.4730, -8.7426], requires_grad=True)\n",
      "Loss= tensor(1.9805, grad_fn=<DivBackward0>)\n",
      "i= 438\n",
      "w.grad= tensor([[ -1.6818,  -3.6685,   9.6997],\n",
      "        [ -0.5939,   3.5679, -11.1551]])\n",
      "b.grad= tensor([-67.9984, 350.2770])\n",
      "new w tensor([[-0.4355,  0.8079,  0.7794],\n",
      "        [-0.2109,  0.8379,  0.8586]], requires_grad=True)\n",
      "new b tensor([ 1.4764, -8.7601], requires_grad=True)\n",
      "Loss= tensor(1.9706, grad_fn=<DivBackward0>)\n",
      "i= 439\n",
      "w.grad= tensor([[ -1.6728,  -3.6523,   9.6559],\n",
      "        [ -0.6027,   3.5517, -11.1134]])\n",
      "b.grad= tensor([-67.9221, 350.1456])\n",
      "new w tensor([[-0.4354,  0.8081,  0.7789],\n",
      "        [-0.2109,  0.8377,  0.8592]], requires_grad=True)\n",
      "new b tensor([ 1.4798, -8.7777], requires_grad=True)\n",
      "Loss= tensor(1.9608, grad_fn=<DivBackward0>)\n",
      "i= 440\n",
      "w.grad= tensor([[ -1.6622,  -3.6343,   9.6133],\n",
      "        [ -0.6109,   3.5364, -11.0715]])\n",
      "b.grad= tensor([-67.8461, 350.0143])\n",
      "new w tensor([[-0.4353,  0.8082,  0.7784],\n",
      "        [-0.2108,  0.8375,  0.8597]], requires_grad=True)\n",
      "new b tensor([ 1.4832, -8.7952], requires_grad=True)\n",
      "Loss= tensor(1.9511, grad_fn=<DivBackward0>)\n",
      "i= 441\n",
      "w.grad= tensor([[ -1.6531,  -3.6180,   9.5700],\n",
      "        [ -0.6207,   3.5191, -11.0309]])\n",
      "b.grad= tensor([-67.7701, 349.8830])\n",
      "new w tensor([[-0.4352,  0.8084,  0.7779],\n",
      "        [-0.2108,  0.8373,  0.8603]], requires_grad=True)\n",
      "new b tensor([ 1.4866, -8.8126], requires_grad=True)\n",
      "Loss= tensor(1.9415, grad_fn=<DivBackward0>)\n",
      "i= 442\n",
      "w.grad= tensor([[ -1.6436,  -3.6014,   9.5272],\n",
      "        [ -0.6293,   3.5032, -10.9897]])\n",
      "b.grad= tensor([-67.6944, 349.7517])\n",
      "new w tensor([[-0.4351,  0.8086,  0.7774],\n",
      "        [-0.2108,  0.8371,  0.8608]], requires_grad=True)\n",
      "new b tensor([ 1.4900, -8.8301], requires_grad=True)\n",
      "Loss= tensor(1.9320, grad_fn=<DivBackward0>)\n",
      "i= 443\n",
      "w.grad= tensor([[ -1.6343,  -3.5848,   9.4845],\n",
      "        [ -0.6374,   3.4880, -10.9484]])\n",
      "b.grad= tensor([-67.6189, 349.6206])\n",
      "new w tensor([[-0.4350,  0.8088,  0.7770],\n",
      "        [-0.2107,  0.8370,  0.8614]], requires_grad=True)\n",
      "new b tensor([ 1.4934, -8.8476], requires_grad=True)\n",
      "Loss= tensor(1.9226, grad_fn=<DivBackward0>)\n",
      "i= 444\n",
      "w.grad= tensor([[ -1.6245,  -3.5679,   9.4423],\n",
      "        [ -0.6456,   3.4727, -10.9073]])\n",
      "b.grad= tensor([-67.5435, 349.4895])\n",
      "new w tensor([[-0.4350,  0.8090,  0.7765],\n",
      "        [-0.2107,  0.8368,  0.8619]], requires_grad=True)\n",
      "new b tensor([ 1.4967, -8.8651], requires_grad=True)\n",
      "Loss= tensor(1.9133, grad_fn=<DivBackward0>)\n",
      "i= 445\n",
      "w.grad= tensor([[ -1.6150,  -3.5512,   9.4002],\n",
      "        [ -0.6545,   3.4566, -10.8669]])\n",
      "b.grad= tensor([-67.4683, 349.3584])\n",
      "new w tensor([[-0.4349,  0.8091,  0.7760],\n",
      "        [-0.2107,  0.8366,  0.8624]], requires_grad=True)\n",
      "new b tensor([ 1.5001, -8.8826], requires_grad=True)\n",
      "Loss= tensor(1.9040, grad_fn=<DivBackward0>)\n",
      "i= 446\n",
      "w.grad= tensor([[ -1.6057,  -3.5349,   9.3582],\n",
      "        [ -0.6630,   3.4410, -10.8265]])\n",
      "b.grad= tensor([-67.3933, 349.2274])\n",
      "new w tensor([[-0.4348,  0.8093,  0.7756],\n",
      "        [-0.2106,  0.8365,  0.8630]], requires_grad=True)\n",
      "new b tensor([ 1.5035, -8.9000], requires_grad=True)\n",
      "Loss= tensor(1.8949, grad_fn=<DivBackward0>)\n",
      "i= 447\n",
      "w.grad= tensor([[ -1.5969,  -3.5191,   9.3160],\n",
      "        [ -0.6720,   3.4246, -10.7868]])\n",
      "b.grad= tensor([-67.3184, 349.0965])\n",
      "new w tensor([[-0.4347,  0.8095,  0.7751],\n",
      "        [-0.2106,  0.8363,  0.8635]], requires_grad=True)\n",
      "new b tensor([ 1.5069, -8.9175], requires_grad=True)\n",
      "Loss= tensor(1.8858, grad_fn=<DivBackward0>)\n",
      "i= 448\n",
      "w.grad= tensor([[ -1.5874,  -3.5026,   9.2746],\n",
      "        [ -0.6794,   3.4103, -10.7460]])\n",
      "b.grad= tensor([-67.2438, 348.9657])\n",
      "new w tensor([[-0.4346,  0.8097,  0.7746],\n",
      "        [-0.2106,  0.8361,  0.8641]], requires_grad=True)\n",
      "new b tensor([ 1.5102, -8.9349], requires_grad=True)\n",
      "Loss= tensor(1.8769, grad_fn=<DivBackward0>)\n",
      "i= 449\n",
      "w.grad= tensor([[ -1.5788,  -3.4871,   9.2327],\n",
      "        [ -0.6883,   3.3943, -10.7066]])\n",
      "b.grad= tensor([-67.1693, 348.8348])\n",
      "new w tensor([[-0.4346,  0.8098,  0.7742],\n",
      "        [-0.2105,  0.8359,  0.8646]], requires_grad=True)\n",
      "new b tensor([ 1.5136, -8.9524], requires_grad=True)\n",
      "Loss= tensor(1.8680, grad_fn=<DivBackward0>)\n",
      "i= 450\n",
      "w.grad= tensor([[ -1.5687,  -3.4700,   9.1921],\n",
      "        [ -0.6964,   3.3793, -10.6667]])\n",
      "b.grad= tensor([-67.0950, 348.7041])\n",
      "new w tensor([[-0.4345,  0.8100,  0.7737],\n",
      "        [-0.2105,  0.8358,  0.8651]], requires_grad=True)\n",
      "new b tensor([ 1.5169, -8.9698], requires_grad=True)\n",
      "Loss= tensor(1.8593, grad_fn=<DivBackward0>)\n",
      "i= 451\n",
      "w.grad= tensor([[ -1.5600,  -3.4545,   9.1507],\n",
      "        [ -0.7048,   3.3637, -10.6273]])\n",
      "b.grad= tensor([-67.0208, 348.5734])\n",
      "new w tensor([[-0.4344,  0.8102,  0.7733],\n",
      "        [-0.2105,  0.8356,  0.8657]], requires_grad=True)\n",
      "new b tensor([ 1.5203, -8.9872], requires_grad=True)\n",
      "Loss= tensor(1.8506, grad_fn=<DivBackward0>)\n",
      "i= 452\n",
      "w.grad= tensor([[ -1.5506,  -3.4380,   9.1102],\n",
      "        [ -0.7126,   3.3490, -10.5877]])\n",
      "b.grad= tensor([-66.9469, 348.4427])\n",
      "new w tensor([[-0.4343,  0.8104,  0.7728],\n",
      "        [-0.2104,  0.8354,  0.8662]], requires_grad=True)\n",
      "new b tensor([ 1.5236, -9.0047], requires_grad=True)\n",
      "Loss= tensor(1.8420, grad_fn=<DivBackward0>)\n",
      "i= 453\n",
      "w.grad= tensor([[ -1.5422,  -3.4229,   9.0691],\n",
      "        [ -0.7210,   3.3337, -10.5487]])\n",
      "b.grad= tensor([-66.8731, 348.3121])\n",
      "new w tensor([[-0.4342,  0.8105,  0.7723],\n",
      "        [-0.2104,  0.8353,  0.8667]], requires_grad=True)\n",
      "new b tensor([ 1.5270, -9.0221], requires_grad=True)\n",
      "Loss= tensor(1.8335, grad_fn=<DivBackward0>)\n",
      "i= 454\n",
      "w.grad= tensor([[ -1.5325,  -3.4062,   9.0291],\n",
      "        [ -0.7289,   3.3190, -10.5095]])\n",
      "b.grad= tensor([-66.7994, 348.1816])\n",
      "new w tensor([[-0.4342,  0.8107,  0.7719],\n",
      "        [-0.2104,  0.8351,  0.8672]], requires_grad=True)\n",
      "new b tensor([ 1.5303, -9.0395], requires_grad=True)\n",
      "Loss= tensor(1.8251, grad_fn=<DivBackward0>)\n",
      "i= 455\n",
      "w.grad= tensor([[ -1.5245,  -3.3915,   8.9882],\n",
      "        [ -0.7376,   3.3033, -10.4711]])\n",
      "b.grad= tensor([-66.7260, 348.0511])\n",
      "new w tensor([[-0.4341,  0.8109,  0.7714],\n",
      "        [-0.2103,  0.8349,  0.8678]], requires_grad=True)\n",
      "new b tensor([ 1.5337, -9.0569], requires_grad=True)\n",
      "Loss= tensor(1.8167, grad_fn=<DivBackward0>)\n",
      "i= 456\n",
      "w.grad= tensor([[ -1.5149,  -3.3751,   8.9485],\n",
      "        [ -0.7450,   3.2890, -10.4322]])\n",
      "b.grad= tensor([-66.6527, 347.9207])\n",
      "new w tensor([[-0.4340,  0.8110,  0.7710],\n",
      "        [-0.2103,  0.8348,  0.8683]], requires_grad=True)\n",
      "new b tensor([ 1.5370, -9.0743], requires_grad=True)\n",
      "Loss= tensor(1.8085, grad_fn=<DivBackward0>)\n",
      "i= 457\n",
      "w.grad= tensor([[ -1.5061,  -3.3596,   8.9085],\n",
      "        [ -0.7537,   3.2735, -10.3942]])\n",
      "b.grad= tensor([-66.5796, 347.7903])\n",
      "new w tensor([[-0.4339,  0.8112,  0.7706],\n",
      "        [-0.2102,  0.8346,  0.8688]], requires_grad=True)\n",
      "new b tensor([ 1.5403, -9.0917], requires_grad=True)\n",
      "Loss= tensor(1.8003, grad_fn=<DivBackward0>)\n",
      "i= 458\n",
      "w.grad= tensor([[ -1.4976,  -3.3445,   8.8686],\n",
      "        [ -0.7609,   3.2597, -10.3554]])\n",
      "b.grad= tensor([-66.5066, 347.6599])\n",
      "new w tensor([[-0.4339,  0.8114,  0.7701],\n",
      "        [-0.2102,  0.8344,  0.8693]], requires_grad=True)\n",
      "new b tensor([ 1.5436, -9.1090], requires_grad=True)\n",
      "Loss= tensor(1.7922, grad_fn=<DivBackward0>)\n",
      "i= 459\n",
      "w.grad= tensor([[ -1.4880,  -3.3282,   8.8295],\n",
      "        [ -0.7692,   3.2444, -10.3176]])\n",
      "b.grad= tensor([-66.4338, 347.5296])\n",
      "new w tensor([[-0.4338,  0.8115,  0.7697],\n",
      "        [-0.2102,  0.8343,  0.8698]], requires_grad=True)\n",
      "new b tensor([ 1.5470, -9.1264], requires_grad=True)\n",
      "Loss= tensor(1.7842, grad_fn=<DivBackward0>)\n",
      "i= 460\n",
      "w.grad= tensor([[ -1.4800,  -3.3136,   8.7897],\n",
      "        [ -0.7772,   3.2299, -10.2797]])\n",
      "b.grad= tensor([-66.3612, 347.3994])\n",
      "new w tensor([[-0.4337,  0.8117,  0.7692],\n",
      "        [-0.2101,  0.8341,  0.8704]], requires_grad=True)\n",
      "new b tensor([ 1.5503, -9.1438], requires_grad=True)\n",
      "Loss= tensor(1.7763, grad_fn=<DivBackward0>)\n",
      "i= 461\n",
      "w.grad= tensor([[ -1.4706,  -3.2974,   8.7510],\n",
      "        [ -0.7849,   3.2154, -10.2419]])\n",
      "b.grad= tensor([-66.2887, 347.2691])\n",
      "new w tensor([[-0.4336,  0.8119,  0.7688],\n",
      "        [-0.2101,  0.8340,  0.8709]], requires_grad=True)\n",
      "new b tensor([ 1.5536, -9.1612], requires_grad=True)\n",
      "Loss= tensor(1.7685, grad_fn=<DivBackward0>)\n",
      "i= 462\n",
      "w.grad= tensor([[ -1.4629,  -3.2832,   8.7114],\n",
      "        [ -0.7922,   3.2014, -10.2040]])\n",
      "b.grad= tensor([-66.2165, 347.1390])\n",
      "new w tensor([[-0.4336,  0.8120,  0.7684],\n",
      "        [-0.2101,  0.8338,  0.8714]], requires_grad=True)\n",
      "new b tensor([ 1.5569, -9.1785], requires_grad=True)\n",
      "Loss= tensor(1.7607, grad_fn=<DivBackward0>)\n",
      "i= 463\n",
      "w.grad= tensor([[ -1.4542,  -3.2681,   8.6726],\n",
      "        [ -0.7999,   3.1871, -10.1666]])\n",
      "b.grad= tensor([-66.1443, 347.0089])\n",
      "new w tensor([[-0.4335,  0.8122,  0.7679],\n",
      "        [-0.2100,  0.8336,  0.8719]], requires_grad=True)\n",
      "new b tensor([ 1.5602, -9.1959], requires_grad=True)\n",
      "Loss= tensor(1.7531, grad_fn=<DivBackward0>)\n",
      "i= 464\n",
      "w.grad= tensor([[ -1.4453,  -3.2526,   8.6341],\n",
      "        [ -0.8088,   3.1715, -10.1301]])\n",
      "b.grad= tensor([-66.0723, 346.8788])\n",
      "new w tensor([[-0.4334,  0.8124,  0.7675],\n",
      "        [-0.2100,  0.8335,  0.8724]], requires_grad=True)\n",
      "new b tensor([ 1.5635, -9.2132], requires_grad=True)\n",
      "Loss= tensor(1.7455, grad_fn=<DivBackward0>)\n",
      "i= 465\n",
      "w.grad= tensor([[ -1.4369,  -3.2377,   8.5956],\n",
      "        [ -0.8156,   3.1581, -10.0925]])\n",
      "b.grad= tensor([-66.0005, 346.7487])\n",
      "new w tensor([[-0.4334,  0.8125,  0.7671],\n",
      "        [-0.2099,  0.8333,  0.8729]], requires_grad=True)\n",
      "new b tensor([ 1.5668, -9.2305], requires_grad=True)\n",
      "Loss= tensor(1.7380, grad_fn=<DivBackward0>)\n",
      "i= 466\n",
      "w.grad= tensor([[ -1.4282,  -3.2226,   8.5574],\n",
      "        [ -0.8234,   3.1437, -10.0557]])\n",
      "b.grad= tensor([-65.9289, 346.6188])\n",
      "new w tensor([[-0.4333,  0.8127,  0.7666],\n",
      "        [-0.2099,  0.8332,  0.8734]], requires_grad=True)\n",
      "new b tensor([ 1.5701, -9.2479], requires_grad=True)\n",
      "Loss= tensor(1.7305, grad_fn=<DivBackward0>)\n",
      "i= 467\n",
      "w.grad= tensor([[ -1.4195,  -3.2074,   8.5195],\n",
      "        [ -0.8312,   3.1292, -10.0192]])\n",
      "b.grad= tensor([-65.8574, 346.4888])\n",
      "new w tensor([[-0.4332,  0.8128,  0.7662],\n",
      "        [-0.2098,  0.8330,  0.8739]], requires_grad=True)\n",
      "new b tensor([ 1.5734, -9.2652], requires_grad=True)\n",
      "Loss= tensor(1.7232, grad_fn=<DivBackward0>)\n",
      "i= 468\n",
      "w.grad= tensor([[-1.4114, -3.1929,  8.4813],\n",
      "        [-0.8386,  3.1154, -9.9825]])\n",
      "b.grad= tensor([-65.7861, 346.3589])\n",
      "new w tensor([[-0.4331,  0.8130,  0.7658],\n",
      "        [-0.2098,  0.8329,  0.8744]], requires_grad=True)\n",
      "new b tensor([ 1.5767, -9.2825], requires_grad=True)\n",
      "Loss= tensor(1.7159, grad_fn=<DivBackward0>)\n",
      "i= 469\n",
      "w.grad= tensor([[-1.4036, -3.1789,  8.4432],\n",
      "        [-0.8458,  3.1016, -9.9459]])\n",
      "b.grad= tensor([-65.7149, 346.2291])\n",
      "new w tensor([[-0.4331,  0.8132,  0.7654],\n",
      "        [-0.2098,  0.8327,  0.8749]], requires_grad=True)\n",
      "new b tensor([ 1.5800, -9.2998], requires_grad=True)\n",
      "Loss= tensor(1.7087, grad_fn=<DivBackward0>)\n",
      "i= 470\n",
      "w.grad= tensor([[-1.3945, -3.1633,  8.4061],\n",
      "        [-0.8542,  3.0866, -9.9104]])\n",
      "b.grad= tensor([-65.6439, 346.0992])\n",
      "new w tensor([[-0.4330,  0.8133,  0.7649],\n",
      "        [-0.2097,  0.8326,  0.8754]], requires_grad=True)\n",
      "new b tensor([ 1.5833, -9.3171], requires_grad=True)\n",
      "Loss= tensor(1.7016, grad_fn=<DivBackward0>)\n",
      "i= 471\n",
      "w.grad= tensor([[-1.3865, -3.1489,  8.3685],\n",
      "        [-0.8609,  3.0736, -9.8738]])\n",
      "b.grad= tensor([-65.5730, 345.9695])\n",
      "new w tensor([[-0.4329,  0.8135,  0.7645],\n",
      "        [-0.2097,  0.8324,  0.8759]], requires_grad=True)\n",
      "new b tensor([ 1.5865, -9.3344], requires_grad=True)\n",
      "Loss= tensor(1.6945, grad_fn=<DivBackward0>)\n",
      "i= 472\n",
      "w.grad= tensor([[-1.3778, -3.1338,  8.3316],\n",
      "        [-0.8684,  3.0596, -9.8380]])\n",
      "b.grad= tensor([-65.5023, 345.8397])\n",
      "new w tensor([[-0.4329,  0.8136,  0.7641],\n",
      "        [-0.2096,  0.8322,  0.8764]], requires_grad=True)\n",
      "new b tensor([ 1.5898, -9.3517], requires_grad=True)\n",
      "Loss= tensor(1.6875, grad_fn=<DivBackward0>)\n",
      "i= 473\n",
      "w.grad= tensor([[-1.3700, -3.1198,  8.2943],\n",
      "        [-0.8757,  3.0459, -9.8022]])\n",
      "b.grad= tensor([-65.4317, 345.7100])\n",
      "new w tensor([[-0.4328,  0.8138,  0.7637],\n",
      "        [-0.2096,  0.8321,  0.8769]], requires_grad=True)\n",
      "new b tensor([ 1.5931, -9.3690], requires_grad=True)\n",
      "Loss= tensor(1.6806, grad_fn=<DivBackward0>)\n",
      "i= 474\n",
      "w.grad= tensor([[-1.3622, -3.1058,  8.2571],\n",
      "        [-0.8825,  3.0328, -9.7663]])\n",
      "b.grad= tensor([-65.3613, 345.5803])\n",
      "new w tensor([[-0.4327,  0.8139,  0.7633],\n",
      "        [-0.2095,  0.8319,  0.8774]], requires_grad=True)\n",
      "new b tensor([ 1.5964, -9.3863], requires_grad=True)\n",
      "Loss= tensor(1.6738, grad_fn=<DivBackward0>)\n",
      "i= 475\n",
      "w.grad= tensor([[-1.3542, -3.0917,  8.2202],\n",
      "        [-0.8901,  3.0189, -9.7311]])\n",
      "b.grad= tensor([-65.2911, 345.4507])\n",
      "new w tensor([[-0.4327,  0.8141,  0.7629],\n",
      "        [-0.2095,  0.8318,  0.8778]], requires_grad=True)\n",
      "new b tensor([ 1.5996, -9.4036], requires_grad=True)\n",
      "Loss= tensor(1.6670, grad_fn=<DivBackward0>)\n",
      "i= 476\n",
      "w.grad= tensor([[-1.3460, -3.0772,  8.1838],\n",
      "        [-0.8981,  3.0044, -9.6964]])\n",
      "b.grad= tensor([-65.2210, 345.3211])\n",
      "new w tensor([[-0.4326,  0.8143,  0.7625],\n",
      "        [-0.2095,  0.8316,  0.8783]], requires_grad=True)\n",
      "new b tensor([ 1.6029, -9.4208], requires_grad=True)\n",
      "Loss= tensor(1.6603, grad_fn=<DivBackward0>)\n",
      "i= 477\n",
      "w.grad= tensor([[-1.3376, -3.0627,  8.1476],\n",
      "        [-0.9045,  2.9917, -9.6608]])\n",
      "b.grad= tensor([-65.1511, 345.1915])\n",
      "new w tensor([[-0.4325,  0.8144,  0.7620],\n",
      "        [-0.2094,  0.8315,  0.8788]], requires_grad=True)\n",
      "new b tensor([ 1.6061, -9.4381], requires_grad=True)\n",
      "Loss= tensor(1.6537, grad_fn=<DivBackward0>)\n",
      "i= 478\n",
      "w.grad= tensor([[-1.3299, -3.0489,  8.1112],\n",
      "        [-0.9126,  2.9774, -9.6264]])\n",
      "b.grad= tensor([-65.0813, 345.0620])\n",
      "new w tensor([[-0.4325,  0.8146,  0.7616],\n",
      "        [-0.2094,  0.8313,  0.8793]], requires_grad=True)\n",
      "new b tensor([ 1.6094, -9.4553], requires_grad=True)\n",
      "Loss= tensor(1.6471, grad_fn=<DivBackward0>)\n",
      "i= 479\n",
      "w.grad= tensor([[-1.3219, -3.0349,  8.0751],\n",
      "        [-0.9189,  2.9649, -9.5912]])\n",
      "b.grad= tensor([-65.0116, 344.9324])\n",
      "new w tensor([[-0.4324,  0.8147,  0.7612],\n",
      "        [-0.2093,  0.8312,  0.8798]], requires_grad=True)\n",
      "new b tensor([ 1.6126, -9.4726], requires_grad=True)\n",
      "Loss= tensor(1.6407, grad_fn=<DivBackward0>)\n",
      "i= 480\n",
      "w.grad= tensor([[-1.3134, -3.0202,  8.0397],\n",
      "        [-0.9263,  2.9511, -9.5567]])\n",
      "b.grad= tensor([-64.9421, 344.8029])\n",
      "new w tensor([[-0.4323,  0.8149,  0.7608],\n",
      "        [-0.2093,  0.8310,  0.8803]], requires_grad=True)\n",
      "new b tensor([ 1.6159, -9.4898], requires_grad=True)\n",
      "Loss= tensor(1.6342, grad_fn=<DivBackward0>)\n",
      "i= 481\n",
      "w.grad= tensor([[-1.3057, -3.0063,  8.0039],\n",
      "        [-0.9332,  2.9381, -9.5222]])\n",
      "b.grad= tensor([-64.8728, 344.6735])\n",
      "new w tensor([[-0.4323,  0.8150,  0.7604],\n",
      "        [-0.2092,  0.8309,  0.8807]], requires_grad=True)\n",
      "new b tensor([ 1.6191, -9.5071], requires_grad=True)\n",
      "Loss= tensor(1.6279, grad_fn=<DivBackward0>)\n",
      "i= 482\n",
      "w.grad= tensor([[-1.2983, -2.9931,  7.9679],\n",
      "        [-0.9407,  2.9245, -9.4883]])\n",
      "b.grad= tensor([-64.8036, 344.5441])\n",
      "new w tensor([[-0.4322,  0.8152,  0.7600],\n",
      "        [-0.2092,  0.8308,  0.8812]], requires_grad=True)\n",
      "new b tensor([ 1.6224, -9.5243], requires_grad=True)\n",
      "Loss= tensor(1.6216, grad_fn=<DivBackward0>)\n",
      "i= 483\n",
      "w.grad= tensor([[-1.2900, -2.9787,  7.9329],\n",
      "        [-0.9474,  2.9115, -9.4540]])\n",
      "b.grad= tensor([-64.7345, 344.4147])\n",
      "new w tensor([[-0.4321,  0.8153,  0.7596],\n",
      "        [-0.2091,  0.8306,  0.8817]], requires_grad=True)\n",
      "new b tensor([ 1.6256, -9.5415], requires_grad=True)\n",
      "Loss= tensor(1.6154, grad_fn=<DivBackward0>)\n",
      "i= 484\n",
      "w.grad= tensor([[-1.2825, -2.9652,  7.8975],\n",
      "        [-0.9539,  2.8989, -9.4197]])\n",
      "b.grad= tensor([-64.6656, 344.2853])\n",
      "new w tensor([[-0.4321,  0.8155,  0.7592],\n",
      "        [-0.2091,  0.8305,  0.8821]], requires_grad=True)\n",
      "new b tensor([ 1.6288, -9.5587], requires_grad=True)\n",
      "Loss= tensor(1.6093, grad_fn=<DivBackward0>)\n",
      "i= 485\n",
      "w.grad= tensor([[-1.2748, -2.9516,  7.8624],\n",
      "        [-0.9609,  2.8858, -9.3859]])\n",
      "b.grad= tensor([-64.5968, 344.1560])\n",
      "new w tensor([[-0.4320,  0.8156,  0.7589],\n",
      "        [-0.2090,  0.8303,  0.8826]], requires_grad=True)\n",
      "new b tensor([ 1.6321, -9.5759], requires_grad=True)\n",
      "Loss= tensor(1.6032, grad_fn=<DivBackward0>)\n",
      "i= 486\n",
      "w.grad= tensor([[-1.2671, -2.9381,  7.8274],\n",
      "        [-0.9682,  2.8723, -9.3526]])\n",
      "b.grad= tensor([-64.5282, 344.0267])\n",
      "new w tensor([[-0.4319,  0.8158,  0.7585],\n",
      "        [-0.2090,  0.8302,  0.8831]], requires_grad=True)\n",
      "new b tensor([ 1.6353, -9.5931], requires_grad=True)\n",
      "Loss= tensor(1.5972, grad_fn=<DivBackward0>)\n",
      "i= 487\n",
      "w.grad= tensor([[-1.2588, -2.9237,  7.7932],\n",
      "        [-0.9744,  2.8601, -9.3186]])\n",
      "b.grad= tensor([-64.4597, 343.8974])\n",
      "new w tensor([[-0.4319,  0.8159,  0.7581],\n",
      "        [-0.2089,  0.8300,  0.8836]], requires_grad=True)\n",
      "new b tensor([ 1.6385, -9.6103], requires_grad=True)\n",
      "Loss= tensor(1.5912, grad_fn=<DivBackward0>)\n",
      "i= 488\n",
      "w.grad= tensor([[-1.2515, -2.9106,  7.7584],\n",
      "        [-0.9816,  2.8469, -9.2855]])\n",
      "b.grad= tensor([-64.3914, 343.7682])\n",
      "new w tensor([[-0.4318,  0.8160,  0.7577],\n",
      "        [-0.2089,  0.8299,  0.8840]], requires_grad=True)\n",
      "new b tensor([ 1.6417, -9.6275], requires_grad=True)\n",
      "Loss= tensor(1.5853, grad_fn=<DivBackward0>)\n",
      "i= 489\n",
      "w.grad= tensor([[-1.2440, -2.8974,  7.7239],\n",
      "        [-0.9883,  2.8342, -9.2524]])\n",
      "b.grad= tensor([-64.3232, 343.6390])\n",
      "new w tensor([[-0.4318,  0.8162,  0.7573],\n",
      "        [-0.2088,  0.8297,  0.8845]], requires_grad=True)\n",
      "new b tensor([ 1.6450, -9.6447], requires_grad=True)\n",
      "Loss= tensor(1.5795, grad_fn=<DivBackward0>)\n",
      "i= 490\n",
      "w.grad= tensor([[-1.2365, -2.8841,  7.6896],\n",
      "        [-0.9950,  2.8213, -9.2193]])\n",
      "b.grad= tensor([-64.2552, 343.5098])\n",
      "new w tensor([[-0.4317,  0.8163,  0.7569],\n",
      "        [-0.2088,  0.8296,  0.8849]], requires_grad=True)\n",
      "new b tensor([ 1.6482, -9.6619], requires_grad=True)\n",
      "Loss= tensor(1.5738, grad_fn=<DivBackward0>)\n",
      "i= 491\n",
      "w.grad= tensor([[-1.2290, -2.8708,  7.6554],\n",
      "        [-1.0018,  2.8086, -9.1864]])\n",
      "b.grad= tensor([-64.1873, 343.3806])\n",
      "new w tensor([[-0.4316,  0.8165,  0.7565],\n",
      "        [-0.2087,  0.8295,  0.8854]], requires_grad=True)\n",
      "new b tensor([ 1.6514, -9.6790], requires_grad=True)\n",
      "Loss= tensor(1.5681, grad_fn=<DivBackward0>)\n",
      "i= 492\n",
      "w.grad= tensor([[-1.2215, -2.8576,  7.6215],\n",
      "        [-1.0083,  2.7962, -9.1536]])\n",
      "b.grad= tensor([-64.1195, 343.2515])\n",
      "new w tensor([[-0.4316,  0.8166,  0.7561],\n",
      "        [-0.2087,  0.8293,  0.8859]], requires_grad=True)\n",
      "new b tensor([ 1.6546, -9.6962], requires_grad=True)\n",
      "Loss= tensor(1.5625, grad_fn=<DivBackward0>)\n",
      "i= 493\n",
      "w.grad= tensor([[-1.2140, -2.8443,  7.5877],\n",
      "        [-1.0151,  2.7834, -9.1211]])\n",
      "b.grad= tensor([-64.0519, 343.1224])\n",
      "new w tensor([[-0.4315,  0.8168,  0.7558],\n",
      "        [-0.2086,  0.8292,  0.8863]], requires_grad=True)\n",
      "new b tensor([ 1.6578, -9.7134], requires_grad=True)\n",
      "Loss= tensor(1.5569, grad_fn=<DivBackward0>)\n",
      "i= 494\n",
      "w.grad= tensor([[-1.2066, -2.8311,  7.5541],\n",
      "        [-1.0215,  2.7712, -9.0885]])\n",
      "b.grad= tensor([-63.9844, 342.9933])\n",
      "new w tensor([[-0.4315,  0.8169,  0.7554],\n",
      "        [-0.2086,  0.8290,  0.8868]], requires_grad=True)\n",
      "new b tensor([ 1.6610, -9.7305], requires_grad=True)\n",
      "Loss= tensor(1.5514, grad_fn=<DivBackward0>)\n",
      "i= 495\n",
      "w.grad= tensor([[-1.1994, -2.8182,  7.5205],\n",
      "        [-1.0287,  2.7579, -9.0567]])\n",
      "b.grad= tensor([-63.9170, 342.8642])\n",
      "new w tensor([[-0.4314,  0.8170,  0.7550],\n",
      "        [-0.2085,  0.8289,  0.8872]], requires_grad=True)\n",
      "new b tensor([ 1.6642, -9.7477], requires_grad=True)\n",
      "Loss= tensor(1.5459, grad_fn=<DivBackward0>)\n",
      "i= 496\n",
      "w.grad= tensor([[-1.1925, -2.8058,  7.4868],\n",
      "        [-1.0345,  2.7464, -9.0240]])\n",
      "b.grad= tensor([-63.8498, 342.7351])\n",
      "new w tensor([[-0.4313,  0.8172,  0.7546],\n",
      "        [-0.2085,  0.8288,  0.8877]], requires_grad=True)\n",
      "new b tensor([ 1.6674, -9.7648], requires_grad=True)\n",
      "Loss= tensor(1.5405, grad_fn=<DivBackward0>)\n",
      "i= 497\n",
      "w.grad= tensor([[-1.1844, -2.7920,  7.4542],\n",
      "        [-1.0412,  2.7339, -8.9922]])\n",
      "b.grad= tensor([-63.7827, 342.6061])\n",
      "new w tensor([[-0.4313,  0.8173,  0.7543],\n",
      "        [-0.2084,  0.8286,  0.8881]], requires_grad=True)\n",
      "new b tensor([ 1.6706, -9.7819], requires_grad=True)\n",
      "Loss= tensor(1.5352, grad_fn=<DivBackward0>)\n",
      "i= 498\n",
      "w.grad= tensor([[-1.1773, -2.7793,  7.4211],\n",
      "        [-1.0473,  2.7221, -8.9600]])\n",
      "b.grad= tensor([-63.7158, 342.4771])\n",
      "new w tensor([[-0.4312,  0.8175,  0.7539],\n",
      "        [-0.2084,  0.8285,  0.8886]], requires_grad=True)\n",
      "new b tensor([ 1.6738, -9.7990], requires_grad=True)\n",
      "Loss= tensor(1.5299, grad_fn=<DivBackward0>)\n",
      "i= 499\n",
      "w.grad= tensor([[-1.1702, -2.7666,  7.3881],\n",
      "        [-1.0540,  2.7096, -8.9285]])\n",
      "b.grad= tensor([-63.6490, 342.3481])\n",
      "new w tensor([[-0.4312,  0.8176,  0.7535],\n",
      "        [-0.2083,  0.8284,  0.8890]], requires_grad=True)\n",
      "new b tensor([ 1.6769, -9.8162], requires_grad=True)\n",
      "Loss= tensor(1.5247, grad_fn=<DivBackward0>)\n",
      "i= 500\n",
      "w.grad= tensor([[-1.1632, -2.7541,  7.3552],\n",
      "        [-1.0603,  2.6974, -8.8969]])\n",
      "b.grad= tensor([-63.5823, 342.2191])\n",
      "new w tensor([[-0.4311,  0.8177,  0.7532],\n",
      "        [-0.2083,  0.8282,  0.8895]], requires_grad=True)\n",
      "new b tensor([ 1.6801, -9.8333], requires_grad=True)\n",
      "Loss= tensor(1.5196, grad_fn=<DivBackward0>)\n",
      "i= 501\n",
      "w.grad= tensor([[-1.1555, -2.7408,  7.3231],\n",
      "        [-1.0668,  2.6850, -8.8656]])\n",
      "b.grad= tensor([-63.5158, 342.0902])\n",
      "new w tensor([[-0.4310,  0.8179,  0.7528],\n",
      "        [-0.2082,  0.8281,  0.8899]], requires_grad=True)\n",
      "new b tensor([ 1.6833, -9.8504], requires_grad=True)\n",
      "Loss= tensor(1.5145, grad_fn=<DivBackward0>)\n",
      "i= 502\n",
      "w.grad= tensor([[-1.1489, -2.7287,  7.2903],\n",
      "        [-1.0728,  2.6734, -8.8341]])\n",
      "b.grad= tensor([-63.4494, 341.9612])\n",
      "new w tensor([[-0.4310,  0.8180,  0.7524],\n",
      "        [-0.2082,  0.8280,  0.8903]], requires_grad=True)\n",
      "new b tensor([ 1.6865, -9.8675], requires_grad=True)\n",
      "Loss= tensor(1.5094, grad_fn=<DivBackward0>)\n",
      "i= 503\n",
      "w.grad= tensor([[-1.1419, -2.7162,  7.2580],\n",
      "        [-1.0791,  2.6615, -8.8029]])\n",
      "b.grad= tensor([-63.3831, 341.8323])\n",
      "new w tensor([[-0.4309,  0.8182,  0.7521],\n",
      "        [-0.2081,  0.8278,  0.8908]], requires_grad=True)\n",
      "new b tensor([ 1.6896, -9.8846], requires_grad=True)\n",
      "Loss= tensor(1.5045, grad_fn=<DivBackward0>)\n",
      "i= 504\n",
      "w.grad= tensor([[-1.1341, -2.7030,  7.2263],\n",
      "        [-1.0854,  2.6496, -8.7720]])\n",
      "b.grad= tensor([-63.3169, 341.7034])\n",
      "new w tensor([[-0.4309,  0.8183,  0.7517],\n",
      "        [-0.2081,  0.8277,  0.8912]], requires_grad=True)\n",
      "new b tensor([ 1.6928, -9.9017], requires_grad=True)\n",
      "Loss= tensor(1.4995, grad_fn=<DivBackward0>)\n",
      "i= 505\n",
      "w.grad= tensor([[-1.1277, -2.6912,  7.1939],\n",
      "        [-1.0924,  2.6366, -8.7417]])\n",
      "b.grad= tensor([-63.2509, 341.5745])\n",
      "new w tensor([[-0.4308,  0.8184,  0.7513],\n",
      "        [-0.2080,  0.8276,  0.8917]], requires_grad=True)\n",
      "new b tensor([ 1.6960, -9.9187], requires_grad=True)\n",
      "Loss= tensor(1.4947, grad_fn=<DivBackward0>)\n",
      "i= 506\n",
      "w.grad= tensor([[-1.1201, -2.6781,  7.1626],\n",
      "        [-1.0975,  2.6261, -8.7102]])\n",
      "b.grad= tensor([-63.1851, 341.4456])\n",
      "new w tensor([[-0.4308,  0.8186,  0.7510],\n",
      "        [-0.2080,  0.8274,  0.8921]], requires_grad=True)\n",
      "new b tensor([ 1.6991, -9.9358], requires_grad=True)\n",
      "Loss= tensor(1.4898, grad_fn=<DivBackward0>)\n",
      "i= 507\n",
      "w.grad= tensor([[-1.1132, -2.6657,  7.1308],\n",
      "        [-1.1041,  2.6138, -8.6799]])\n",
      "b.grad= tensor([-63.1193, 341.3168])\n",
      "new w tensor([[-0.4307,  0.8187,  0.7506],\n",
      "        [-0.2079,  0.8273,  0.8925]], requires_grad=True)\n",
      "new b tensor([ 1.7023, -9.9529], requires_grad=True)\n",
      "Loss= tensor(1.4851, grad_fn=<DivBackward0>)\n",
      "i= 508\n",
      "w.grad= tensor([[-1.1074, -2.6548,  7.0985],\n",
      "        [-1.1099,  2.6026, -8.6493]])\n",
      "b.grad= tensor([-63.0537, 341.1880])\n",
      "new w tensor([[-0.4306,  0.8188,  0.7503],\n",
      "        [-0.2078,  0.8272,  0.8930]], requires_grad=True)\n",
      "new b tensor([ 1.7054, -9.9699], requires_grad=True)\n",
      "Loss= tensor(1.4804, grad_fn=<DivBackward0>)\n",
      "i= 509\n",
      "w.grad= tensor([[-1.0995, -2.6414,  7.0678],\n",
      "        [-1.1165,  2.5902, -8.6193]])\n",
      "b.grad= tensor([-62.9882, 341.0591])\n",
      "new w tensor([[-0.4306,  0.8190,  0.7499],\n",
      "        [-0.2078,  0.8270,  0.8934]], requires_grad=True)\n",
      "new b tensor([ 1.7086, -9.9870], requires_grad=True)\n",
      "Loss= tensor(1.4757, grad_fn=<DivBackward0>)\n",
      "i= 510\n",
      "w.grad= tensor([[-1.0930, -2.6296,  7.0363],\n",
      "        [-1.1220,  2.5792, -8.5888]])\n",
      "b.grad= tensor([-62.9228, 340.9303])\n",
      "new w tensor([[-0.4305,  0.8191,  0.7496],\n",
      "        [-0.2077,  0.8269,  0.8938]], requires_grad=True)\n",
      "new b tensor([  1.7117, -10.0040], requires_grad=True)\n",
      "Loss= tensor(1.4711, grad_fn=<DivBackward0>)\n",
      "i= 511\n",
      "w.grad= tensor([[-1.0857, -2.6171,  7.0055],\n",
      "        [-1.1283,  2.5673, -8.5589]])\n",
      "b.grad= tensor([-62.8576, 340.8015])\n",
      "new w tensor([[-0.4305,  0.8192,  0.7492],\n",
      "        [-0.2077,  0.8268,  0.8942]], requires_grad=True)\n",
      "new b tensor([  1.7149, -10.0211], requires_grad=True)\n",
      "Loss= tensor(1.4666, grad_fn=<DivBackward0>)\n",
      "i= 512\n",
      "w.grad= tensor([[-1.0793, -2.6054,  6.9743],\n",
      "        [-1.1345,  2.5556, -8.5292]])\n",
      "b.grad= tensor([-62.7924, 340.6727])\n",
      "new w tensor([[-0.4304,  0.8193,  0.7489],\n",
      "        [-0.2076,  0.8267,  0.8947]], requires_grad=True)\n",
      "new b tensor([  1.7180, -10.0381], requires_grad=True)\n",
      "Loss= tensor(1.4621, grad_fn=<DivBackward0>)\n",
      "i= 513\n",
      "w.grad= tensor([[-1.0723, -2.5931,  6.9437],\n",
      "        [-1.1396,  2.5451, -8.4989]])\n",
      "b.grad= tensor([-62.7274, 340.5439])\n",
      "new w tensor([[-0.4304,  0.8195,  0.7485],\n",
      "        [-0.2076,  0.8265,  0.8951]], requires_grad=True)\n",
      "new b tensor([  1.7211, -10.0551], requires_grad=True)\n",
      "Loss= tensor(1.4576, grad_fn=<DivBackward0>)\n",
      "i= 514\n",
      "w.grad= tensor([[-1.0661, -2.5817,  6.9127],\n",
      "        [-1.1460,  2.5332, -8.4696]])\n",
      "b.grad= tensor([-62.6626, 340.4151])\n",
      "new w tensor([[-0.4303,  0.8196,  0.7482],\n",
      "        [-0.2075,  0.8264,  0.8955]], requires_grad=True)\n",
      "new b tensor([  1.7243, -10.0722], requires_grad=True)\n",
      "Loss= tensor(1.4532, grad_fn=<DivBackward0>)\n",
      "i= 515\n",
      "w.grad= tensor([[-1.0588, -2.5692,  6.8825],\n",
      "        [-1.1520,  2.5216, -8.4402]])\n",
      "b.grad= tensor([-62.5978, 340.2864])\n",
      "new w tensor([[-0.4303,  0.8197,  0.7478],\n",
      "        [-0.2074,  0.8263,  0.8959]], requires_grad=True)\n",
      "new b tensor([  1.7274, -10.0892], requires_grad=True)\n",
      "Loss= tensor(1.4489, grad_fn=<DivBackward0>)\n",
      "i= 516\n",
      "w.grad= tensor([[-1.0525, -2.5577,  6.8520],\n",
      "        [-1.1574,  2.5107, -8.4105]])\n",
      "b.grad= tensor([-62.5332, 340.1576])\n",
      "new w tensor([[-0.4302,  0.8199,  0.7475],\n",
      "        [-0.2074,  0.8262,  0.8964]], requires_grad=True)\n",
      "new b tensor([  1.7305, -10.1062], requires_grad=True)\n",
      "Loss= tensor(1.4446, grad_fn=<DivBackward0>)\n",
      "i= 517\n",
      "w.grad= tensor([[-1.0456, -2.5457,  6.8219],\n",
      "        [-1.1634,  2.4995, -8.3814]])\n",
      "b.grad= tensor([-62.4687, 340.0289])\n",
      "new w tensor([[-0.4302,  0.8200,  0.7472],\n",
      "        [-0.2073,  0.8260,  0.8968]], requires_grad=True)\n",
      "new b tensor([  1.7337, -10.1232], requires_grad=True)\n",
      "Loss= tensor(1.4403, grad_fn=<DivBackward0>)\n",
      "i= 518\n",
      "w.grad= tensor([[-1.0394, -2.5343,  6.7915],\n",
      "        [-1.1691,  2.4885, -8.3523]])\n",
      "b.grad= tensor([-62.4043, 339.9001])\n",
      "new w tensor([[-0.4301,  0.8201,  0.7468],\n",
      "        [-0.2073,  0.8259,  0.8972]], requires_grad=True)\n",
      "new b tensor([  1.7368, -10.1402], requires_grad=True)\n",
      "Loss= tensor(1.4361, grad_fn=<DivBackward0>)\n",
      "i= 519\n",
      "w.grad= tensor([[-1.0326, -2.5224,  6.7617],\n",
      "        [-1.1753,  2.4768, -8.3236]])\n",
      "b.grad= tensor([-62.3401, 339.7714])\n",
      "new w tensor([[-0.4301,  0.8202,  0.7465],\n",
      "        [-0.2072,  0.8258,  0.8976]], requires_grad=True)\n",
      "new b tensor([  1.7399, -10.1572], requires_grad=True)\n",
      "Loss= tensor(1.4320, grad_fn=<DivBackward0>)\n",
      "i= 520\n",
      "w.grad= tensor([[-1.0264, -2.5112,  6.7317],\n",
      "        [-1.1810,  2.4656, -8.2948]])\n",
      "b.grad= tensor([-62.2760, 339.6427])\n",
      "new w tensor([[-0.4300,  0.8204,  0.7461],\n",
      "        [-0.2072,  0.8257,  0.8980]], requires_grad=True)\n",
      "new b tensor([  1.7430, -10.1741], requires_grad=True)\n",
      "Loss= tensor(1.4279, grad_fn=<DivBackward0>)\n",
      "i= 521\n",
      "w.grad= tensor([[-1.0193, -2.4990,  6.7024],\n",
      "        [-1.1861,  2.4552, -8.2657]])\n",
      "b.grad= tensor([-62.2119, 339.5140])\n",
      "new w tensor([[-0.4300,  0.8205,  0.7458],\n",
      "        [-0.2071,  0.8255,  0.8984]], requires_grad=True)\n",
      "new b tensor([  1.7461, -10.1911], requires_grad=True)\n",
      "Loss= tensor(1.4238, grad_fn=<DivBackward0>)\n",
      "i= 522\n",
      "w.grad= tensor([[-1.0130, -2.4875,  6.6727],\n",
      "        [-1.1919,  2.4443, -8.2372]])\n",
      "b.grad= tensor([-62.1480, 339.3853])\n",
      "new w tensor([[-0.4299,  0.8206,  0.7455],\n",
      "        [-0.2070,  0.8254,  0.8989]], requires_grad=True)\n",
      "new b tensor([  1.7492, -10.2081], requires_grad=True)\n",
      "Loss= tensor(1.4198, grad_fn=<DivBackward0>)\n",
      "i= 523\n",
      "w.grad= tensor([[-1.0066, -2.4762,  6.6432],\n",
      "        [-1.1975,  2.4334, -8.2088]])\n",
      "b.grad= tensor([-62.0843, 339.2566])\n",
      "new w tensor([[-0.4299,  0.8207,  0.7451],\n",
      "        [-0.2070,  0.8253,  0.8993]], requires_grad=True)\n",
      "new b tensor([  1.7523, -10.2250], requires_grad=True)\n",
      "Loss= tensor(1.4159, grad_fn=<DivBackward0>)\n",
      "i= 524\n",
      "w.grad= tensor([[-1.0002, -2.4649,  6.6140],\n",
      "        [-1.2036,  2.4220, -8.1808]])\n",
      "b.grad= tensor([-62.0206, 339.1279])\n",
      "new w tensor([[-0.4298,  0.8209,  0.7448],\n",
      "        [-0.2069,  0.8252,  0.8997]], requires_grad=True)\n",
      "new b tensor([  1.7554, -10.2420], requires_grad=True)\n",
      "Loss= tensor(1.4120, grad_fn=<DivBackward0>)\n",
      "i= 525\n",
      "w.grad= tensor([[-0.9940, -2.4537,  6.5847],\n",
      "        [-1.2081,  2.4124, -8.1518]])\n",
      "b.grad= tensor([-61.9571, 338.9992])\n",
      "new w tensor([[-0.4298,  0.8210,  0.7445],\n",
      "        [-0.2069,  0.8250,  0.9001]], requires_grad=True)\n",
      "new b tensor([  1.7585, -10.2590], requires_grad=True)\n",
      "Loss= tensor(1.4081, grad_fn=<DivBackward0>)\n",
      "i= 526\n",
      "w.grad= tensor([[-0.9872, -2.4418,  6.5560],\n",
      "        [-1.2148,  2.4003, -8.1246]])\n",
      "b.grad= tensor([-61.8937, 338.8705])\n",
      "new w tensor([[-0.4297,  0.8211,  0.7441],\n",
      "        [-0.2068,  0.8249,  0.9005]], requires_grad=True)\n",
      "new b tensor([  1.7616, -10.2759], requires_grad=True)\n",
      "Loss= tensor(1.4043, grad_fn=<DivBackward0>)\n",
      "i= 527\n",
      "w.grad= tensor([[-0.9820, -2.4319,  6.5264],\n",
      "        [-1.2201,  2.3898, -8.0966]])\n",
      "b.grad= tensor([-61.8304, 338.7418])\n",
      "new w tensor([[-0.4297,  0.8212,  0.7438],\n",
      "        [-0.2067,  0.8248,  0.9009]], requires_grad=True)\n",
      "new b tensor([  1.7647, -10.2928], requires_grad=True)\n",
      "Loss= tensor(1.4005, grad_fn=<DivBackward0>)\n",
      "i= 528\n",
      "w.grad= tensor([[-0.9748, -2.4196,  6.4982],\n",
      "        [-1.2254,  2.3793, -8.0686]])\n",
      "b.grad= tensor([-61.7672, 338.6131])\n",
      "new w tensor([[-0.4296,  0.8214,  0.7435],\n",
      "        [-0.2067,  0.8247,  0.9013]], requires_grad=True)\n",
      "new b tensor([  1.7678, -10.3098], requires_grad=True)\n",
      "Loss= tensor(1.3968, grad_fn=<DivBackward0>)\n",
      "i= 529\n",
      "w.grad= tensor([[-0.9686, -2.4086,  6.4696],\n",
      "        [-1.2309,  2.3686, -8.0410]])\n",
      "b.grad= tensor([-61.7041, 338.4844])\n",
      "new w tensor([[-0.4296,  0.8215,  0.7432],\n",
      "        [-0.2066,  0.8246,  0.9017]], requires_grad=True)\n",
      "new b tensor([  1.7709, -10.3267], requires_grad=True)\n",
      "Loss= tensor(1.3931, grad_fn=<DivBackward0>)\n",
      "i= 530\n",
      "w.grad= tensor([[-0.9627, -2.3978,  6.4409],\n",
      "        [-1.2359,  2.3585, -8.0132]])\n",
      "b.grad= tensor([-61.6411, 338.3557])\n",
      "new w tensor([[-0.4295,  0.8216,  0.7429],\n",
      "        [-0.2065,  0.8245,  0.9021]], requires_grad=True)\n",
      "new b tensor([  1.7740, -10.3436], requires_grad=True)\n",
      "Loss= tensor(1.3895, grad_fn=<DivBackward0>)\n",
      "i= 531\n",
      "w.grad= tensor([[-0.9564, -2.3866,  6.4127],\n",
      "        [-1.2414,  2.3479, -7.9858]])\n",
      "b.grad= tensor([-61.5783, 338.2270])\n",
      "new w tensor([[-0.4295,  0.8217,  0.7425],\n",
      "        [-0.2065,  0.8243,  0.9025]], requires_grad=True)\n",
      "new b tensor([  1.7770, -10.3605], requires_grad=True)\n",
      "Loss= tensor(1.3859, grad_fn=<DivBackward0>)\n",
      "i= 532\n",
      "w.grad= tensor([[-0.9503, -2.3758,  6.3843],\n",
      "        [-1.2472,  2.3369, -7.9588]])\n",
      "b.grad= tensor([-61.5156, 338.0983])\n",
      "new w tensor([[-0.4294,  0.8218,  0.7422],\n",
      "        [-0.2064,  0.8242,  0.9029]], requires_grad=True)\n",
      "new b tensor([  1.7801, -10.3774], requires_grad=True)\n",
      "Loss= tensor(1.3824, grad_fn=<DivBackward0>)\n",
      "i= 533\n",
      "w.grad= tensor([[-0.9443, -2.3649,  6.3562],\n",
      "        [-1.2520,  2.3271, -7.9312]])\n",
      "b.grad= tensor([-61.4529, 337.9696])\n",
      "new w tensor([[-0.4294,  0.8219,  0.7419],\n",
      "        [-0.2064,  0.8241,  0.9033]], requires_grad=True)\n",
      "new b tensor([  1.7832, -10.3943], requires_grad=True)\n",
      "Loss= tensor(1.3788, grad_fn=<DivBackward0>)\n",
      "i= 534\n",
      "w.grad= tensor([[-0.9380, -2.3538,  6.3284],\n",
      "        [-1.2571,  2.3170, -7.9041]])\n",
      "b.grad= tensor([-61.3904, 337.8409])\n",
      "new w tensor([[-0.4293,  0.8221,  0.7416],\n",
      "        [-0.2063,  0.8240,  0.9037]], requires_grad=True)\n",
      "new b tensor([  1.7863, -10.4112], requires_grad=True)\n",
      "Loss= tensor(1.3754, grad_fn=<DivBackward0>)\n",
      "i= 535\n",
      "w.grad= tensor([[-0.9316, -2.3427,  6.3008],\n",
      "        [-1.2633,  2.3055, -7.8778]])\n",
      "b.grad= tensor([-61.3280, 337.7122])\n",
      "new w tensor([[-0.4293,  0.8222,  0.7413],\n",
      "        [-0.2062,  0.8239,  0.9041]], requires_grad=True)\n",
      "new b tensor([  1.7893, -10.4281], requires_grad=True)\n",
      "Loss= tensor(1.3720, grad_fn=<DivBackward0>)\n",
      "i= 536\n",
      "w.grad= tensor([[-0.9257, -2.3320,  6.2729],\n",
      "        [-1.2675,  2.2966, -7.8502]])\n",
      "b.grad= tensor([-61.2657, 337.5836])\n",
      "new w tensor([[-0.4292,  0.8223,  0.7410],\n",
      "        [-0.2062,  0.8238,  0.9045]], requires_grad=True)\n",
      "new b tensor([  1.7924, -10.4450], requires_grad=True)\n",
      "Loss= tensor(1.3686, grad_fn=<DivBackward0>)\n",
      "i= 537\n",
      "w.grad= tensor([[-0.9198, -2.3214,  6.2453],\n",
      "        [-1.2736,  2.2854, -7.8242]])\n",
      "b.grad= tensor([-61.2036, 337.4549])\n",
      "new w tensor([[-0.4292,  0.8224,  0.7406],\n",
      "        [-0.2061,  0.8236,  0.9049]], requires_grad=True)\n",
      "new b tensor([  1.7955, -10.4619], requires_grad=True)\n",
      "Loss= tensor(1.3653, grad_fn=<DivBackward0>)\n",
      "i= 538\n",
      "w.grad= tensor([[-0.9139, -2.3109,  6.2178],\n",
      "        [-1.2783,  2.2757, -7.7972]])\n",
      "b.grad= tensor([-61.1415, 337.3262])\n",
      "new w tensor([[-0.4291,  0.8225,  0.7403],\n",
      "        [-0.2060,  0.8235,  0.9053]], requires_grad=True)\n",
      "new b tensor([  1.7985, -10.4787], requires_grad=True)\n",
      "Loss= tensor(1.3620, grad_fn=<DivBackward0>)\n",
      "i= 539\n",
      "w.grad= tensor([[-0.9079, -2.3002,  6.1905],\n",
      "        [-1.2836,  2.2654, -7.7710]])\n",
      "b.grad= tensor([-61.0795, 337.1975])\n",
      "new w tensor([[-0.4291,  0.8226,  0.7400],\n",
      "        [-0.2060,  0.8234,  0.9056]], requires_grad=True)\n",
      "new b tensor([  1.8016, -10.4956], requires_grad=True)\n",
      "Loss= tensor(1.3587, grad_fn=<DivBackward0>)\n",
      "i= 540\n",
      "w.grad= tensor([[-0.9021, -2.2897,  6.1632],\n",
      "        [-1.2885,  2.2557, -7.7445]])\n",
      "b.grad= tensor([-61.0177, 337.0688])\n",
      "new w tensor([[-0.4290,  0.8228,  0.7397],\n",
      "        [-0.2059,  0.8233,  0.9060]], requires_grad=True)\n",
      "new b tensor([  1.8046, -10.5124], requires_grad=True)\n",
      "Loss= tensor(1.3555, grad_fn=<DivBackward0>)\n",
      "i= 541\n",
      "w.grad= tensor([[-0.8962, -2.2791,  6.1362],\n",
      "        [-1.2934,  2.2457, -7.7181]])\n",
      "b.grad= tensor([-60.9560, 336.9401])\n",
      "new w tensor([[-0.4290,  0.8229,  0.7394],\n",
      "        [-0.2058,  0.8232,  0.9064]], requires_grad=True)\n",
      "new b tensor([  1.8077, -10.5293], requires_grad=True)\n",
      "Loss= tensor(1.3523, grad_fn=<DivBackward0>)\n",
      "i= 542\n",
      "w.grad= tensor([[-0.8897, -2.2679,  6.1097],\n",
      "        [-1.2987,  2.2355, -7.6922]])\n",
      "b.grad= tensor([-60.8943, 336.8114])\n",
      "new w tensor([[-0.4290,  0.8230,  0.7391],\n",
      "        [-0.2058,  0.8231,  0.9068]], requires_grad=True)\n",
      "new b tensor([  1.8107, -10.5461], requires_grad=True)\n",
      "Loss= tensor(1.3492, grad_fn=<DivBackward0>)\n",
      "i= 543\n",
      "w.grad= tensor([[-0.8845, -2.2583,  6.0823],\n",
      "        [-1.3036,  2.2255, -7.6661]])\n",
      "b.grad= tensor([-60.8328, 336.6826])\n",
      "new w tensor([[-0.4289,  0.8231,  0.7388],\n",
      "        [-0.2057,  0.8230,  0.9072]], requires_grad=True)\n",
      "new b tensor([  1.8138, -10.5630], requires_grad=True)\n",
      "Loss= tensor(1.3461, grad_fn=<DivBackward0>)\n",
      "i= 544\n",
      "w.grad= tensor([[-0.8787, -2.2479,  6.0557],\n",
      "        [-1.3086,  2.2158, -7.6403]])\n",
      "b.grad= tensor([-60.7714, 336.5539])\n",
      "new w tensor([[-0.4289,  0.8232,  0.7385],\n",
      "        [-0.2057,  0.8229,  0.9076]], requires_grad=True)\n",
      "new b tensor([  1.8168, -10.5798], requires_grad=True)\n",
      "Loss= tensor(1.3431, grad_fn=<DivBackward0>)\n",
      "i= 545\n",
      "w.grad= tensor([[-0.8727, -2.2374,  6.0292],\n",
      "        [-1.3138,  2.2057, -7.6146]])\n",
      "b.grad= tensor([-60.7101, 336.4252])\n",
      "new w tensor([[-0.4288,  0.8233,  0.7382],\n",
      "        [-0.2056,  0.8227,  0.9080]], requires_grad=True)\n",
      "new b tensor([  1.8198, -10.5966], requires_grad=True)\n",
      "Loss= tensor(1.3401, grad_fn=<DivBackward0>)\n",
      "i= 546\n",
      "w.grad= tensor([[-0.8670, -2.2271,  6.0028],\n",
      "        [-1.3182,  2.1966, -7.5887]])\n",
      "b.grad= tensor([-60.6489, 336.2965])\n",
      "new w tensor([[-0.4288,  0.8234,  0.7379],\n",
      "        [-0.2055,  0.8226,  0.9083]], requires_grad=True)\n",
      "new b tensor([  1.8229, -10.6134], requires_grad=True)\n",
      "Loss= tensor(1.3371, grad_fn=<DivBackward0>)\n",
      "i= 547\n",
      "w.grad= tensor([[-0.8611, -2.2166,  5.9766],\n",
      "        [-1.3240,  2.1857, -7.5637]])\n",
      "b.grad= tensor([-60.5878, 336.1677])\n",
      "new w tensor([[-0.4287,  0.8235,  0.7376],\n",
      "        [-0.2055,  0.8225,  0.9087]], requires_grad=True)\n",
      "new b tensor([  1.8259, -10.6302], requires_grad=True)\n",
      "Loss= tensor(1.3341, grad_fn=<DivBackward0>)\n",
      "i= 548\n",
      "w.grad= tensor([[-0.8558, -2.2069,  5.9501],\n",
      "        [-1.3283,  2.1766, -7.5380]])\n",
      "b.grad= tensor([-60.5268, 336.0390])\n",
      "new w tensor([[-0.4287,  0.8237,  0.7373],\n",
      "        [-0.2054,  0.8224,  0.9091]], requires_grad=True)\n",
      "new b tensor([  1.8289, -10.6470], requires_grad=True)\n",
      "Loss= tensor(1.3312, grad_fn=<DivBackward0>)\n",
      "i= 549\n",
      "w.grad= tensor([[-0.8503, -2.1969,  5.9239],\n",
      "        [-1.3334,  2.1669, -7.5128]])\n",
      "b.grad= tensor([-60.4659, 335.9102])\n",
      "new w tensor([[-0.4287,  0.8238,  0.7370],\n",
      "        [-0.2053,  0.8223,  0.9095]], requires_grad=True)\n",
      "new b tensor([  1.8319, -10.6638], requires_grad=True)\n",
      "Loss= tensor(1.3284, grad_fn=<DivBackward0>)\n",
      "i= 550\n",
      "w.grad= tensor([[-0.8438, -2.1859,  5.8985],\n",
      "        [-1.3381,  2.1574, -7.4876]])\n",
      "b.grad= tensor([-60.4051, 335.7815])\n",
      "new w tensor([[-0.4286,  0.8239,  0.7367],\n",
      "        [-0.2053,  0.8222,  0.9098]], requires_grad=True)\n",
      "new b tensor([  1.8350, -10.6806], requires_grad=True)\n",
      "Loss= tensor(1.3256, grad_fn=<DivBackward0>)\n",
      "i= 551\n",
      "w.grad= tensor([[-0.8385, -2.1763,  5.8724],\n",
      "        [-1.3435,  2.1472, -7.4629]])\n",
      "b.grad= tensor([-60.3444, 335.6527])\n",
      "new w tensor([[-0.4286,  0.8240,  0.7364],\n",
      "        [-0.2052,  0.8221,  0.9102]], requires_grad=True)\n",
      "new b tensor([  1.8380, -10.6974], requires_grad=True)\n",
      "Loss= tensor(1.3228, grad_fn=<DivBackward0>)\n",
      "i= 552\n",
      "w.grad= tensor([[-0.8328, -2.1661,  5.8468],\n",
      "        [-1.3479,  2.1380, -7.4377]])\n",
      "b.grad= tensor([-60.2838, 335.5239])\n",
      "new w tensor([[-0.4285,  0.8241,  0.7361],\n",
      "        [-0.2051,  0.8220,  0.9106]], requires_grad=True)\n",
      "new b tensor([  1.8410, -10.7142], requires_grad=True)\n",
      "Loss= tensor(1.3200, grad_fn=<DivBackward0>)\n",
      "i= 553\n",
      "w.grad= tensor([[-0.8275, -2.1564,  5.8210],\n",
      "        [-1.3520,  2.1292, -7.4124]])\n",
      "b.grad= tensor([-60.2233, 335.3951])\n",
      "new w tensor([[-0.4285,  0.8242,  0.7358],\n",
      "        [-0.2051,  0.8219,  0.9110]], requires_grad=True)\n",
      "new b tensor([  1.8440, -10.7309], requires_grad=True)\n",
      "Loss= tensor(1.3173, grad_fn=<DivBackward0>)\n",
      "i= 554\n",
      "w.grad= tensor([[-0.8221, -2.1466,  5.7954],\n",
      "        [-1.3568,  2.1199, -7.3877]])\n",
      "b.grad= tensor([-60.1629, 335.2663])\n",
      "new w tensor([[-0.4284,  0.8243,  0.7355],\n",
      "        [-0.2050,  0.8218,  0.9113]], requires_grad=True)\n",
      "new b tensor([  1.8470, -10.7477], requires_grad=True)\n",
      "Loss= tensor(1.3146, grad_fn=<DivBackward0>)\n",
      "i= 555\n",
      "w.grad= tensor([[-0.8164, -2.1365,  5.7702],\n",
      "        [-1.3620,  2.1099, -7.3634]])\n",
      "b.grad= tensor([-60.1026, 335.1375])\n",
      "new w tensor([[-0.4284,  0.8244,  0.7352],\n",
      "        [-0.2049,  0.8217,  0.9117]], requires_grad=True)\n",
      "new b tensor([  1.8500, -10.7645], requires_grad=True)\n",
      "Loss= tensor(1.3120, grad_fn=<DivBackward0>)\n",
      "i= 556\n",
      "w.grad= tensor([[-0.8110, -2.1267,  5.7449],\n",
      "        [-1.3670,  2.1003, -7.3392]])\n",
      "b.grad= tensor([-60.0425, 335.0086])\n",
      "new w tensor([[-0.4284,  0.8245,  0.7350],\n",
      "        [-0.2048,  0.8216,  0.9121]], requires_grad=True)\n",
      "new b tensor([  1.8530, -10.7812], requires_grad=True)\n",
      "Loss= tensor(1.3094, grad_fn=<DivBackward0>)\n",
      "i= 557\n",
      "w.grad= tensor([[-0.8056, -2.1171,  5.7197],\n",
      "        [-1.3710,  2.0916, -7.3144]])\n",
      "b.grad= tensor([-59.9824, 334.8798])\n",
      "new w tensor([[-0.4283,  0.8246,  0.7347],\n",
      "        [-0.2048,  0.8215,  0.9124]], requires_grad=True)\n",
      "new b tensor([  1.8560, -10.7980], requires_grad=True)\n",
      "Loss= tensor(1.3068, grad_fn=<DivBackward0>)\n",
      "i= 558\n",
      "w.grad= tensor([[-0.7999, -2.1070,  5.6948],\n",
      "        [-1.3761,  2.0818, -7.2904]])\n",
      "b.grad= tensor([-59.9224, 334.7509])\n",
      "new w tensor([[-0.4283,  0.8247,  0.7344],\n",
      "        [-0.2047,  0.8214,  0.9128]], requires_grad=True)\n",
      "new b tensor([  1.8590, -10.8147], requires_grad=True)\n",
      "Loss= tensor(1.3043, grad_fn=<DivBackward0>)\n",
      "i= 559\n",
      "w.grad= tensor([[-0.7948, -2.0977,  5.6697],\n",
      "        [-1.3804,  2.0728, -7.2661]])\n",
      "b.grad= tensor([-59.8625, 334.6221])\n",
      "new w tensor([[-0.4282,  0.8248,  0.7341],\n",
      "        [-0.2046,  0.8212,  0.9132]], requires_grad=True)\n",
      "new b tensor([  1.8620, -10.8314], requires_grad=True)\n",
      "Loss= tensor(1.3018, grad_fn=<DivBackward0>)\n",
      "i= 560\n",
      "w.grad= tensor([[-0.7894, -2.0880,  5.6450],\n",
      "        [-1.3850,  2.0637, -7.2421]])\n",
      "b.grad= tensor([-59.8027, 334.4932])\n",
      "new w tensor([[-0.4282,  0.8249,  0.7338],\n",
      "        [-0.2046,  0.8211,  0.9135]], requires_grad=True)\n",
      "new b tensor([  1.8650, -10.8482], requires_grad=True)\n",
      "Loss= tensor(1.2993, grad_fn=<DivBackward0>)\n",
      "i= 561\n",
      "w.grad= tensor([[-0.7837, -2.0779,  5.6205],\n",
      "        [-1.3890,  2.0553, -7.2177]])\n",
      "b.grad= tensor([-59.7430, 334.3643])\n",
      "new w tensor([[-0.4282,  0.8250,  0.7335],\n",
      "        [-0.2045,  0.8210,  0.9139]], requires_grad=True)\n",
      "new b tensor([  1.8680, -10.8649], requires_grad=True)\n",
      "Loss= tensor(1.2969, grad_fn=<DivBackward0>)\n",
      "i= 562\n",
      "w.grad= tensor([[-0.7785, -2.0686,  5.5958],\n",
      "        [-1.3940,  2.0456, -7.1942]])\n",
      "b.grad= tensor([-59.6834, 334.2354])\n",
      "new w tensor([[-0.4281,  0.8252,  0.7333],\n",
      "        [-0.2044,  0.8209,  0.9142]], requires_grad=True)\n",
      "new b tensor([  1.8710, -10.8816], requires_grad=True)\n",
      "Loss= tensor(1.2945, grad_fn=<DivBackward0>)\n",
      "i= 563\n",
      "w.grad= tensor([[-0.7736, -2.0594,  5.5712],\n",
      "        [-1.3986,  2.0366, -7.1705]])\n",
      "b.grad= tensor([-59.6239, 334.1064])\n",
      "new w tensor([[-0.4281,  0.8253,  0.7330],\n",
      "        [-0.2044,  0.8208,  0.9146]], requires_grad=True)\n",
      "new b tensor([  1.8739, -10.8983], requires_grad=True)\n",
      "Loss= tensor(1.2922, grad_fn=<DivBackward0>)\n",
      "i= 564\n",
      "w.grad= tensor([[-0.7679, -2.0495,  5.5471],\n",
      "        [-1.4031,  2.0275, -7.1469]])\n",
      "b.grad= tensor([-59.5645, 333.9775])\n",
      "new w tensor([[-0.4280,  0.8254,  0.7327],\n",
      "        [-0.2043,  0.8207,  0.9149]], requires_grad=True)\n",
      "new b tensor([  1.8769, -10.9150], requires_grad=True)\n",
      "Loss= tensor(1.2898, grad_fn=<DivBackward0>)\n",
      "i= 565\n",
      "w.grad= tensor([[-0.7630, -2.0405,  5.5227],\n",
      "        [-1.4079,  2.0180, -7.1237]])\n",
      "b.grad= tensor([-59.5052, 333.8485])\n",
      "new w tensor([[-0.4280,  0.8255,  0.7324],\n",
      "        [-0.2042,  0.8206,  0.9153]], requires_grad=True)\n",
      "new b tensor([  1.8799, -10.9317], requires_grad=True)\n",
      "Loss= tensor(1.2875, grad_fn=<DivBackward0>)\n",
      "i= 566\n",
      "w.grad= tensor([[-0.7573, -2.0305,  5.4989],\n",
      "        [-1.4118,  2.0098, -7.0999]])\n",
      "b.grad= tensor([-59.4459, 333.7196])\n",
      "new w tensor([[-0.4280,  0.8256,  0.7322],\n",
      "        [-0.2042,  0.8205,  0.9157]], requires_grad=True)\n",
      "new b tensor([  1.8829, -10.9484], requires_grad=True)\n",
      "Loss= tensor(1.2853, grad_fn=<DivBackward0>)\n",
      "i= 567\n",
      "w.grad= tensor([[-0.7526, -2.0218,  5.4745],\n",
      "        [-1.4165,  2.0004, -7.0768]])\n",
      "b.grad= tensor([-59.3868, 333.5906])\n",
      "new w tensor([[-0.4279,  0.8257,  0.7319],\n",
      "        [-0.2041,  0.8204,  0.9160]], requires_grad=True)\n",
      "new b tensor([  1.8858, -10.9650], requires_grad=True)\n",
      "Loss= tensor(1.2830, grad_fn=<DivBackward0>)\n",
      "i= 568\n",
      "w.grad= tensor([[-0.7473, -2.0123,  5.4507],\n",
      "        [-1.4203,  1.9923, -7.0532]])\n",
      "b.grad= tensor([-59.3278, 333.4616])\n",
      "new w tensor([[-0.4279,  0.8258,  0.7316],\n",
      "        [-0.2040,  0.8203,  0.9164]], requires_grad=True)\n",
      "new b tensor([  1.8888, -10.9817], requires_grad=True)\n",
      "Loss= tensor(1.2808, grad_fn=<DivBackward0>)\n",
      "i= 569\n",
      "w.grad= tensor([[-0.7420, -2.0029,  5.4270],\n",
      "        [-1.4254,  1.9825, -7.0306]])\n",
      "b.grad= tensor([-59.2689, 333.3326])\n",
      "new w tensor([[-0.4279,  0.8259,  0.7313],\n",
      "        [-0.2039,  0.8202,  0.9167]], requires_grad=True)\n",
      "new b tensor([  1.8918, -10.9984], requires_grad=True)\n",
      "Loss= tensor(1.2787, grad_fn=<DivBackward0>)\n",
      "i= 570\n",
      "w.grad= tensor([[-0.7375, -1.9944,  5.4029],\n",
      "        [-1.4285,  1.9753, -7.0068]])\n",
      "b.grad= tensor([-59.2100, 333.2035])\n",
      "new w tensor([[-0.4278,  0.8260,  0.7311],\n",
      "        [-0.2039,  0.8201,  0.9171]], requires_grad=True)\n",
      "new b tensor([  1.8947, -11.0150], requires_grad=True)\n",
      "Loss= tensor(1.2765, grad_fn=<DivBackward0>)\n",
      "i= 571\n",
      "w.grad= tensor([[-0.7322, -1.9850,  5.3795],\n",
      "        [-1.4338,  1.9654, -6.9845]])\n",
      "b.grad= tensor([-59.1513, 333.0745])\n",
      "new w tensor([[-0.4278,  0.8261,  0.7308],\n",
      "        [-0.2038,  0.8200,  0.9174]], requires_grad=True)\n",
      "new b tensor([  1.8977, -11.0317], requires_grad=True)\n",
      "Loss= tensor(1.2744, grad_fn=<DivBackward0>)\n",
      "i= 572\n",
      "w.grad= tensor([[-0.7267, -1.9754,  5.3563],\n",
      "        [-1.4379,  1.9570, -6.9617]])\n",
      "b.grad= tensor([-59.0926, 332.9454])\n",
      "new w tensor([[-0.4278,  0.8262,  0.7305],\n",
      "        [-0.2037,  0.8199,  0.9178]], requires_grad=True)\n",
      "new b tensor([  1.9006, -11.0483], requires_grad=True)\n",
      "Loss= tensor(1.2723, grad_fn=<DivBackward0>)\n",
      "i= 573\n",
      "w.grad= tensor([[-0.7223, -1.9671,  5.3326],\n",
      "        [-1.4422,  1.9482, -6.9390]])\n",
      "b.grad= tensor([-59.0340, 332.8163])\n",
      "new w tensor([[-0.4277,  0.8263,  0.7303],\n",
      "        [-0.2037,  0.8198,  0.9181]], requires_grad=True)\n",
      "new b tensor([  1.9036, -11.0650], requires_grad=True)\n",
      "Loss= tensor(1.2703, grad_fn=<DivBackward0>)\n",
      "i= 574\n",
      "w.grad= tensor([[-0.7166, -1.9572,  5.3098],\n",
      "        [-1.4461,  1.9400, -6.9162]])\n",
      "b.grad= tensor([-58.9756, 332.6872])\n",
      "new w tensor([[-0.4277,  0.8264,  0.7300],\n",
      "        [-0.2036,  0.8197,  0.9185]], requires_grad=True)\n",
      "new b tensor([  1.9065, -11.0816], requires_grad=True)\n",
      "Loss= tensor(1.2683, grad_fn=<DivBackward0>)\n",
      "i= 575\n",
      "w.grad= tensor([[-0.7120, -1.9487,  5.2864],\n",
      "        [-1.4506,  1.9309, -6.8940]])\n",
      "b.grad= tensor([-58.9172, 332.5580])\n",
      "new w tensor([[-0.4276,  0.8265,  0.7297],\n",
      "        [-0.2035,  0.8197,  0.9188]], requires_grad=True)\n",
      "new b tensor([  1.9095, -11.0982], requires_grad=True)\n",
      "Loss= tensor(1.2663, grad_fn=<DivBackward0>)\n",
      "i= 576\n",
      "w.grad= tensor([[-0.7067, -1.9392,  5.2636],\n",
      "        [-1.4546,  1.9228, -6.8714]])\n",
      "b.grad= tensor([-58.8589, 332.4289])\n",
      "new w tensor([[-0.4276,  0.8266,  0.7295],\n",
      "        [-0.2034,  0.8196,  0.9191]], requires_grad=True)\n",
      "new b tensor([  1.9124, -11.1149], requires_grad=True)\n",
      "Loss= tensor(1.2643, grad_fn=<DivBackward0>)\n",
      "i= 577\n",
      "w.grad= tensor([[-0.7019, -1.9306,  5.2406],\n",
      "        [-1.4591,  1.9137, -6.8494]])\n",
      "b.grad= tensor([-58.8006, 332.2997])\n",
      "new w tensor([[-0.4276,  0.8266,  0.7292],\n",
      "        [-0.2034,  0.8195,  0.9195]], requires_grad=True)\n",
      "new b tensor([  1.9154, -11.1315], requires_grad=True)\n",
      "Loss= tensor(1.2624, grad_fn=<DivBackward0>)\n",
      "i= 578\n",
      "w.grad= tensor([[-0.6967, -1.9214,  5.2180],\n",
      "        [-1.4626,  1.9061, -6.8268]])\n",
      "b.grad= tensor([-58.7425, 332.1705])\n",
      "new w tensor([[-0.4275,  0.8267,  0.7289],\n",
      "        [-0.2033,  0.8194,  0.9198]], requires_grad=True)\n",
      "new b tensor([  1.9183, -11.1481], requires_grad=True)\n",
      "Loss= tensor(1.2605, grad_fn=<DivBackward0>)\n",
      "i= 579\n",
      "w.grad= tensor([[-0.6923, -1.9131,  5.1949],\n",
      "        [-1.4668,  1.8976, -6.8048]])\n",
      "b.grad= tensor([-58.6845, 332.0413])\n",
      "new w tensor([[-0.4275,  0.8268,  0.7287],\n",
      "        [-0.2032,  0.8193,  0.9202]], requires_grad=True)\n",
      "new b tensor([  1.9212, -11.1647], requires_grad=True)\n",
      "Loss= tensor(1.2587, grad_fn=<DivBackward0>)\n",
      "i= 580\n",
      "w.grad= tensor([[-0.6876, -1.9046,  5.1722],\n",
      "        [-1.4715,  1.8887, -6.7832]])\n",
      "b.grad= tensor([-58.6265, 331.9120])\n",
      "new w tensor([[-0.4275,  0.8269,  0.7284],\n",
      "        [-0.2031,  0.8192,  0.9205]], requires_grad=True)\n",
      "new b tensor([  1.9242, -11.1813], requires_grad=True)\n",
      "Loss= tensor(1.2568, grad_fn=<DivBackward0>)\n",
      "i= 581\n",
      "w.grad= tensor([[-0.6824, -1.8954,  5.1499],\n",
      "        [-1.4751,  1.8807, -6.7610]])\n",
      "b.grad= tensor([-58.5687, 331.7827])\n",
      "new w tensor([[-0.4274,  0.8270,  0.7282],\n",
      "        [-0.2031,  0.8191,  0.9208]], requires_grad=True)\n",
      "new b tensor([  1.9271, -11.1979], requires_grad=True)\n",
      "Loss= tensor(1.2550, grad_fn=<DivBackward0>)\n",
      "i= 582\n",
      "w.grad= tensor([[-0.6773, -1.8864,  5.1277],\n",
      "        [-1.4789,  1.8728, -6.7390]])\n",
      "b.grad= tensor([-58.5109, 331.6534])\n",
      "new w tensor([[-0.4274,  0.8271,  0.7279],\n",
      "        [-0.2030,  0.8190,  0.9212]], requires_grad=True)\n",
      "new b tensor([  1.9300, -11.2145], requires_grad=True)\n",
      "Loss= tensor(1.2532, grad_fn=<DivBackward0>)\n",
      "i= 583\n",
      "w.grad= tensor([[-0.6729, -1.8781,  5.1051],\n",
      "        [-1.4833,  1.8640, -6.7177]])\n",
      "b.grad= tensor([-58.4532, 331.5241])\n",
      "new w tensor([[-0.4274,  0.8272,  0.7277],\n",
      "        [-0.2029,  0.8189,  0.9215]], requires_grad=True)\n",
      "new b tensor([  1.9330, -11.2310], requires_grad=True)\n",
      "Loss= tensor(1.2515, grad_fn=<DivBackward0>)\n",
      "i= 584\n",
      "w.grad= tensor([[-0.6680, -1.8693,  5.0831],\n",
      "        [-1.4875,  1.8556, -6.6963]])\n",
      "b.grad= tensor([-58.3956, 331.3947])\n",
      "new w tensor([[-0.4273,  0.8273,  0.7274],\n",
      "        [-0.2028,  0.8188,  0.9219]], requires_grad=True)\n",
      "new b tensor([  1.9359, -11.2476], requires_grad=True)\n",
      "Loss= tensor(1.2498, grad_fn=<DivBackward0>)\n",
      "i= 585\n",
      "w.grad= tensor([[-0.6634, -1.8609,  5.0609],\n",
      "        [-1.4911,  1.8480, -6.6744]])\n",
      "b.grad= tensor([-58.3381, 331.2654])\n",
      "new w tensor([[-0.4273,  0.8274,  0.7272],\n",
      "        [-0.2028,  0.8187,  0.9222]], requires_grad=True)\n",
      "new b tensor([  1.9388, -11.2642], requires_grad=True)\n",
      "Loss= tensor(1.2481, grad_fn=<DivBackward0>)\n",
      "i= 586\n",
      "w.grad= tensor([[-0.6590, -1.8529,  5.0386],\n",
      "        [-1.4953,  1.8395, -6.6532]])\n",
      "b.grad= tensor([-58.2807, 331.1360])\n",
      "new w tensor([[-0.4273,  0.8275,  0.7269],\n",
      "        [-0.2027,  0.8186,  0.9225]], requires_grad=True)\n",
      "new b tensor([  1.9417, -11.2807], requires_grad=True)\n",
      "Loss= tensor(1.2464, grad_fn=<DivBackward0>)\n",
      "i= 587\n",
      "w.grad= tensor([[-0.6539, -1.8438,  5.0170],\n",
      "        [-1.4992,  1.8313, -6.6320]])\n",
      "b.grad= tensor([-58.2233, 331.0066])\n",
      "new w tensor([[-0.4272,  0.8276,  0.7266],\n",
      "        [-0.2026,  0.8185,  0.9229]], requires_grad=True)\n",
      "new b tensor([  1.9446, -11.2973], requires_grad=True)\n",
      "Loss= tensor(1.2448, grad_fn=<DivBackward0>)\n",
      "i= 588\n",
      "w.grad= tensor([[-0.6493, -1.8355,  4.9951],\n",
      "        [-1.5024,  1.8242, -6.6103]])\n",
      "b.grad= tensor([-58.1660, 330.8771])\n",
      "new w tensor([[-0.4272,  0.8277,  0.7264],\n",
      "        [-0.2025,  0.8184,  0.9232]], requires_grad=True)\n",
      "new b tensor([  1.9475, -11.3138], requires_grad=True)\n",
      "Loss= tensor(1.2432, grad_fn=<DivBackward0>)\n",
      "i= 589\n",
      "w.grad= tensor([[-0.6443, -1.8266,  4.9737],\n",
      "        [-1.5070,  1.8151, -6.5897]])\n",
      "b.grad= tensor([-58.1089, 330.7477])\n",
      "new w tensor([[-0.4272,  0.8278,  0.7262],\n",
      "        [-0.2025,  0.8183,  0.9235]], requires_grad=True)\n",
      "new b tensor([  1.9504, -11.3304], requires_grad=True)\n",
      "Loss= tensor(1.2416, grad_fn=<DivBackward0>)\n",
      "i= 590\n",
      "w.grad= tensor([[-0.6401, -1.8189,  4.9517],\n",
      "        [-1.5107,  1.8074, -6.5686]])\n",
      "b.grad= tensor([-58.0518, 330.6182])\n",
      "new w tensor([[-0.4271,  0.8279,  0.7259],\n",
      "        [-0.2024,  0.8183,  0.9238]], requires_grad=True)\n",
      "new b tensor([  1.9533, -11.3469], requires_grad=True)\n",
      "Loss= tensor(1.2400, grad_fn=<DivBackward0>)\n",
      "i= 591\n",
      "w.grad= tensor([[-0.6351, -1.8101,  4.9305],\n",
      "        [-1.5142,  1.7998, -6.5475]])\n",
      "b.grad= tensor([-57.9947, 330.4886])\n",
      "new w tensor([[-0.4271,  0.8280,  0.7257],\n",
      "        [-0.2023,  0.8182,  0.9242]], requires_grad=True)\n",
      "new b tensor([  1.9562, -11.3634], requires_grad=True)\n",
      "Loss= tensor(1.2385, grad_fn=<DivBackward0>)\n",
      "i= 592\n",
      "w.grad= tensor([[-0.6309, -1.8021,  4.9089],\n",
      "        [-1.5184,  1.7913, -6.5269]])\n",
      "b.grad= tensor([-57.9378, 330.3591])\n",
      "new w tensor([[-0.4271,  0.8280,  0.7254],\n",
      "        [-0.2022,  0.8181,  0.9245]], requires_grad=True)\n",
      "new b tensor([  1.9591, -11.3799], requires_grad=True)\n",
      "Loss= tensor(1.2370, grad_fn=<DivBackward0>)\n",
      "i= 593\n",
      "w.grad= tensor([[-0.6261, -1.7935,  4.8878],\n",
      "        [-1.5218,  1.7841, -6.5058]])\n",
      "b.grad= tensor([-57.8810, 330.2295])\n",
      "new w tensor([[-0.4270,  0.8281,  0.7252],\n",
      "        [-0.2022,  0.8180,  0.9248]], requires_grad=True)\n",
      "new b tensor([  1.9620, -11.3964], requires_grad=True)\n",
      "Loss= tensor(1.2355, grad_fn=<DivBackward0>)\n",
      "i= 594\n",
      "w.grad= tensor([[-0.6216, -1.7854,  4.8666],\n",
      "        [-1.5258,  1.7760, -6.4854]])\n",
      "b.grad= tensor([-57.8242, 330.0999])\n",
      "new w tensor([[-0.4270,  0.8282,  0.7249],\n",
      "        [-0.2021,  0.8179,  0.9251]], requires_grad=True)\n",
      "new b tensor([  1.9649, -11.4129], requires_grad=True)\n",
      "Loss= tensor(1.2340, grad_fn=<DivBackward0>)\n",
      "i= 595\n",
      "w.grad= tensor([[-0.6168, -1.7767,  4.8457],\n",
      "        [-1.5293,  1.7685, -6.4647]])\n",
      "b.grad= tensor([-57.7675, 329.9703])\n",
      "new w tensor([[-0.4270,  0.8283,  0.7247],\n",
      "        [-0.2020,  0.8178,  0.9255]], requires_grad=True)\n",
      "new b tensor([  1.9678, -11.4294], requires_grad=True)\n",
      "Loss= tensor(1.2326, grad_fn=<DivBackward0>)\n",
      "i= 596\n",
      "w.grad= tensor([[-0.6127, -1.7692,  4.8244],\n",
      "        [-1.5333,  1.7602, -6.4445]])\n",
      "b.grad= tensor([-57.7109, 329.8406])\n",
      "new w tensor([[-0.4270,  0.8284,  0.7244],\n",
      "        [-0.2019,  0.8177,  0.9258]], requires_grad=True)\n",
      "new b tensor([  1.9707, -11.4459], requires_grad=True)\n",
      "Loss= tensor(1.2312, grad_fn=<DivBackward0>)\n",
      "i= 597\n",
      "w.grad= tensor([[-0.6084, -1.7612,  4.8034],\n",
      "        [-1.5368,  1.7527, -6.4240]])\n",
      "b.grad= tensor([-57.6544, 329.7109])\n",
      "new w tensor([[-0.4269,  0.8285,  0.7242],\n",
      "        [-0.2019,  0.8176,  0.9261]], requires_grad=True)\n",
      "new b tensor([  1.9736, -11.4624], requires_grad=True)\n",
      "Loss= tensor(1.2298, grad_fn=<DivBackward0>)\n",
      "i= 598\n",
      "w.grad= tensor([[-0.6036, -1.7527,  4.7828],\n",
      "        [-1.5401,  1.7455, -6.4034]])\n",
      "b.grad= tensor([-57.5979, 329.5812])\n",
      "new w tensor([[-0.4269,  0.8286,  0.7240],\n",
      "        [-0.2018,  0.8175,  0.9264]], requires_grad=True)\n",
      "new b tensor([  1.9764, -11.4789], requires_grad=True)\n",
      "Loss= tensor(1.2285, grad_fn=<DivBackward0>)\n",
      "i= 599\n",
      "w.grad= tensor([[-0.5995, -1.7451,  4.7618],\n",
      "        [-1.5436,  1.7379, -6.3831]])\n",
      "b.grad= tensor([-57.5415, 329.4514])\n",
      "new w tensor([[-0.4269,  0.8287,  0.7237],\n",
      "        [-0.2017,  0.8175,  0.9268]], requires_grad=True)\n",
      "new b tensor([  1.9793, -11.4954], requires_grad=True)\n",
      "Loss= tensor(1.2271, grad_fn=<DivBackward0>)\n",
      "i= 600\n",
      "w.grad= tensor([[-0.5950, -1.7370,  4.7413],\n",
      "        [-1.5475,  1.7300, -6.3633]])\n",
      "b.grad= tensor([-57.4852, 329.3216])\n",
      "new w tensor([[-0.4268,  0.8287,  0.7235],\n",
      "        [-0.2016,  0.8174,  0.9271]], requires_grad=True)\n",
      "new b tensor([  1.9822, -11.5118], requires_grad=True)\n",
      "Loss= tensor(1.2258, grad_fn=<DivBackward0>)\n",
      "i= 601\n",
      "w.grad= tensor([[-0.5901, -1.7284,  4.7211],\n",
      "        [-1.5508,  1.7228, -6.3430]])\n",
      "b.grad= tensor([-57.4290, 329.1918])\n",
      "new w tensor([[-0.4268,  0.8288,  0.7233],\n",
      "        [-0.2016,  0.8173,  0.9274]], requires_grad=True)\n",
      "new b tensor([  1.9851, -11.5283], requires_grad=True)\n",
      "Loss= tensor(1.2245, grad_fn=<DivBackward0>)\n",
      "i= 602\n",
      "w.grad= tensor([[-0.5864, -1.7213,  4.7002],\n",
      "        [-1.5557,  1.7138, -6.3241]])\n",
      "b.grad= tensor([-57.3729, 329.0620])\n",
      "new w tensor([[-0.4268,  0.8289,  0.7230],\n",
      "        [-0.2015,  0.8172,  0.9277]], requires_grad=True)\n",
      "new b tensor([  1.9879, -11.5448], requires_grad=True)\n",
      "Loss= tensor(1.2233, grad_fn=<DivBackward0>)\n",
      "i= 603\n",
      "w.grad= tensor([[-0.5818, -1.7132,  4.6800],\n",
      "        [-1.5576,  1.7080, -6.3031]])\n",
      "b.grad= tensor([-57.3168, 328.9321])\n",
      "new w tensor([[-0.4267,  0.8290,  0.7228],\n",
      "        [-0.2014,  0.8171,  0.9280]], requires_grad=True)\n",
      "new b tensor([  1.9908, -11.5612], requires_grad=True)\n",
      "Loss= tensor(1.2220, grad_fn=<DivBackward0>)\n",
      "i= 604\n",
      "w.grad= tensor([[-0.5775, -1.7051,  4.6598],\n",
      "        [-1.5623,  1.6993, -6.2841]])\n",
      "b.grad= tensor([-57.2608, 328.8022])\n",
      "new w tensor([[-0.4267,  0.8291,  0.7225],\n",
      "        [-0.2013,  0.8170,  0.9283]], requires_grad=True)\n",
      "new b tensor([  1.9937, -11.5776], requires_grad=True)\n",
      "Loss= tensor(1.2208, grad_fn=<DivBackward0>)\n",
      "i= 605\n",
      "w.grad= tensor([[-0.5731, -1.6974,  4.6397],\n",
      "        [-1.5654,  1.6923, -6.2643]])\n",
      "b.grad= tensor([-57.2049, 328.6722])\n",
      "new w tensor([[-0.4267,  0.8292,  0.7223],\n",
      "        [-0.2012,  0.8169,  0.9286]], requires_grad=True)\n",
      "new b tensor([  1.9965, -11.5941], requires_grad=True)\n",
      "Loss= tensor(1.2197, grad_fn=<DivBackward0>)\n",
      "i= 606\n",
      "w.grad= tensor([[-0.5692, -1.6900,  4.6194],\n",
      "        [-1.5689,  1.6850, -6.2446]])\n",
      "b.grad= tensor([-57.1491, 328.5422])\n",
      "new w tensor([[-0.4267,  0.8293,  0.7221],\n",
      "        [-0.2012,  0.8169,  0.9290]], requires_grad=True)\n",
      "new b tensor([  1.9994, -11.6105], requires_grad=True)\n",
      "Loss= tensor(1.2185, grad_fn=<DivBackward0>)\n",
      "i= 607\n",
      "w.grad= tensor([[-0.5642, -1.6814,  4.6000],\n",
      "        [-1.5724,  1.6775, -6.2252]])\n",
      "b.grad= tensor([-57.0933, 328.4122])\n",
      "new w tensor([[-0.4266,  0.8293,  0.7219],\n",
      "        [-0.2011,  0.8168,  0.9293]], requires_grad=True)\n",
      "new b tensor([  2.0022, -11.6269], requires_grad=True)\n",
      "Loss= tensor(1.2174, grad_fn=<DivBackward0>)\n",
      "i= 608\n",
      "w.grad= tensor([[-0.5605, -1.6743,  4.5797],\n",
      "        [-1.5755,  1.6705, -6.2056]])\n",
      "b.grad= tensor([-57.0376, 328.2822])\n",
      "new w tensor([[-0.4266,  0.8294,  0.7216],\n",
      "        [-0.2010,  0.8167,  0.9296]], requires_grad=True)\n",
      "new b tensor([  2.0051, -11.6433], requires_grad=True)\n",
      "Loss= tensor(1.2162, grad_fn=<DivBackward0>)\n",
      "i= 609\n",
      "w.grad= tensor([[-0.5559, -1.6661,  4.5602],\n",
      "        [-1.5790,  1.6632, -6.1863]])\n",
      "b.grad= tensor([-56.9820, 328.1521])\n",
      "new w tensor([[-0.4266,  0.8295,  0.7214],\n",
      "        [-0.2009,  0.8166,  0.9299]], requires_grad=True)\n",
      "new b tensor([  2.0079, -11.6597], requires_grad=True)\n",
      "Loss= tensor(1.2151, grad_fn=<DivBackward0>)\n",
      "i= 610\n",
      "w.grad= tensor([[-0.5521, -1.6590,  4.5402],\n",
      "        [-1.5823,  1.6561, -6.1670]])\n",
      "b.grad= tensor([-56.9265, 328.0220])\n",
      "new w tensor([[-0.4265,  0.8296,  0.7212],\n",
      "        [-0.2008,  0.8165,  0.9302]], requires_grad=True)\n",
      "new b tensor([  2.0108, -11.6761], requires_grad=True)\n",
      "Loss= tensor(1.2141, grad_fn=<DivBackward0>)\n",
      "i= 611\n",
      "w.grad= tensor([[-0.5477, -1.6510,  4.5208],\n",
      "        [-1.5865,  1.6480, -6.1484]])\n",
      "b.grad= tensor([-56.8710, 327.8919])\n",
      "new w tensor([[-0.4265,  0.8297,  0.7209],\n",
      "        [-0.2008,  0.8164,  0.9305]], requires_grad=True)\n",
      "new b tensor([  2.0136, -11.6925], requires_grad=True)\n",
      "Loss= tensor(1.2130, grad_fn=<DivBackward0>)\n",
      "i= 612\n",
      "w.grad= tensor([[-0.5436, -1.6436,  4.5012],\n",
      "        [-1.5886,  1.6422, -6.1286]])\n",
      "b.grad= tensor([-56.8156, 327.7617])\n",
      "new w tensor([[-0.4265,  0.8298,  0.7207],\n",
      "        [-0.2007,  0.8164,  0.9308]], requires_grad=True)\n",
      "new b tensor([  2.0165, -11.7089], requires_grad=True)\n",
      "Loss= tensor(1.2120, grad_fn=<DivBackward0>)\n",
      "i= 613\n",
      "w.grad= tensor([[-0.5398, -1.6363,  4.4816],\n",
      "        [-1.5928,  1.6342, -6.1101]])\n",
      "b.grad= tensor([-56.7603, 327.6315])\n",
      "new w tensor([[-0.4265,  0.8298,  0.7205],\n",
      "        [-0.2006,  0.8163,  0.9311]], requires_grad=True)\n",
      "new b tensor([  2.0193, -11.7253], requires_grad=True)\n",
      "Loss= tensor(1.2110, grad_fn=<DivBackward0>)\n",
      "i= 614\n",
      "w.grad= tensor([[-0.5350, -1.6282,  4.4627],\n",
      "        [-1.5960,  1.6272, -6.0911]])\n",
      "b.grad= tensor([-56.7051, 327.5013])\n",
      "new w tensor([[-0.4264,  0.8299,  0.7203],\n",
      "        [-0.2005,  0.8162,  0.9314]], requires_grad=True)\n",
      "new b tensor([  2.0221, -11.7417], requires_grad=True)\n",
      "Loss= tensor(1.2100, grad_fn=<DivBackward0>)\n",
      "i= 615\n",
      "w.grad= tensor([[-0.5310, -1.6207,  4.4435],\n",
      "        [-1.5993,  1.6200, -6.0724]])\n",
      "b.grad= tensor([-56.6499, 327.3710])\n",
      "new w tensor([[-0.4264,  0.8300,  0.7201],\n",
      "        [-0.2004,  0.8161,  0.9317]], requires_grad=True)\n",
      "new b tensor([  2.0250, -11.7581], requires_grad=True)\n",
      "Loss= tensor(1.2090, grad_fn=<DivBackward0>)\n",
      "i= 616\n",
      "w.grad= tensor([[-0.5270, -1.6134,  4.4242],\n",
      "        [-1.6031,  1.6122, -6.0541]])\n",
      "b.grad= tensor([-56.5948, 327.2407])\n",
      "new w tensor([[-0.4264,  0.8301,  0.7198],\n",
      "        [-0.2004,  0.8160,  0.9320]], requires_grad=True)\n",
      "new b tensor([  2.0278, -11.7744], requires_grad=True)\n",
      "Loss= tensor(1.2081, grad_fn=<DivBackward0>)\n",
      "i= 617\n",
      "w.grad= tensor([[-0.5233, -1.6064,  4.4049],\n",
      "        [-1.6053,  1.6064, -6.0347]])\n",
      "b.grad= tensor([-56.5398, 327.1103])\n",
      "new w tensor([[-0.4264,  0.8302,  0.7196],\n",
      "        [-0.2003,  0.8160,  0.9323]], requires_grad=True)\n",
      "new b tensor([  2.0306, -11.7908], requires_grad=True)\n",
      "Loss= tensor(1.2072, grad_fn=<DivBackward0>)\n",
      "i= 618\n",
      "w.grad= tensor([[-0.5195, -1.5993,  4.3858],\n",
      "        [-1.6094,  1.5985, -6.0167]])\n",
      "b.grad= tensor([-56.4848, 326.9799])\n",
      "new w tensor([[-0.4263,  0.8302,  0.7194],\n",
      "        [-0.2002,  0.8159,  0.9326]], requires_grad=True)\n",
      "new b tensor([  2.0335, -11.8071], requires_grad=True)\n",
      "Loss= tensor(1.2063, grad_fn=<DivBackward0>)\n",
      "i= 619\n",
      "w.grad= tensor([[-0.5144, -1.5908,  4.3676],\n",
      "        [-1.6125,  1.5917, -5.9981]])\n",
      "b.grad= tensor([-56.4299, 326.8495])\n",
      "new w tensor([[-0.4263,  0.8303,  0.7192],\n",
      "        [-0.2001,  0.8158,  0.9329]], requires_grad=True)\n",
      "new b tensor([  2.0363, -11.8235], requires_grad=True)\n",
      "Loss= tensor(1.2054, grad_fn=<DivBackward0>)\n",
      "i= 620\n",
      "w.grad= tensor([[-0.5118, -1.5850,  4.3479],\n",
      "        [-1.6151,  1.5854, -5.9794]])\n",
      "b.grad= tensor([-56.3751, 326.7190])\n",
      "new w tensor([[-0.4263,  0.8304,  0.7190],\n",
      "        [-0.2000,  0.8157,  0.9332]], requires_grad=True)\n",
      "new b tensor([  2.0391, -11.8398], requires_grad=True)\n",
      "Loss= tensor(1.2045, grad_fn=<DivBackward0>)\n",
      "i= 621\n",
      "w.grad= tensor([[-0.5069, -1.5768,  4.3297],\n",
      "        [-1.6186,  1.5781, -5.9613]])\n",
      "b.grad= tensor([-56.3204, 326.5885])\n",
      "new w tensor([[-0.4263,  0.8305,  0.7187],\n",
      "        [-0.2000,  0.8156,  0.9335]], requires_grad=True)\n",
      "new b tensor([  2.0419, -11.8561], requires_grad=True)\n",
      "Loss= tensor(1.2037, grad_fn=<DivBackward0>)\n",
      "i= 622\n",
      "w.grad= tensor([[-0.5034, -1.5699,  4.3109],\n",
      "        [-1.6215,  1.5716, -5.9429]])\n",
      "b.grad= tensor([-56.2657, 326.4580])\n",
      "new w tensor([[-0.4262,  0.8306,  0.7185],\n",
      "        [-0.1999,  0.8156,  0.9338]], requires_grad=True)\n",
      "new b tensor([  2.0447, -11.8725], requires_grad=True)\n",
      "Loss= tensor(1.2029, grad_fn=<DivBackward0>)\n",
      "i= 623\n",
      "w.grad= tensor([[-0.4991, -1.5624,  4.2925],\n",
      "        [-1.6250,  1.5645, -5.9249]])\n",
      "b.grad= tensor([-56.2111, 326.3274])\n",
      "new w tensor([[-0.4262,  0.8306,  0.7183],\n",
      "        [-0.1998,  0.8155,  0.9341]], requires_grad=True)\n",
      "new b tensor([  2.0475, -11.8888], requires_grad=True)\n",
      "Loss= tensor(1.2021, grad_fn=<DivBackward0>)\n",
      "i= 624\n",
      "w.grad= tensor([[-0.4957, -1.5557,  4.2738],\n",
      "        [-1.6282,  1.5575, -5.9070]])\n",
      "b.grad= tensor([-56.1566, 326.1968])\n",
      "new w tensor([[-0.4262,  0.8307,  0.7181],\n",
      "        [-0.1997,  0.8154,  0.9344]], requires_grad=True)\n",
      "new b tensor([  2.0503, -11.9051], requires_grad=True)\n",
      "Loss= tensor(1.2013, grad_fn=<DivBackward0>)\n",
      "i= 625\n",
      "w.grad= tensor([[-0.4913, -1.5481,  4.2557],\n",
      "        [-1.6313,  1.5507, -5.8891]])\n",
      "b.grad= tensor([-56.1021, 326.0662])\n",
      "new w tensor([[-0.4262,  0.8308,  0.7179],\n",
      "        [-0.1996,  0.8153,  0.9347]], requires_grad=True)\n",
      "new b tensor([  2.0532, -11.9214], requires_grad=True)\n",
      "Loss= tensor(1.2006, grad_fn=<DivBackward0>)\n",
      "i= 626\n",
      "w.grad= tensor([[-0.4877, -1.5412,  4.2373],\n",
      "        [-1.6343,  1.5441, -5.8711]])\n",
      "b.grad= tensor([-56.0477, 325.9355])\n",
      "new w tensor([[-0.4261,  0.8309,  0.7177],\n",
      "        [-0.1996,  0.8153,  0.9350]], requires_grad=True)\n",
      "new b tensor([  2.0560, -11.9377], requires_grad=True)\n",
      "Loss= tensor(1.1998, grad_fn=<DivBackward0>)\n",
      "i= 627\n",
      "w.grad= tensor([[-0.4836, -1.5339,  4.2192],\n",
      "        [-1.6374,  1.5373, -5.8533]])\n",
      "b.grad= tensor([-55.9934, 325.8047])\n",
      "new w tensor([[-0.4261,  0.8309,  0.7175],\n",
      "        [-0.1995,  0.8152,  0.9353]], requires_grad=True)\n",
      "new b tensor([  2.0588, -11.9540], requires_grad=True)\n",
      "Loss= tensor(1.1991, grad_fn=<DivBackward0>)\n",
      "i= 628\n",
      "w.grad= tensor([[-0.4803, -1.5274,  4.2008],\n",
      "        [-1.6403,  1.5309, -5.8354]])\n",
      "b.grad= tensor([-55.9391, 325.6740])\n",
      "new w tensor([[-0.4261,  0.8310,  0.7172],\n",
      "        [-0.1994,  0.8151,  0.9356]], requires_grad=True)\n",
      "new b tensor([  2.0616, -11.9703], requires_grad=True)\n",
      "Loss= tensor(1.1984, grad_fn=<DivBackward0>)\n",
      "i= 629\n",
      "w.grad= tensor([[-0.4762, -1.5202,  4.1829],\n",
      "        [-1.6436,  1.5240, -5.8180]])\n",
      "b.grad= tensor([-55.8849, 325.5431])\n",
      "new w tensor([[-0.4261,  0.8311,  0.7170],\n",
      "        [-0.1993,  0.8150,  0.9359]], requires_grad=True)\n",
      "new b tensor([  2.0643, -11.9865], requires_grad=True)\n",
      "Loss= tensor(1.1977, grad_fn=<DivBackward0>)\n",
      "i= 630\n",
      "w.grad= tensor([[-0.4726, -1.5134,  4.1648],\n",
      "        [-1.6464,  1.5175, -5.8003]])\n",
      "b.grad= tensor([-55.8308, 325.4123])\n",
      "new w tensor([[-0.4260,  0.8312,  0.7168],\n",
      "        [-0.1992,  0.8149,  0.9362]], requires_grad=True)\n",
      "new b tensor([  2.0671, -12.0028], requires_grad=True)\n",
      "Loss= tensor(1.1971, grad_fn=<DivBackward0>)\n",
      "i= 631\n",
      "w.grad= tensor([[-0.4688, -1.5065,  4.1469],\n",
      "        [-1.6495,  1.5106, -5.7829]])\n",
      "b.grad= tensor([-55.7767, 325.2814])\n",
      "new w tensor([[-0.4260,  0.8313,  0.7166],\n",
      "        [-0.1991,  0.8149,  0.9365]], requires_grad=True)\n",
      "new b tensor([  2.0699, -12.0191], requires_grad=True)\n",
      "Loss= tensor(1.1964, grad_fn=<DivBackward0>)\n",
      "i= 632\n",
      "w.grad= tensor([[-0.4649, -1.4995,  4.1292],\n",
      "        [-1.6521,  1.5048, -5.7652]])\n",
      "b.grad= tensor([-55.7227, 325.1504])\n",
      "new w tensor([[-0.4260,  0.8313,  0.7164],\n",
      "        [-0.1991,  0.8148,  0.9367]], requires_grad=True)\n",
      "new b tensor([  2.0727, -12.0353], requires_grad=True)\n",
      "Loss= tensor(1.1958, grad_fn=<DivBackward0>)\n",
      "i= 633\n",
      "w.grad= tensor([[-0.4610, -1.4924,  4.1116],\n",
      "        [-1.6554,  1.4976, -5.7481]])\n",
      "b.grad= tensor([-55.6687, 325.0194])\n",
      "new w tensor([[-0.4260,  0.8314,  0.7162],\n",
      "        [-0.1990,  0.8147,  0.9370]], requires_grad=True)\n",
      "new b tensor([  2.0755, -12.0516], requires_grad=True)\n",
      "Loss= tensor(1.1952, grad_fn=<DivBackward0>)\n",
      "i= 634\n",
      "w.grad= tensor([[-0.4578, -1.4862,  4.0936],\n",
      "        [-1.6579,  1.4916, -5.7306]])\n",
      "b.grad= tensor([-55.6149, 324.8884])\n",
      "new w tensor([[-0.4259,  0.8315,  0.7160],\n",
      "        [-0.1989,  0.8146,  0.9373]], requires_grad=True)\n",
      "new b tensor([  2.0783, -12.0678], requires_grad=True)\n",
      "Loss= tensor(1.1946, grad_fn=<DivBackward0>)\n",
      "i= 635\n",
      "w.grad= tensor([[-0.4532, -1.4783,  4.0767],\n",
      "        [-1.6610,  1.4851, -5.7134]])\n",
      "b.grad= tensor([-55.5611, 324.7574])\n",
      "new w tensor([[-0.4259,  0.8316,  0.7158],\n",
      "        [-0.1988,  0.8146,  0.9376]], requires_grad=True)\n",
      "new b tensor([  2.0811, -12.0841], requires_grad=True)\n",
      "Loss= tensor(1.1941, grad_fn=<DivBackward0>)\n",
      "i= 636\n",
      "w.grad= tensor([[-0.4505, -1.4726,  4.0586],\n",
      "        [-1.6638,  1.4788, -5.6962]])\n",
      "b.grad= tensor([-55.5073, 324.6263])\n",
      "new w tensor([[-0.4259,  0.8316,  0.7156],\n",
      "        [-0.1987,  0.8145,  0.9379]], requires_grad=True)\n",
      "new b tensor([  2.0838, -12.1003], requires_grad=True)\n",
      "Loss= tensor(1.1935, grad_fn=<DivBackward0>)\n",
      "i= 637\n",
      "w.grad= tensor([[-0.4466, -1.4655,  4.0414],\n",
      "        [-1.6670,  1.4721, -5.6794]])\n",
      "b.grad= tensor([-55.4536, 324.4951])\n",
      "new w tensor([[-0.4259,  0.8317,  0.7154],\n",
      "        [-0.1986,  0.8144,  0.9382]], requires_grad=True)\n",
      "new b tensor([  2.0866, -12.1165], requires_grad=True)\n",
      "Loss= tensor(1.1930, grad_fn=<DivBackward0>)\n",
      "i= 638\n",
      "w.grad= tensor([[-0.4434, -1.4594,  4.0238],\n",
      "        [-1.6701,  1.4653, -5.6626]])\n",
      "b.grad= tensor([-55.4000, 324.3639])\n",
      "new w tensor([[-0.4259,  0.8318,  0.7152],\n",
      "        [-0.1986,  0.8143,  0.9385]], requires_grad=True)\n",
      "new b tensor([  2.0894, -12.1327], requires_grad=True)\n",
      "Loss= tensor(1.1925, grad_fn=<DivBackward0>)\n",
      "i= 639\n",
      "w.grad= tensor([[-0.4393, -1.4522,  4.0068],\n",
      "        [-1.6725,  1.4594, -5.6454]])\n",
      "b.grad= tensor([-55.3465, 324.2327])\n",
      "new w tensor([[-0.4258,  0.8318,  0.7150],\n",
      "        [-0.1985,  0.8143,  0.9387]], requires_grad=True)\n",
      "new b tensor([  2.0921, -12.1489], requires_grad=True)\n",
      "Loss= tensor(1.1920, grad_fn=<DivBackward0>)\n",
      "i= 640\n",
      "w.grad= tensor([[-0.4355, -1.4454,  3.9897],\n",
      "        [-1.6748,  1.4538, -5.6283]])\n",
      "b.grad= tensor([-55.2930, 324.1014])\n",
      "new w tensor([[-0.4258,  0.8319,  0.7148],\n",
      "        [-0.1984,  0.8142,  0.9390]], requires_grad=True)\n",
      "new b tensor([  2.0949, -12.1651], requires_grad=True)\n",
      "Loss= tensor(1.1915, grad_fn=<DivBackward0>)\n",
      "i= 641\n",
      "w.grad= tensor([[-0.4319, -1.4388,  3.9727],\n",
      "        [-1.6786,  1.4463, -5.6122]])\n",
      "b.grad= tensor([-55.2396, 323.9701])\n",
      "new w tensor([[-0.4258,  0.8320,  0.7146],\n",
      "        [-0.1983,  0.8141,  0.9393]], requires_grad=True)\n",
      "new b tensor([  2.0977, -12.1813], requires_grad=True)\n",
      "Loss= tensor(1.1910, grad_fn=<DivBackward0>)\n",
      "i= 642\n",
      "w.grad= tensor([[-0.4286, -1.4325,  3.9555],\n",
      "        [-1.6810,  1.4404, -5.5953]])\n",
      "b.grad= tensor([-55.1862, 323.8387])\n",
      "new w tensor([[-0.4258,  0.8321,  0.7144],\n",
      "        [-0.1982,  0.8141,  0.9396]], requires_grad=True)\n",
      "new b tensor([  2.1004, -12.1975], requires_grad=True)\n",
      "Loss= tensor(1.1906, grad_fn=<DivBackward0>)\n",
      "i= 643\n",
      "w.grad= tensor([[-0.4246, -1.4254,  3.9389],\n",
      "        [-1.6832,  1.4348, -5.5783]])\n",
      "b.grad= tensor([-55.1329, 323.7073])\n",
      "new w tensor([[-0.4257,  0.8321,  0.7142],\n",
      "        [-0.1981,  0.8140,  0.9399]], requires_grad=True)\n",
      "new b tensor([  2.1032, -12.2137], requires_grad=True)\n",
      "Loss= tensor(1.1902, grad_fn=<DivBackward0>)\n",
      "i= 644\n",
      "w.grad= tensor([[-0.4215, -1.4194,  3.9218],\n",
      "        [-1.6871,  1.4274, -5.5626]])\n",
      "b.grad= tensor([-55.0796, 323.5758])\n",
      "new w tensor([[-0.4257,  0.8322,  0.7140],\n",
      "        [-0.1981,  0.8139,  0.9401]], requires_grad=True)\n",
      "new b tensor([  2.1059, -12.2299], requires_grad=True)\n",
      "Loss= tensor(1.1898, grad_fn=<DivBackward0>)\n",
      "i= 645\n",
      "w.grad= tensor([[-0.4175, -1.4124,  3.9053],\n",
      "        [-1.6891,  1.4221, -5.5456]])\n",
      "b.grad= tensor([-55.0265, 323.4443])\n",
      "new w tensor([[-0.4257,  0.8323,  0.7138],\n",
      "        [-0.1980,  0.8138,  0.9404]], requires_grad=True)\n",
      "new b tensor([  2.1087, -12.2461], requires_grad=True)\n",
      "Loss= tensor(1.1894, grad_fn=<DivBackward0>)\n",
      "i= 646\n",
      "w.grad= tensor([[-0.4141, -1.4060,  3.8886],\n",
      "        [-1.6915,  1.4162, -5.5290]])\n",
      "b.grad= tensor([-54.9733, 323.3128])\n",
      "new w tensor([[-0.4257,  0.8323,  0.7136],\n",
      "        [-0.1979,  0.8138,  0.9407]], requires_grad=True)\n",
      "new b tensor([  2.1114, -12.2622], requires_grad=True)\n",
      "Loss= tensor(1.1890, grad_fn=<DivBackward0>)\n",
      "i= 647\n",
      "w.grad= tensor([[-0.4112, -1.4002,  3.8716],\n",
      "        [-1.6944,  1.4100, -5.5129]])\n",
      "b.grad= tensor([-54.9203, 323.1812])\n",
      "new w tensor([[-0.4257,  0.8324,  0.7134],\n",
      "        [-0.1978,  0.8137,  0.9410]], requires_grad=True)\n",
      "new b tensor([  2.1142, -12.2784], requires_grad=True)\n",
      "Loss= tensor(1.1886, grad_fn=<DivBackward0>)\n",
      "i= 648\n",
      "w.grad= tensor([[-0.4073, -1.3933,  3.8553],\n",
      "        [-1.6977,  1.4031, -5.4971]])\n",
      "b.grad= tensor([-54.8673, 323.0495])\n",
      "new w tensor([[-0.4256,  0.8325,  0.7132],\n",
      "        [-0.1977,  0.8136,  0.9412]], requires_grad=True)\n",
      "new b tensor([  2.1169, -12.2945], requires_grad=True)\n",
      "Loss= tensor(1.1883, grad_fn=<DivBackward0>)\n",
      "i= 649\n",
      "w.grad= tensor([[-0.4039, -1.3870,  3.8388],\n",
      "        [-1.7002,  1.3972, -5.4809]])\n",
      "b.grad= tensor([-54.8143, 322.9178])\n",
      "new w tensor([[-0.4256,  0.8326,  0.7130],\n",
      "        [-0.1976,  0.8136,  0.9415]], requires_grad=True)\n",
      "new b tensor([  2.1197, -12.3107], requires_grad=True)\n",
      "Loss= tensor(1.1880, grad_fn=<DivBackward0>)\n",
      "i= 650\n",
      "w.grad= tensor([[-0.4005, -1.3807,  3.8224],\n",
      "        [-1.7023,  1.3919, -5.4644]])\n",
      "b.grad= tensor([-54.7614, 322.7861])\n",
      "new w tensor([[-0.4256,  0.8326,  0.7128],\n",
      "        [-0.1976,  0.8135,  0.9418]], requires_grad=True)\n",
      "new b tensor([  2.1224, -12.3268], requires_grad=True)\n",
      "Loss= tensor(1.1877, grad_fn=<DivBackward0>)\n",
      "i= 651\n",
      "w.grad= tensor([[-0.3971, -1.3744,  3.8061],\n",
      "        [-1.7050,  1.3858, -5.4484]])\n",
      "b.grad= tensor([-54.7086, 322.6543])\n",
      "new w tensor([[-0.4256,  0.8327,  0.7127],\n",
      "        [-0.1975,  0.8134,  0.9421]], requires_grad=True)\n",
      "new b tensor([  2.1251, -12.3430], requires_grad=True)\n",
      "Loss= tensor(1.1874, grad_fn=<DivBackward0>)\n",
      "i= 652\n",
      "w.grad= tensor([[-0.3939, -1.3682,  3.7898],\n",
      "        [-1.7074,  1.3800, -5.4324]])\n",
      "b.grad= tensor([-54.6558, 322.5225])\n",
      "new w tensor([[-0.4256,  0.8328,  0.7125],\n",
      "        [-0.1974,  0.8134,  0.9423]], requires_grad=True)\n",
      "new b tensor([  2.1279, -12.3591], requires_grad=True)\n",
      "Loss= tensor(1.1871, grad_fn=<DivBackward0>)\n",
      "i= 653\n",
      "w.grad= tensor([[-0.3903, -1.3617,  3.7738],\n",
      "        [-1.7109,  1.3730, -5.4171]])\n",
      "b.grad= tensor([-54.6031, 322.3906])\n",
      "new w tensor([[-0.4255,  0.8328,  0.7123],\n",
      "        [-0.1973,  0.8133,  0.9426]], requires_grad=True)\n",
      "new b tensor([  2.1306, -12.3752], requires_grad=True)\n",
      "Loss= tensor(1.1868, grad_fn=<DivBackward0>)\n",
      "i= 654\n",
      "w.grad= tensor([[-0.3874, -1.3562,  3.7574],\n",
      "        [-1.7130,  1.3677, -5.4010]])\n",
      "b.grad= tensor([-54.5505, 322.2587])\n",
      "new w tensor([[-0.4255,  0.8329,  0.7121],\n",
      "        [-0.1972,  0.8132,  0.9429]], requires_grad=True)\n",
      "new b tensor([  2.1333, -12.3913], requires_grad=True)\n",
      "Loss= tensor(1.1866, grad_fn=<DivBackward0>)\n",
      "i= 655\n",
      "w.grad= tensor([[-0.3835, -1.3493,  3.7417],\n",
      "        [-1.7152,  1.3622, -5.3851]])\n",
      "b.grad= tensor([-54.4979, 322.1267])\n",
      "new w tensor([[-0.4255,  0.8330,  0.7119],\n",
      "        [-0.1971,  0.8132,  0.9431]], requires_grad=True)\n",
      "new b tensor([  2.1361, -12.4074], requires_grad=True)\n",
      "Loss= tensor(1.1864, grad_fn=<DivBackward0>)\n",
      "i= 656\n",
      "w.grad= tensor([[-0.3804, -1.3433,  3.7256],\n",
      "        [-1.7181,  1.3559, -5.3696]])\n",
      "b.grad= tensor([-54.4454, 321.9947])\n",
      "new w tensor([[-0.4255,  0.8330,  0.7117],\n",
      "        [-0.1970,  0.8131,  0.9434]], requires_grad=True)\n",
      "new b tensor([  2.1388, -12.4235], requires_grad=True)\n",
      "Loss= tensor(1.1861, grad_fn=<DivBackward0>)\n",
      "i= 657\n",
      "w.grad= tensor([[-0.3770, -1.3371,  3.7099],\n",
      "        [-1.7208,  1.3498, -5.3541]])\n",
      "b.grad= tensor([-54.3929, 321.8626])\n",
      "new w tensor([[-0.4255,  0.8331,  0.7115],\n",
      "        [-0.1970,  0.8130,  0.9437]], requires_grad=True)\n",
      "new b tensor([  2.1415, -12.4396], requires_grad=True)\n",
      "Loss= tensor(1.1859, grad_fn=<DivBackward0>)\n",
      "i= 658\n",
      "w.grad= tensor([[-0.3737, -1.3310,  3.6940],\n",
      "        [-1.7230,  1.3446, -5.3383]])\n",
      "b.grad= tensor([-54.3405, 321.7305])\n",
      "new w tensor([[-0.4254,  0.8332,  0.7113],\n",
      "        [-0.1969,  0.8129,  0.9439]], requires_grad=True)\n",
      "new b tensor([  2.1442, -12.4557], requires_grad=True)\n",
      "Loss= tensor(1.1858, grad_fn=<DivBackward0>)\n",
      "i= 659\n",
      "w.grad= tensor([[-0.3705, -1.3249,  3.6783],\n",
      "        [-1.7255,  1.3386, -5.3229]])\n",
      "b.grad= tensor([-54.2881, 321.5983])\n",
      "new w tensor([[-0.4254,  0.8332,  0.7112],\n",
      "        [-0.1968,  0.8129,  0.9442]], requires_grad=True)\n",
      "new b tensor([  2.1469, -12.4718], requires_grad=True)\n",
      "Loss= tensor(1.1856, grad_fn=<DivBackward0>)\n",
      "i= 660\n",
      "w.grad= tensor([[-0.3678, -1.3196,  3.6622],\n",
      "        [-1.7284,  1.3323, -5.3079]])\n",
      "b.grad= tensor([-54.2358, 321.4661])\n",
      "new w tensor([[-0.4254,  0.8333,  0.7110],\n",
      "        [-0.1967,  0.8128,  0.9445]], requires_grad=True)\n",
      "new b tensor([  2.1496, -12.4879], requires_grad=True)\n",
      "Loss= tensor(1.1854, grad_fn=<DivBackward0>)\n",
      "i= 661\n",
      "w.grad= tensor([[-0.3635, -1.3124,  3.6474],\n",
      "        [-1.7301,  1.3274, -5.2920]])\n",
      "b.grad= tensor([-54.1835, 321.3338])\n",
      "new w tensor([[-0.4254,  0.8334,  0.7108],\n",
      "        [-0.1966,  0.8127,  0.9447]], requires_grad=True)\n",
      "new b tensor([  2.1523, -12.5039], requires_grad=True)\n",
      "Loss= tensor(1.1853, grad_fn=<DivBackward0>)\n",
      "i= 662\n",
      "w.grad= tensor([[-0.3609, -1.3071,  3.6314],\n",
      "        [-1.7323,  1.3221, -5.2765]])\n",
      "b.grad= tensor([-54.1313, 321.2015])\n",
      "new w tensor([[-0.4254,  0.8334,  0.7106],\n",
      "        [-0.1965,  0.8127,  0.9450]], requires_grad=True)\n",
      "new b tensor([  2.1551, -12.5200], requires_grad=True)\n",
      "Loss= tensor(1.1852, grad_fn=<DivBackward0>)\n",
      "i= 663\n",
      "w.grad= tensor([[-0.3576, -1.3011,  3.6160],\n",
      "        [-1.7351,  1.3161, -5.2616]])\n",
      "b.grad= tensor([-54.0792, 321.0692])\n",
      "new w tensor([[-0.4254,  0.8335,  0.7104],\n",
      "        [-0.1964,  0.8126,  0.9453]], requires_grad=True)\n",
      "new b tensor([  2.1578, -12.5360], requires_grad=True)\n",
      "Loss= tensor(1.1850, grad_fn=<DivBackward0>)\n",
      "i= 664\n",
      "w.grad= tensor([[-0.3544, -1.2951,  3.6007],\n",
      "        [-1.7383,  1.3095, -5.2471]])\n",
      "b.grad= tensor([-54.0271, 320.9368])\n",
      "new w tensor([[-0.4253,  0.8336,  0.7103],\n",
      "        [-0.1963,  0.8126,  0.9455]], requires_grad=True)\n",
      "new b tensor([  2.1605, -12.5521], requires_grad=True)\n",
      "Loss= tensor(1.1850, grad_fn=<DivBackward0>)\n",
      "i= 665\n",
      "w.grad= tensor([[-0.3513, -1.2893,  3.5853],\n",
      "        [-1.7395,  1.3052, -5.2312]])\n",
      "b.grad= tensor([-53.9751, 320.8043])\n",
      "new w tensor([[-0.4253,  0.8336,  0.7101],\n",
      "        [-0.1963,  0.8125,  0.9458]], requires_grad=True)\n",
      "new b tensor([  2.1632, -12.5681], requires_grad=True)\n",
      "Loss= tensor(1.1849, grad_fn=<DivBackward0>)\n",
      "i= 666\n",
      "w.grad= tensor([[-0.3482, -1.2834,  3.5701],\n",
      "        [-1.7426,  1.2988, -5.2167]])\n",
      "b.grad= tensor([-53.9231, 320.6718])\n",
      "new w tensor([[-0.4253,  0.8337,  0.7099],\n",
      "        [-0.1962,  0.8124,  0.9461]], requires_grad=True)\n",
      "new b tensor([  2.1659, -12.5842], requires_grad=True)\n",
      "Loss= tensor(1.1848, grad_fn=<DivBackward0>)\n",
      "i= 667\n",
      "w.grad= tensor([[-0.3447, -1.2773,  3.5551],\n",
      "        [-1.7445,  1.2939, -5.2014]])\n",
      "b.grad= tensor([-53.8712, 320.5392])\n",
      "new w tensor([[-0.4253,  0.8337,  0.7097],\n",
      "        [-0.1961,  0.8124,  0.9463]], requires_grad=True)\n",
      "new b tensor([  2.1685, -12.6002], requires_grad=True)\n",
      "Loss= tensor(1.1847, grad_fn=<DivBackward0>)\n",
      "i= 668\n",
      "w.grad= tensor([[-0.3422, -1.2721,  3.5396],\n",
      "        [-1.7470,  1.2880, -5.1867]])\n",
      "b.grad= tensor([-53.8193, 320.4066])\n",
      "new w tensor([[-0.4253,  0.8338,  0.7095],\n",
      "        [-0.1960,  0.8123,  0.9466]], requires_grad=True)\n",
      "new b tensor([  2.1712, -12.6162], requires_grad=True)\n",
      "Loss= tensor(1.1847, grad_fn=<DivBackward0>)\n",
      "i= 669\n",
      "w.grad= tensor([[-0.3385, -1.2656,  3.5250],\n",
      "        [-1.7494,  1.2824, -5.1720]])\n",
      "b.grad= tensor([-53.7675, 320.2740])\n",
      "new w tensor([[-0.4253,  0.8339,  0.7094],\n",
      "        [-0.1959,  0.8122,  0.9468]], requires_grad=True)\n",
      "new b tensor([  2.1739, -12.6322], requires_grad=True)\n",
      "Loss= tensor(1.1847, grad_fn=<DivBackward0>)\n",
      "i= 670\n",
      "w.grad= tensor([[-0.3356, -1.2601,  3.5099],\n",
      "        [-1.7519,  1.2768, -5.1572]])\n",
      "b.grad= tensor([-53.7157, 320.1412])\n",
      "new w tensor([[-0.4252,  0.8339,  0.7092],\n",
      "        [-0.1958,  0.8122,  0.9471]], requires_grad=True)\n",
      "new b tensor([  2.1766, -12.6482], requires_grad=True)\n",
      "Loss= tensor(1.1847, grad_fn=<DivBackward0>)\n",
      "i= 671\n",
      "w.grad= tensor([[-0.3322, -1.2539,  3.4953],\n",
      "        [-1.7537,  1.2719, -5.1423]])\n",
      "b.grad= tensor([-53.6640, 320.0085])\n",
      "new w tensor([[-0.4252,  0.8340,  0.7090],\n",
      "        [-0.1957,  0.8121,  0.9473]], requires_grad=True)\n",
      "new b tensor([  2.1793, -12.6642], requires_grad=True)\n",
      "Loss= tensor(1.1847, grad_fn=<DivBackward0>)\n",
      "i= 672\n",
      "w.grad= tensor([[-0.3294, -1.2485,  3.4803],\n",
      "        [-1.7566,  1.2657, -5.1282]])\n",
      "b.grad= tensor([-53.6123, 319.8756])\n",
      "new w tensor([[-0.4252,  0.8341,  0.7088],\n",
      "        [-0.1956,  0.8120,  0.9476]], requires_grad=True)\n",
      "new b tensor([  2.1820, -12.6802], requires_grad=True)\n",
      "Loss= tensor(1.1847, grad_fn=<DivBackward0>)\n",
      "i= 673\n",
      "w.grad= tensor([[-0.3264, -1.2428,  3.4655],\n",
      "        [-1.7585,  1.2608, -5.1133]])\n",
      "b.grad= tensor([-53.5607, 319.7427])\n",
      "new w tensor([[-0.4252,  0.8341,  0.7087],\n",
      "        [-0.1956,  0.8120,  0.9479]], requires_grad=True)\n",
      "new b tensor([  2.1847, -12.6962], requires_grad=True)\n",
      "Loss= tensor(1.1847, grad_fn=<DivBackward0>)\n",
      "i= 674\n",
      "w.grad= tensor([[-0.3231, -1.2369,  3.4510],\n",
      "        [-1.7610,  1.2552, -5.0990]])\n",
      "b.grad= tensor([-53.5091, 319.6098])\n",
      "new w tensor([[-0.4252,  0.8342,  0.7085],\n",
      "        [-0.1955,  0.8119,  0.9481]], requires_grad=True)\n",
      "new b tensor([  2.1873, -12.7122], requires_grad=True)\n",
      "Loss= tensor(1.1848, grad_fn=<DivBackward0>)\n",
      "i= 675\n",
      "w.grad= tensor([[-0.3202, -1.2314,  3.4363],\n",
      "        [-1.7637,  1.2491, -5.0850]])\n",
      "b.grad= tensor([-53.4576, 319.4768])\n",
      "new w tensor([[-0.4252,  0.8342,  0.7083],\n",
      "        [-0.1954,  0.8118,  0.9484]], requires_grad=True)\n",
      "new b tensor([  2.1900, -12.7282], requires_grad=True)\n",
      "Loss= tensor(1.1848, grad_fn=<DivBackward0>)\n",
      "i= 676\n",
      "w.grad= tensor([[-0.3175, -1.2261,  3.4216],\n",
      "        [-1.7652,  1.2447, -5.0702]])\n",
      "b.grad= tensor([-53.4062, 319.3438])\n",
      "new w tensor([[-0.4251,  0.8343,  0.7082],\n",
      "        [-0.1953,  0.8118,  0.9486]], requires_grad=True)\n",
      "new b tensor([  2.1927, -12.7441], requires_grad=True)\n",
      "Loss= tensor(1.1849, grad_fn=<DivBackward0>)\n",
      "i= 677\n",
      "w.grad= tensor([[-0.3140, -1.2199,  3.4075],\n",
      "        [-1.7676,  1.2391, -5.0560]])\n",
      "b.grad= tensor([-53.3548, 319.2107])\n",
      "new w tensor([[-0.4251,  0.8344,  0.7080],\n",
      "        [-0.1952,  0.8117,  0.9489]], requires_grad=True)\n",
      "new b tensor([  2.1953, -12.7601], requires_grad=True)\n",
      "Loss= tensor(1.1850, grad_fn=<DivBackward0>)\n",
      "i= 678\n",
      "w.grad= tensor([[-0.3110, -1.2143,  3.3931],\n",
      "        [-1.7693,  1.2343, -5.0415]])\n",
      "b.grad= tensor([-53.3034, 319.0775])\n",
      "new w tensor([[-0.4251,  0.8344,  0.7078],\n",
      "        [-0.1951,  0.8117,  0.9491]], requires_grad=True)\n",
      "new b tensor([  2.1980, -12.7761], requires_grad=True)\n",
      "Loss= tensor(1.1851, grad_fn=<DivBackward0>)\n",
      "i= 679\n",
      "w.grad= tensor([[-0.3086, -1.2095,  3.3783],\n",
      "        [-1.7715,  1.2291, -5.0273]])\n",
      "b.grad= tensor([-53.2521, 318.9443])\n",
      "new w tensor([[-0.4251,  0.8345,  0.7076],\n",
      "        [-0.1950,  0.8116,  0.9494]], requires_grad=True)\n",
      "new b tensor([  2.2007, -12.7920], requires_grad=True)\n",
      "Loss= tensor(1.1852, grad_fn=<DivBackward0>)\n",
      "i= 680\n",
      "w.grad= tensor([[-0.3052, -1.2034,  3.3644],\n",
      "        [-1.7741,  1.2233, -5.0135]])\n",
      "b.grad= tensor([-53.2008, 318.8111])\n",
      "new w tensor([[-0.4251,  0.8346,  0.7075],\n",
      "        [-0.1949,  0.8115,  0.9496]], requires_grad=True)\n",
      "new b tensor([  2.2033, -12.8079], requires_grad=True)\n",
      "Loss= tensor(1.1853, grad_fn=<DivBackward0>)\n",
      "i= 681\n",
      "w.grad= tensor([[-0.3027, -1.1983,  3.3499],\n",
      "        [-1.7759,  1.2185, -4.9993]])\n",
      "b.grad= tensor([-53.1496, 318.6778])\n",
      "new w tensor([[-0.4251,  0.8346,  0.7073],\n",
      "        [-0.1949,  0.8115,  0.9499]], requires_grad=True)\n",
      "new b tensor([  2.2060, -12.8239], requires_grad=True)\n",
      "Loss= tensor(1.1854, grad_fn=<DivBackward0>)\n",
      "i= 682\n",
      "w.grad= tensor([[-0.2998, -1.1930,  3.3357],\n",
      "        [-1.7777,  1.2137, -4.9850]])\n",
      "b.grad= tensor([-53.0984, 318.5444])\n",
      "new w tensor([[-0.4250,  0.8347,  0.7071],\n",
      "        [-0.1948,  0.8114,  0.9501]], requires_grad=True)\n",
      "new b tensor([  2.2086, -12.8398], requires_grad=True)\n",
      "Loss= tensor(1.1856, grad_fn=<DivBackward0>)\n",
      "i= 683\n",
      "w.grad= tensor([[-0.2966, -1.1872,  3.3218],\n",
      "        [-1.7803,  1.2080, -4.9714]])\n",
      "b.grad= tensor([-53.0473, 318.4110])\n",
      "new w tensor([[-0.4250,  0.8347,  0.7070],\n",
      "        [-0.1947,  0.8114,  0.9504]], requires_grad=True)\n",
      "new b tensor([  2.2113, -12.8557], requires_grad=True)\n",
      "Loss= tensor(1.1857, grad_fn=<DivBackward0>)\n",
      "i= 684\n",
      "w.grad= tensor([[-0.2936, -1.1816,  3.3079],\n",
      "        [-1.7829,  1.2023, -4.9579]])\n",
      "b.grad= tensor([-52.9963, 318.2775])\n",
      "new w tensor([[-0.4250,  0.8348,  0.7068],\n",
      "        [-0.1946,  0.8113,  0.9506]], requires_grad=True)\n",
      "new b tensor([  2.2139, -12.8716], requires_grad=True)\n",
      "Loss= tensor(1.1859, grad_fn=<DivBackward0>)\n",
      "i= 685\n",
      "w.grad= tensor([[-0.2914, -1.1770,  3.2935],\n",
      "        [-1.7845,  1.1977, -4.9438]])\n",
      "b.grad= tensor([-52.9453, 318.1440])\n",
      "new w tensor([[-0.4250,  0.8348,  0.7066],\n",
      "        [-0.1945,  0.8112,  0.9509]], requires_grad=True)\n",
      "new b tensor([  2.2166, -12.8875], requires_grad=True)\n",
      "Loss= tensor(1.1861, grad_fn=<DivBackward0>)\n",
      "i= 686\n",
      "w.grad= tensor([[-0.2882, -1.1712,  3.2799],\n",
      "        [-1.7866,  1.1925, -4.9302]])\n",
      "b.grad= tensor([-52.8943, 318.0104])\n",
      "new w tensor([[-0.4250,  0.8349,  0.7065],\n",
      "        [-0.1944,  0.8112,  0.9511]], requires_grad=True)\n",
      "new b tensor([  2.2192, -12.9034], requires_grad=True)\n",
      "Loss= tensor(1.1863, grad_fn=<DivBackward0>)\n",
      "i= 687\n",
      "w.grad= tensor([[-0.2853, -1.1658,  3.2660],\n",
      "        [-1.7884,  1.1877, -4.9164]])\n",
      "b.grad= tensor([-52.8434, 317.8768])\n",
      "new w tensor([[-0.4250,  0.8350,  0.7063],\n",
      "        [-0.1943,  0.8111,  0.9514]], requires_grad=True)\n",
      "new b tensor([  2.2219, -12.9193], requires_grad=True)\n",
      "Loss= tensor(1.1865, grad_fn=<DivBackward0>)\n",
      "i= 688\n",
      "w.grad= tensor([[-0.2824, -1.1605,  3.2523],\n",
      "        [-1.7905,  1.1827, -4.9027]])\n",
      "b.grad= tensor([-52.7925, 317.7431])\n",
      "new w tensor([[-0.4250,  0.8350,  0.7062],\n",
      "        [-0.1942,  0.8111,  0.9516]], requires_grad=True)\n",
      "new b tensor([  2.2245, -12.9352], requires_grad=True)\n",
      "Loss= tensor(1.1867, grad_fn=<DivBackward0>)\n",
      "i= 689\n",
      "w.grad= tensor([[-0.2802, -1.1558,  3.2382],\n",
      "        [-1.7927,  1.1773, -4.8893]])\n",
      "b.grad= tensor([-52.7417, 317.6093])\n",
      "new w tensor([[-0.4249,  0.8351,  0.7060],\n",
      "        [-0.1941,  0.8110,  0.9519]], requires_grad=True)\n",
      "new b tensor([  2.2272, -12.9511], requires_grad=True)\n",
      "Loss= tensor(1.1869, grad_fn=<DivBackward0>)\n",
      "i= 690\n",
      "w.grad= tensor([[-0.2763, -1.1493,  3.2253],\n",
      "        [-1.7948,  1.1723, -4.8759]])\n",
      "b.grad= tensor([-52.6909, 317.4755])\n",
      "new w tensor([[-0.4249,  0.8351,  0.7058],\n",
      "        [-0.1940,  0.8109,  0.9521]], requires_grad=True)\n",
      "new b tensor([  2.2298, -12.9670], requires_grad=True)\n",
      "Loss= tensor(1.1872, grad_fn=<DivBackward0>)\n",
      "i= 691\n",
      "w.grad= tensor([[-0.2742, -1.1450,  3.2112],\n",
      "        [-1.7963,  1.1679, -4.8621]])\n",
      "b.grad= tensor([-52.6402, 317.3416])\n",
      "new w tensor([[-0.4249,  0.8352,  0.7057],\n",
      "        [-0.1940,  0.8109,  0.9523]], requires_grad=True)\n",
      "new b tensor([  2.2324, -12.9828], requires_grad=True)\n",
      "Loss= tensor(1.1874, grad_fn=<DivBackward0>)\n",
      "i= 692\n",
      "w.grad= tensor([[-0.2717, -1.1399,  3.1976],\n",
      "        [-1.7988,  1.1624, -4.8491]])\n",
      "b.grad= tensor([-52.5895, 317.2077])\n",
      "new w tensor([[-0.4249,  0.8353,  0.7055],\n",
      "        [-0.1939,  0.8108,  0.9526]], requires_grad=True)\n",
      "new b tensor([  2.2351, -12.9987], requires_grad=True)\n",
      "Loss= tensor(1.1877, grad_fn=<DivBackward0>)\n",
      "i= 693\n",
      "w.grad= tensor([[-0.2683, -1.1341,  3.1845],\n",
      "        [-1.8004,  1.1579, -4.8355]])\n",
      "b.grad= tensor([-52.5388, 317.0737])\n",
      "new w tensor([[-0.4249,  0.8353,  0.7054],\n",
      "        [-0.1938,  0.8108,  0.9528]], requires_grad=True)\n",
      "new b tensor([  2.2377, -13.0146], requires_grad=True)\n",
      "Loss= tensor(1.1880, grad_fn=<DivBackward0>)\n",
      "i= 694\n",
      "w.grad= tensor([[-0.2664, -1.1298,  3.1706],\n",
      "        [-1.8027,  1.1525, -4.8225]])\n",
      "b.grad= tensor([-52.4883, 316.9397])\n",
      "new w tensor([[-0.4249,  0.8354,  0.7052],\n",
      "        [-0.1937,  0.8107,  0.9531]], requires_grad=True)\n",
      "new b tensor([  2.2403, -13.0304], requires_grad=True)\n",
      "Loss= tensor(1.1882, grad_fn=<DivBackward0>)\n",
      "i= 695\n",
      "w.grad= tensor([[-0.2626, -1.1235,  3.1580],\n",
      "        [-1.8045,  1.1477, -4.8092]])\n",
      "b.grad= tensor([-52.4377, 316.8056])\n",
      "new w tensor([[-0.4249,  0.8354,  0.7050],\n",
      "        [-0.1936,  0.8107,  0.9533]], requires_grad=True)\n",
      "new b tensor([  2.2429, -13.0462], requires_grad=True)\n",
      "Loss= tensor(1.1885, grad_fn=<DivBackward0>)\n",
      "i= 696\n",
      "w.grad= tensor([[-0.2605, -1.1191,  3.1443],\n",
      "        [-1.8066,  1.1426, -4.7962]])\n",
      "b.grad= tensor([-52.3872, 316.6714])\n",
      "new w tensor([[-0.4249,  0.8355,  0.7049],\n",
      "        [-0.1935,  0.8106,  0.9535]], requires_grad=True)\n",
      "new b tensor([  2.2455, -13.0621], requires_grad=True)\n",
      "Loss= tensor(1.1888, grad_fn=<DivBackward0>)\n",
      "i= 697\n",
      "w.grad= tensor([[-0.2581, -1.1143,  3.1309],\n",
      "        [-1.8080,  1.1384, -4.7827]])\n",
      "b.grad= tensor([-52.3367, 316.5372])\n",
      "new w tensor([[-0.4248,  0.8355,  0.7047],\n",
      "        [-0.1934,  0.8105,  0.9538]], requires_grad=True)\n",
      "new b tensor([  2.2482, -13.0779], requires_grad=True)\n",
      "Loss= tensor(1.1892, grad_fn=<DivBackward0>)\n",
      "i= 698\n",
      "w.grad= tensor([[-0.2548, -1.1085,  3.1182],\n",
      "        [-1.8107,  1.1325, -4.7704]])\n",
      "b.grad= tensor([-52.2863, 316.4030])\n",
      "new w tensor([[-0.4248,  0.8356,  0.7046],\n",
      "        [-0.1933,  0.8105,  0.9540]], requires_grad=True)\n",
      "new b tensor([  2.2508, -13.0937], requires_grad=True)\n",
      "Loss= tensor(1.1895, grad_fn=<DivBackward0>)\n",
      "i= 699\n",
      "w.grad= tensor([[-0.2524, -1.1037,  3.1049],\n",
      "        [-1.8117,  1.1287, -4.7568]])\n",
      "b.grad= tensor([-52.2360, 316.2686])\n",
      "new w tensor([[-0.4248,  0.8356,  0.7044],\n",
      "        [-0.1932,  0.8104,  0.9543]], requires_grad=True)\n",
      "new b tensor([  2.2534, -13.1095], requires_grad=True)\n",
      "Loss= tensor(1.1898, grad_fn=<DivBackward0>)\n",
      "i= 700\n",
      "w.grad= tensor([[-0.2496, -1.0985,  3.0920],\n",
      "        [-1.8138,  1.1238, -4.7440]])\n",
      "b.grad= tensor([-52.1856, 316.1342])\n",
      "new w tensor([[-0.4248,  0.8357,  0.7043],\n",
      "        [-0.1931,  0.8104,  0.9545]], requires_grad=True)\n",
      "new b tensor([  2.2560, -13.1254], requires_grad=True)\n",
      "Loss= tensor(1.1902, grad_fn=<DivBackward0>)\n",
      "i= 701\n",
      "w.grad= tensor([[-0.2476, -1.0941,  3.0786],\n",
      "        [-1.8154,  1.1194, -4.7309]])\n",
      "b.grad= tensor([-52.1353, 315.9998])\n",
      "new w tensor([[-0.4248,  0.8358,  0.7041],\n",
      "        [-0.1931,  0.8103,  0.9547]], requires_grad=True)\n",
      "new b tensor([  2.2586, -13.1412], requires_grad=True)\n",
      "Loss= tensor(1.1906, grad_fn=<DivBackward0>)\n",
      "i= 702\n",
      "w.grad= tensor([[-0.2446, -1.0889,  3.0659],\n",
      "        [-1.8179,  1.1138, -4.7186]])\n",
      "b.grad= tensor([-52.0851, 315.8653])\n",
      "new w tensor([[-0.4248,  0.8358,  0.7040],\n",
      "        [-0.1930,  0.8103,  0.9550]], requires_grad=True)\n",
      "new b tensor([  2.2612, -13.1569], requires_grad=True)\n",
      "Loss= tensor(1.1909, grad_fn=<DivBackward0>)\n",
      "i= 703\n",
      "w.grad= tensor([[-0.2424, -1.0843,  3.0528],\n",
      "        [-1.8186,  1.1105, -4.7051]])\n",
      "b.grad= tensor([-52.0349, 315.7307])\n",
      "new w tensor([[-0.4248,  0.8359,  0.7038],\n",
      "        [-0.1929,  0.8102,  0.9552]], requires_grad=True)\n",
      "new b tensor([  2.2638, -13.1727], requires_grad=True)\n",
      "Loss= tensor(1.1913, grad_fn=<DivBackward0>)\n",
      "i= 704\n",
      "w.grad= tensor([[-0.2392, -1.0788,  3.0404],\n",
      "        [-1.8214,  1.1046, -4.6931]])\n",
      "b.grad= tensor([-51.9848, 315.5961])\n",
      "new w tensor([[-0.4248,  0.8359,  0.7036],\n",
      "        [-0.1928,  0.8101,  0.9554]], requires_grad=True)\n",
      "new b tensor([  2.2664, -13.1885], requires_grad=True)\n",
      "Loss= tensor(1.1917, grad_fn=<DivBackward0>)\n",
      "i= 705\n",
      "w.grad= tensor([[-0.2366, -1.0737,  3.0277],\n",
      "        [-1.8229,  1.1003, -4.6802]])\n",
      "b.grad= tensor([-51.9347, 315.4614])\n",
      "new w tensor([[-0.4247,  0.8360,  0.7035],\n",
      "        [-0.1927,  0.8101,  0.9557]], requires_grad=True)\n",
      "new b tensor([  2.2690, -13.2043], requires_grad=True)\n",
      "Loss= tensor(1.1921, grad_fn=<DivBackward0>)\n",
      "i= 706\n",
      "w.grad= tensor([[-0.2342, -1.0691,  3.0148],\n",
      "        [-1.8250,  1.0952, -4.6679]])\n",
      "b.grad= tensor([-51.8846, 315.3267])\n",
      "new w tensor([[-0.4247,  0.8360,  0.7033],\n",
      "        [-0.1926,  0.8100,  0.9559]], requires_grad=True)\n",
      "new b tensor([  2.2716, -13.2200], requires_grad=True)\n",
      "Loss= tensor(1.1925, grad_fn=<DivBackward0>)\n",
      "i= 707\n",
      "w.grad= tensor([[-0.2315, -1.0641,  3.0024],\n",
      "        [-1.8257,  1.0917, -4.6546]])\n",
      "b.grad= tensor([-51.8346, 315.1919])\n",
      "new w tensor([[-0.4247,  0.8361,  0.7032],\n",
      "        [-0.1925,  0.8100,  0.9561]], requires_grad=True)\n",
      "new b tensor([  2.2742, -13.2358], requires_grad=True)\n",
      "Loss= tensor(1.1930, grad_fn=<DivBackward0>)\n",
      "i= 708\n",
      "w.grad= tensor([[-0.2291, -1.0593,  2.9897],\n",
      "        [-1.8284,  1.0861, -4.6428]])\n",
      "b.grad= tensor([-51.7846, 315.0570])\n",
      "new w tensor([[-0.4247,  0.8361,  0.7030],\n",
      "        [-0.1924,  0.8099,  0.9564]], requires_grad=True)\n",
      "new b tensor([  2.2768, -13.2516], requires_grad=True)\n",
      "Loss= tensor(1.1934, grad_fn=<DivBackward0>)\n",
      "i= 709\n",
      "w.grad= tensor([[-0.2266, -1.0546,  2.9772],\n",
      "        [-1.8295,  1.0823, -4.6299]])\n",
      "b.grad= tensor([-51.7346, 314.9221])\n",
      "new w tensor([[-0.4247,  0.8362,  0.7029],\n",
      "        [-0.1923,  0.8099,  0.9566]], requires_grad=True)\n",
      "new b tensor([  2.2794, -13.2673], requires_grad=True)\n",
      "Loss= tensor(1.1938, grad_fn=<DivBackward0>)\n",
      "i= 710\n",
      "w.grad= tensor([[-0.2238, -1.0494,  2.9650],\n",
      "        [-1.8314,  1.0778, -4.6176]])\n",
      "b.grad= tensor([-51.6847, 314.7871])\n",
      "new w tensor([[-0.4247,  0.8362,  0.7027],\n",
      "        [-0.1922,  0.8098,  0.9568]], requires_grad=True)\n",
      "new b tensor([  2.2820, -13.2830], requires_grad=True)\n",
      "Loss= tensor(1.1943, grad_fn=<DivBackward0>)\n",
      "i= 711\n",
      "w.grad= tensor([[-0.2217, -1.0451,  2.9523],\n",
      "        [-1.8333,  1.0727, -4.6055]])\n",
      "b.grad= tensor([-51.6349, 314.6521])\n",
      "new w tensor([[-0.4247,  0.8363,  0.7026],\n",
      "        [-0.1921,  0.8098,  0.9571]], requires_grad=True)\n",
      "new b tensor([  2.2845, -13.2988], requires_grad=True)\n",
      "Loss= tensor(1.1948, grad_fn=<DivBackward0>)\n",
      "i= 712\n",
      "w.grad= tensor([[-0.2194, -1.0406,  2.9398],\n",
      "        [-1.8352,  1.0680, -4.5934]])\n",
      "b.grad= tensor([-51.5851, 314.5170])\n",
      "new w tensor([[-0.4247,  0.8363,  0.7025],\n",
      "        [-0.1921,  0.8097,  0.9573]], requires_grad=True)\n",
      "new b tensor([  2.2871, -13.3145], requires_grad=True)\n",
      "Loss= tensor(1.1953, grad_fn=<DivBackward0>)\n",
      "i= 713\n",
      "w.grad= tensor([[-0.2163, -1.0352,  2.9279],\n",
      "        [-1.8366,  1.0640, -4.5809]])\n",
      "b.grad= tensor([-51.5353, 314.3818])\n",
      "new w tensor([[-0.4246,  0.8364,  0.7023],\n",
      "        [-0.1920,  0.8097,  0.9575]], requires_grad=True)\n",
      "new b tensor([  2.2897, -13.3302], requires_grad=True)\n",
      "Loss= tensor(1.1957, grad_fn=<DivBackward0>)\n",
      "i= 714\n",
      "w.grad= tensor([[-0.2141, -1.0308,  2.9155],\n",
      "        [-1.8385,  1.0591, -4.5690]])\n",
      "b.grad= tensor([-51.4856, 314.2466])\n",
      "new w tensor([[-0.4246,  0.8364,  0.7022],\n",
      "        [-0.1919,  0.8096,  0.9578]], requires_grad=True)\n",
      "new b tensor([  2.2923, -13.3459], requires_grad=True)\n",
      "Loss= tensor(1.1962, grad_fn=<DivBackward0>)\n",
      "i= 715\n",
      "w.grad= tensor([[-0.2115, -1.0258,  2.9035],\n",
      "        [-1.8396,  1.0554, -4.5565]])\n",
      "b.grad= tensor([-51.4359, 314.1113])\n",
      "new w tensor([[-0.4246,  0.8365,  0.7020],\n",
      "        [-0.1918,  0.8096,  0.9580]], requires_grad=True)\n",
      "new b tensor([  2.2948, -13.3616], requires_grad=True)\n",
      "Loss= tensor(1.1967, grad_fn=<DivBackward0>)\n",
      "i= 716\n",
      "w.grad= tensor([[-0.2096, -1.0218,  2.8910],\n",
      "        [-1.8416,  1.0506, -4.5446]])\n",
      "b.grad= tensor([-51.3862, 313.9760])\n",
      "new w tensor([[-0.4246,  0.8365,  0.7019],\n",
      "        [-0.1917,  0.8095,  0.9582]], requires_grad=True)\n",
      "new b tensor([  2.2974, -13.3773], requires_grad=True)\n",
      "Loss= tensor(1.1973, grad_fn=<DivBackward0>)\n",
      "i= 717\n",
      "w.grad= tensor([[-0.2073, -1.0172,  2.8789],\n",
      "        [-1.8431,  1.0464, -4.5325]])\n",
      "b.grad= tensor([-51.3366, 313.8406])\n",
      "new w tensor([[-0.4246,  0.8366,  0.7017],\n",
      "        [-0.1916,  0.8095,  0.9584]], requires_grad=True)\n",
      "new b tensor([  2.3000, -13.3930], requires_grad=True)\n",
      "Loss= tensor(1.1978, grad_fn=<DivBackward0>)\n",
      "i= 718\n",
      "w.grad= tensor([[-0.2044, -1.0123,  2.8671],\n",
      "        [-1.8452,  1.0413, -4.5209]])\n",
      "b.grad= tensor([-51.2870, 313.7051])\n",
      "new w tensor([[-0.4246,  0.8366,  0.7016],\n",
      "        [-0.1915,  0.8094,  0.9587]], requires_grad=True)\n",
      "new b tensor([  2.3025, -13.4087], requires_grad=True)\n",
      "Loss= tensor(1.1983, grad_fn=<DivBackward0>)\n",
      "i= 719\n",
      "w.grad= tensor([[-0.2019, -1.0076,  2.8553],\n",
      "        [-1.8467,  1.0371, -4.5090]])\n",
      "b.grad= tensor([-51.2375, 313.5696])\n",
      "new w tensor([[-0.4246,  0.8367,  0.7014],\n",
      "        [-0.1914,  0.8093,  0.9589]], requires_grad=True)\n",
      "new b tensor([  2.3051, -13.4244], requires_grad=True)\n",
      "Loss= tensor(1.1989, grad_fn=<DivBackward0>)\n",
      "i= 720\n",
      "w.grad= tensor([[-0.1998, -1.0033,  2.8432],\n",
      "        [-1.8479,  1.0333, -4.4968]])\n",
      "b.grad= tensor([-51.1880, 313.4340])\n",
      "new w tensor([[-0.4246,  0.8367,  0.7013],\n",
      "        [-0.1913,  0.8093,  0.9591]], requires_grad=True)\n",
      "new b tensor([  2.3077, -13.4401], requires_grad=True)\n",
      "Loss= tensor(1.1994, grad_fn=<DivBackward0>)\n",
      "i= 721\n",
      "w.grad= tensor([[-0.1974, -0.9987,  2.8314],\n",
      "        [-1.8502,  1.0282, -4.4855]])\n",
      "b.grad= tensor([-51.1385, 313.2983])\n",
      "new w tensor([[-0.4246,  0.8368,  0.7012],\n",
      "        [-0.1912,  0.8092,  0.9593]], requires_grad=True)\n",
      "new b tensor([  2.3102, -13.4557], requires_grad=True)\n",
      "Loss= tensor(1.2000, grad_fn=<DivBackward0>)\n",
      "i= 722\n",
      "w.grad= tensor([[-0.1948, -0.9939,  2.8197],\n",
      "        [-1.8511,  1.0245, -4.4734]])\n",
      "b.grad= tensor([-51.0891, 313.1626])\n",
      "new w tensor([[-0.4246,  0.8368,  0.7010],\n",
      "        [-0.1911,  0.8092,  0.9596]], requires_grad=True)\n",
      "new b tensor([  2.3128, -13.4714], requires_grad=True)\n",
      "Loss= tensor(1.2005, grad_fn=<DivBackward0>)\n",
      "i= 723\n",
      "w.grad= tensor([[-0.1922, -0.9892,  2.8082],\n",
      "        [-1.8523,  1.0206, -4.4614]])\n",
      "b.grad= tensor([-51.0397, 313.0268])\n",
      "new w tensor([[-0.4245,  0.8369,  0.7009],\n",
      "        [-0.1910,  0.8091,  0.9598]], requires_grad=True)\n",
      "new b tensor([  2.3153, -13.4870], requires_grad=True)\n",
      "Loss= tensor(1.2011, grad_fn=<DivBackward0>)\n",
      "i= 724\n",
      "w.grad= tensor([[-0.1897, -0.9843,  2.7967],\n",
      "        [-1.8546,  1.0156, -4.4503]])\n",
      "b.grad= tensor([-50.9904, 312.8909])\n",
      "new w tensor([[-0.4245,  0.8369,  0.7007],\n",
      "        [-0.1909,  0.8091,  0.9600]], requires_grad=True)\n",
      "new b tensor([  2.3179, -13.5027], requires_grad=True)\n",
      "Loss= tensor(1.2017, grad_fn=<DivBackward0>)\n",
      "i= 725\n",
      "w.grad= tensor([[-0.1879, -0.9804,  2.7847],\n",
      "        [-1.8552,  1.0124, -4.4380]])\n",
      "b.grad= tensor([-50.9411, 312.7550])\n",
      "new w tensor([[-0.4245,  0.8370,  0.7006],\n",
      "        [-0.1909,  0.8090,  0.9602]], requires_grad=True)\n",
      "new b tensor([  2.3204, -13.5183], requires_grad=True)\n",
      "Loss= tensor(1.2023, grad_fn=<DivBackward0>)\n",
      "i= 726\n",
      "w.grad= tensor([[-0.1859, -0.9764,  2.7729],\n",
      "        [-1.8575,  1.0072, -4.4271]])\n",
      "b.grad= tensor([-50.8918, 312.6190])\n",
      "new w tensor([[-0.4245,  0.8370,  0.7005],\n",
      "        [-0.1908,  0.8090,  0.9604]], requires_grad=True)\n",
      "new b tensor([  2.3230, -13.5340], requires_grad=True)\n",
      "Loss= tensor(1.2029, grad_fn=<DivBackward0>)\n",
      "i= 727\n",
      "w.grad= tensor([[-0.1833, -0.9717,  2.7615],\n",
      "        [-1.8585,  1.0036, -4.4153]])\n",
      "b.grad= tensor([-50.8426, 312.4830])\n",
      "new w tensor([[-0.4245,  0.8371,  0.7003],\n",
      "        [-0.1907,  0.8089,  0.9607]], requires_grad=True)\n",
      "new b tensor([  2.3255, -13.5496], requires_grad=True)\n",
      "Loss= tensor(1.2035, grad_fn=<DivBackward0>)\n",
      "i= 728\n",
      "w.grad= tensor([[-0.1817, -0.9680,  2.7496],\n",
      "        [-1.8602,  0.9992, -4.4039]])\n",
      "b.grad= tensor([-50.7934, 312.3469])\n",
      "new w tensor([[-0.4245,  0.8371,  0.7002],\n",
      "        [-0.1906,  0.8089,  0.9609]], requires_grad=True)\n",
      "new b tensor([  2.3280, -13.5652], requires_grad=True)\n",
      "Loss= tensor(1.2042, grad_fn=<DivBackward0>)\n",
      "i= 729\n",
      "w.grad= tensor([[-0.1787, -0.9628,  2.7387],\n",
      "        [-1.8625,  0.9940, -4.3932]])\n",
      "b.grad= tensor([-50.7442, 312.2107])\n",
      "new w tensor([[-0.4245,  0.8372,  0.7000],\n",
      "        [-0.1905,  0.8088,  0.9611]], requires_grad=True)\n",
      "new b tensor([  2.3306, -13.5808], requires_grad=True)\n",
      "Loss= tensor(1.2048, grad_fn=<DivBackward0>)\n",
      "i= 730\n",
      "w.grad= tensor([[-0.1757, -0.9576,  2.7278],\n",
      "        [-1.8627,  0.9913, -4.3809]])\n",
      "b.grad= tensor([-50.6951, 312.0745])\n",
      "new w tensor([[-0.4245,  0.8372,  0.6999],\n",
      "        [-0.1904,  0.8088,  0.9613]], requires_grad=True)\n",
      "new b tensor([  2.3331, -13.5964], requires_grad=True)\n",
      "Loss= tensor(1.2054, grad_fn=<DivBackward0>)\n",
      "i= 731\n",
      "w.grad= tensor([[-0.1744, -0.9542,  2.7159],\n",
      "        [-1.8647,  0.9867, -4.3699]])\n",
      "b.grad= tensor([-50.6460, 311.9382])\n",
      "new w tensor([[-0.4245,  0.8373,  0.6998],\n",
      "        [-0.1903,  0.8087,  0.9615]], requires_grad=True)\n",
      "new b tensor([  2.3356, -13.6120], requires_grad=True)\n",
      "Loss= tensor(1.2061, grad_fn=<DivBackward0>)\n",
      "i= 732\n",
      "w.grad= tensor([[-0.1722, -0.9501,  2.7046],\n",
      "        [-1.8660,  0.9827, -4.3587]])\n",
      "b.grad= tensor([-50.5970, 311.8018])\n",
      "new w tensor([[-0.4245,  0.8373,  0.6996],\n",
      "        [-0.1902,  0.8087,  0.9618]], requires_grad=True)\n",
      "new b tensor([  2.3382, -13.6276], requires_grad=True)\n",
      "Loss= tensor(1.2068, grad_fn=<DivBackward0>)\n",
      "i= 733\n",
      "w.grad= tensor([[-0.1693, -0.9449,  2.6938],\n",
      "        [-1.8676,  0.9785, -4.3475]])\n",
      "b.grad= tensor([-50.5480, 311.6654])\n",
      "new w tensor([[-0.4245,  0.8374,  0.6995],\n",
      "        [-0.1901,  0.8086,  0.9620]], requires_grad=True)\n",
      "new b tensor([  2.3407, -13.6432], requires_grad=True)\n",
      "Loss= tensor(1.2074, grad_fn=<DivBackward0>)\n",
      "i= 734\n",
      "w.grad= tensor([[-0.1678, -0.9415,  2.6822],\n",
      "        [-1.8697,  0.9735, -4.3368]])\n",
      "b.grad= tensor([-50.4990, 311.5289])\n",
      "new w tensor([[-0.4244,  0.8374,  0.6994],\n",
      "        [-0.1900,  0.8086,  0.9622]], requires_grad=True)\n",
      "new b tensor([  2.3432, -13.6588], requires_grad=True)\n",
      "Loss= tensor(1.2081, grad_fn=<DivBackward0>)\n",
      "i= 735\n",
      "w.grad= tensor([[-0.1652, -0.9368,  2.6713],\n",
      "        [-1.8702,  0.9706, -4.3251]])\n",
      "b.grad= tensor([-50.4501, 311.3923])\n",
      "new w tensor([[-0.4244,  0.8375,  0.6992],\n",
      "        [-0.1899,  0.8085,  0.9624]], requires_grad=True)\n",
      "new b tensor([  2.3458, -13.6743], requires_grad=True)\n",
      "Loss= tensor(1.2088, grad_fn=<DivBackward0>)\n",
      "i= 736\n",
      "w.grad= tensor([[-0.1633, -0.9327,  2.6601],\n",
      "        [-1.8716,  0.9665, -4.3140]])\n",
      "b.grad= tensor([-50.4012, 311.2557])\n",
      "new w tensor([[-0.4244,  0.8375,  0.6991],\n",
      "        [-0.1898,  0.8085,  0.9626]], requires_grad=True)\n",
      "new b tensor([  2.3483, -13.6899], requires_grad=True)\n",
      "Loss= tensor(1.2095, grad_fn=<DivBackward0>)\n",
      "i= 737\n",
      "w.grad= tensor([[-0.1611, -0.9286,  2.6491],\n",
      "        [-1.8729,  0.9626, -4.3030]])\n",
      "b.grad= tensor([-50.3523, 311.1190])\n",
      "new w tensor([[-0.4244,  0.8376,  0.6990],\n",
      "        [-0.1897,  0.8084,  0.9628]], requires_grad=True)\n",
      "new b tensor([  2.3508, -13.7054], requires_grad=True)\n",
      "Loss= tensor(1.2102, grad_fn=<DivBackward0>)\n",
      "i= 738\n",
      "w.grad= tensor([[-0.1586, -0.9240,  2.6383],\n",
      "        [-1.8743,  0.9587, -4.2920]])\n",
      "b.grad= tensor([-50.3035, 310.9822])\n",
      "new w tensor([[-0.4244,  0.8376,  0.6988],\n",
      "        [-0.1896,  0.8084,  0.9631]], requires_grad=True)\n",
      "new b tensor([  2.3533, -13.7210], requires_grad=True)\n",
      "Loss= tensor(1.2109, grad_fn=<DivBackward0>)\n",
      "i= 739\n",
      "w.grad= tensor([[-0.1571, -0.9206,  2.6269],\n",
      "        [-1.8758,  0.9544, -4.2813]])\n",
      "b.grad= tensor([-50.2547, 310.8454])\n",
      "new w tensor([[-0.4244,  0.8377,  0.6987],\n",
      "        [-0.1895,  0.8084,  0.9633]], requires_grad=True)\n",
      "new b tensor([  2.3558, -13.7365], requires_grad=True)\n",
      "Loss= tensor(1.2116, grad_fn=<DivBackward0>)\n",
      "i= 740\n",
      "w.grad= tensor([[-0.1543, -0.9157,  2.6166],\n",
      "        [-1.8777,  0.9498, -4.2708]])\n",
      "b.grad= tensor([-50.2059, 310.7085])\n",
      "new w tensor([[-0.4244,  0.8377,  0.6986],\n",
      "        [-0.1895,  0.8083,  0.9635]], requires_grad=True)\n",
      "new b tensor([  2.3583, -13.7521], requires_grad=True)\n",
      "Loss= tensor(1.2123, grad_fn=<DivBackward0>)\n",
      "i= 741\n",
      "w.grad= tensor([[-0.1525, -0.9118,  2.6056],\n",
      "        [-1.8784,  0.9467, -4.2595]])\n",
      "b.grad= tensor([-50.1572, 310.5715])\n",
      "new w tensor([[-0.4244,  0.8377,  0.6984],\n",
      "        [-0.1894,  0.8083,  0.9637]], requires_grad=True)\n",
      "new b tensor([  2.3608, -13.7676], requires_grad=True)\n",
      "Loss= tensor(1.2131, grad_fn=<DivBackward0>)\n",
      "i= 742\n",
      "w.grad= tensor([[-0.1501, -0.9074,  2.5950],\n",
      "        [-1.8795,  0.9431, -4.2485]])\n",
      "b.grad= tensor([-50.1085, 310.4345])\n",
      "new w tensor([[-0.4244,  0.8378,  0.6983],\n",
      "        [-0.1893,  0.8082,  0.9639]], requires_grad=True)\n",
      "new b tensor([  2.3633, -13.7831], requires_grad=True)\n",
      "Loss= tensor(1.2138, grad_fn=<DivBackward0>)\n",
      "i= 743\n",
      "w.grad= tensor([[-0.1485, -0.9040,  2.5839],\n",
      "        [-1.8818,  0.9378, -4.2385]])\n",
      "b.grad= tensor([-50.0598, 310.2974])\n",
      "new w tensor([[-0.4244,  0.8378,  0.6982],\n",
      "        [-0.1892,  0.8082,  0.9641]], requires_grad=True)\n",
      "new b tensor([  2.3658, -13.7986], requires_grad=True)\n",
      "Loss= tensor(1.2146, grad_fn=<DivBackward0>)\n",
      "i= 744\n",
      "w.grad= tensor([[-0.1463, -0.8998,  2.5733],\n",
      "        [-1.8817,  0.9356, -4.2268]])\n",
      "b.grad= tensor([-50.0112, 310.1603])\n",
      "new w tensor([[-0.4244,  0.8379,  0.6981],\n",
      "        [-0.1891,  0.8081,  0.9643]], requires_grad=True)\n",
      "new b tensor([  2.3683, -13.8141], requires_grad=True)\n",
      "Loss= tensor(1.2153, grad_fn=<DivBackward0>)\n",
      "i= 745\n",
      "w.grad= tensor([[-0.1440, -0.8954,  2.5628],\n",
      "        [-1.8838,  0.9310, -4.2167]])\n",
      "b.grad= tensor([-49.9626, 310.0230])\n",
      "new w tensor([[-0.4244,  0.8379,  0.6979],\n",
      "        [-0.1890,  0.8081,  0.9645]], requires_grad=True)\n",
      "new b tensor([  2.3708, -13.8296], requires_grad=True)\n",
      "Loss= tensor(1.2161, grad_fn=<DivBackward0>)\n",
      "i= 746\n",
      "w.grad= tensor([[-0.1420, -0.8914,  2.5522],\n",
      "        [-1.8852,  0.9269, -4.2063]])\n",
      "b.grad= tensor([-49.9140, 309.8857])\n",
      "new w tensor([[-0.4244,  0.8380,  0.6978],\n",
      "        [-0.1889,  0.8080,  0.9648]], requires_grad=True)\n",
      "new b tensor([  2.3733, -13.8451], requires_grad=True)\n",
      "Loss= tensor(1.2168, grad_fn=<DivBackward0>)\n",
      "i= 747\n",
      "w.grad= tensor([[-0.1398, -0.8872,  2.5418],\n",
      "        [-1.8862,  0.9235, -4.1954]])\n",
      "b.grad= tensor([-49.8655, 309.7484])\n",
      "new w tensor([[-0.4244,  0.8380,  0.6977],\n",
      "        [-0.1888,  0.8080,  0.9650]], requires_grad=True)\n",
      "new b tensor([  2.3758, -13.8606], requires_grad=True)\n",
      "Loss= tensor(1.2176, grad_fn=<DivBackward0>)\n",
      "i= 748\n",
      "w.grad= tensor([[-0.1378, -0.8834,  2.5312],\n",
      "        [-1.8879,  0.9191, -4.1852]])\n",
      "b.grad= tensor([-49.8170, 309.6110])\n",
      "new w tensor([[-0.4243,  0.8381,  0.6975],\n",
      "        [-0.1887,  0.8079,  0.9652]], requires_grad=True)\n",
      "new b tensor([  2.3783, -13.8761], requires_grad=True)\n",
      "Loss= tensor(1.2184, grad_fn=<DivBackward0>)\n",
      "i= 749\n",
      "w.grad= tensor([[-0.1352, -0.8786,  2.5212],\n",
      "        [-1.8893,  0.9151, -4.1749]])\n",
      "b.grad= tensor([-49.7685, 309.4734])\n",
      "new w tensor([[-0.4243,  0.8381,  0.6974],\n",
      "        [-0.1886,  0.8079,  0.9654]], requires_grad=True)\n",
      "new b tensor([  2.3808, -13.8916], requires_grad=True)\n",
      "Loss= tensor(1.2192, grad_fn=<DivBackward0>)\n",
      "i= 750\n",
      "w.grad= tensor([[-0.1341, -0.8757,  2.5102],\n",
      "        [-1.8904,  0.9115, -4.1644]])\n",
      "b.grad= tensor([-49.7201, 309.3359])\n",
      "new w tensor([[-0.4243,  0.8381,  0.6973],\n",
      "        [-0.1885,  0.8078,  0.9656]], requires_grad=True)\n",
      "new b tensor([  2.3833, -13.9071], requires_grad=True)\n",
      "Loss= tensor(1.2200, grad_fn=<DivBackward0>)\n",
      "i= 751\n",
      "w.grad= tensor([[-0.1313, -0.8709,  2.5004],\n",
      "        [-1.8910,  0.9085, -4.1536]])\n",
      "b.grad= tensor([-49.6717, 309.1982])\n",
      "new w tensor([[-0.4243,  0.8382,  0.6972],\n",
      "        [-0.1884,  0.8078,  0.9658]], requires_grad=True)\n",
      "new b tensor([  2.3858, -13.9225], requires_grad=True)\n",
      "Loss= tensor(1.2208, grad_fn=<DivBackward0>)\n",
      "i= 752\n",
      "w.grad= tensor([[-0.1299, -0.8676,  2.4897],\n",
      "        [-1.8925,  0.9043, -4.1435]])\n",
      "b.grad= tensor([-49.6233, 309.0605])\n",
      "new w tensor([[-0.4243,  0.8382,  0.6970],\n",
      "        [-0.1883,  0.8078,  0.9660]], requires_grad=True)\n",
      "new b tensor([  2.3883, -13.9380], requires_grad=True)\n",
      "Loss= tensor(1.2216, grad_fn=<DivBackward0>)\n",
      "i= 753\n",
      "w.grad= tensor([[-0.1276, -0.8634,  2.4797],\n",
      "        [-1.8938,  0.9007, -4.1333]])\n",
      "b.grad= tensor([-49.5750, 308.9228])\n",
      "new w tensor([[-0.4243,  0.8383,  0.6969],\n",
      "        [-0.1882,  0.8077,  0.9662]], requires_grad=True)\n",
      "new b tensor([  2.3907, -13.9534], requires_grad=True)\n",
      "Loss= tensor(1.2225, grad_fn=<DivBackward0>)\n",
      "i= 754\n",
      "w.grad= tensor([[-0.1259, -0.8599,  2.4693],\n",
      "        [-1.8952,  0.8964, -4.1232]])\n",
      "b.grad= tensor([-49.5267, 308.7849])\n",
      "new w tensor([[-0.4243,  0.8383,  0.6968],\n",
      "        [-0.1881,  0.8077,  0.9664]], requires_grad=True)\n",
      "new b tensor([  2.3932, -13.9688], requires_grad=True)\n",
      "Loss= tensor(1.2233, grad_fn=<DivBackward0>)\n",
      "i= 755\n",
      "w.grad= tensor([[-0.1236, -0.8555,  2.4594],\n",
      "        [-1.8964,  0.8927, -4.1131]])\n",
      "b.grad= tensor([-49.4784, 308.6470])\n",
      "new w tensor([[-0.4243,  0.8384,  0.6967],\n",
      "        [-0.1880,  0.8076,  0.9666]], requires_grad=True)\n",
      "new b tensor([  2.3957, -13.9843], requires_grad=True)\n",
      "Loss= tensor(1.2241, grad_fn=<DivBackward0>)\n",
      "i= 756\n",
      "w.grad= tensor([[-0.1219, -0.8520,  2.4491],\n",
      "        [-1.8967,  0.8901, -4.1023]])\n",
      "b.grad= tensor([-49.4302, 308.5091])\n",
      "new w tensor([[-0.4243,  0.8384,  0.6966],\n",
      "        [-0.1879,  0.8076,  0.9668]], requires_grad=True)\n",
      "new b tensor([  2.3982, -13.9997], requires_grad=True)\n",
      "Loss= tensor(1.2250, grad_fn=<DivBackward0>)\n",
      "i= 757\n",
      "w.grad= tensor([[-0.1190, -0.8471,  2.4397],\n",
      "        [-1.8985,  0.8857, -4.0926]])\n",
      "b.grad= tensor([-49.3820, 308.3710])\n",
      "new w tensor([[-0.4243,  0.8385,  0.6964],\n",
      "        [-0.1878,  0.8075,  0.9670]], requires_grad=True)\n",
      "new b tensor([  2.4006, -14.0151], requires_grad=True)\n",
      "Loss= tensor(1.2258, grad_fn=<DivBackward0>)\n",
      "i= 758\n",
      "w.grad= tensor([[-0.1180, -0.8444,  2.4290],\n",
      "        [-1.8994,  0.8824, -4.0824]])\n",
      "b.grad= tensor([-49.3338, 308.2329])\n",
      "new w tensor([[-0.4243,  0.8385,  0.6963],\n",
      "        [-0.1878,  0.8075,  0.9672]], requires_grad=True)\n",
      "new b tensor([  2.4031, -14.0305], requires_grad=True)\n",
      "Loss= tensor(1.2267, grad_fn=<DivBackward0>)\n",
      "i= 759\n",
      "w.grad= tensor([[-0.1161, -0.8405,  2.4191],\n",
      "        [-1.9002,  0.8791, -4.0721]])\n",
      "b.grad= tensor([-49.2857, 308.0948])\n",
      "new w tensor([[-0.4243,  0.8385,  0.6962],\n",
      "        [-0.1877,  0.8074,  0.9674]], requires_grad=True)\n",
      "new b tensor([  2.4056, -14.0459], requires_grad=True)\n",
      "Loss= tensor(1.2275, grad_fn=<DivBackward0>)\n",
      "i= 760\n",
      "w.grad= tensor([[-0.1140, -0.8365,  2.4092],\n",
      "        [-1.9018,  0.8750, -4.0625]])\n",
      "b.grad= tensor([-49.2375, 307.9565])\n",
      "new w tensor([[-0.4243,  0.8386,  0.6961],\n",
      "        [-0.1876,  0.8074,  0.9676]], requires_grad=True)\n",
      "new b tensor([  2.4080, -14.0613], requires_grad=True)\n",
      "Loss= tensor(1.2284, grad_fn=<DivBackward0>)\n",
      "i= 761\n",
      "w.grad= tensor([[-0.1118, -0.8325,  2.3996],\n",
      "        [-1.9024,  0.8721, -4.0522]])\n",
      "b.grad= tensor([-49.1894, 307.8182])\n",
      "new w tensor([[-0.4243,  0.8386,  0.6960],\n",
      "        [-0.1875,  0.8074,  0.9679]], requires_grad=True)\n",
      "new b tensor([  2.4105, -14.0767], requires_grad=True)\n",
      "Loss= tensor(1.2293, grad_fn=<DivBackward0>)\n",
      "i= 762\n",
      "w.grad= tensor([[-0.1104, -0.8294,  2.3893],\n",
      "        [-1.9040,  0.8678, -4.0426]])\n",
      "b.grad= tensor([-49.1414, 307.6798])\n",
      "new w tensor([[-0.4243,  0.8387,  0.6958],\n",
      "        [-0.1874,  0.8073,  0.9681]], requires_grad=True)\n",
      "new b tensor([  2.4129, -14.0921], requires_grad=True)\n",
      "Loss= tensor(1.2302, grad_fn=<DivBackward0>)\n",
      "i= 763\n",
      "w.grad= tensor([[-0.1080, -0.8249,  2.3800],\n",
      "        [-1.9055,  0.8639, -4.0330]])\n",
      "b.grad= tensor([-49.0934, 307.5414])\n",
      "new w tensor([[-0.4243,  0.8387,  0.6957],\n",
      "        [-0.1873,  0.8073,  0.9683]], requires_grad=True)\n",
      "new b tensor([  2.4154, -14.1075], requires_grad=True)\n",
      "Loss= tensor(1.2311, grad_fn=<DivBackward0>)\n",
      "i= 764\n",
      "w.grad= tensor([[-0.1064, -0.8215,  2.3700],\n",
      "        [-1.9055,  0.8616, -4.0224]])\n",
      "b.grad= tensor([-49.0454, 307.4028])\n",
      "new w tensor([[-0.4242,  0.8387,  0.6956],\n",
      "        [-0.1872,  0.8072,  0.9685]], requires_grad=True)\n",
      "new b tensor([  2.4178, -14.1229], requires_grad=True)\n",
      "Loss= tensor(1.2320, grad_fn=<DivBackward0>)\n",
      "i= 765\n",
      "w.grad= tensor([[-0.1046, -0.8178,  2.3603],\n",
      "        [-1.9075,  0.8571, -4.0133]])\n",
      "b.grad= tensor([-48.9974, 307.2642])\n",
      "new w tensor([[-0.4242,  0.8388,  0.6955],\n",
      "        [-0.1871,  0.8072,  0.9687]], requires_grad=True)\n",
      "new b tensor([  2.4203, -14.1382], requires_grad=True)\n",
      "Loss= tensor(1.2329, grad_fn=<DivBackward0>)\n",
      "i= 766\n",
      "w.grad= tensor([[-0.1029, -0.8144,  2.3505],\n",
      "        [-1.9080,  0.8540, -4.0032]])\n",
      "b.grad= tensor([-48.9494, 307.1255])\n",
      "new w tensor([[-0.4242,  0.8388,  0.6954],\n",
      "        [-0.1870,  0.8071,  0.9689]], requires_grad=True)\n",
      "new b tensor([  2.4227, -14.1536], requires_grad=True)\n",
      "Loss= tensor(1.2338, grad_fn=<DivBackward0>)\n",
      "i= 767\n",
      "w.grad= tensor([[-0.1006, -0.8101,  2.3413],\n",
      "        [-1.9092,  0.8504, -3.9937]])\n",
      "b.grad= tensor([-48.9015, 306.9868])\n",
      "new w tensor([[-0.4242,  0.8389,  0.6952],\n",
      "        [-0.1869,  0.8071,  0.9691]], requires_grad=True)\n",
      "new b tensor([  2.4252, -14.1689], requires_grad=True)\n",
      "Loss= tensor(1.2347, grad_fn=<DivBackward0>)\n",
      "i= 768\n",
      "w.grad= tensor([[-0.0986, -0.8064,  2.3318],\n",
      "        [-1.9100,  0.8473, -3.9838]])\n",
      "b.grad= tensor([-48.8536, 306.8480])\n",
      "new w tensor([[-0.4242,  0.8389,  0.6951],\n",
      "        [-0.1868,  0.8071,  0.9693]], requires_grad=True)\n",
      "new b tensor([  2.4276, -14.1843], requires_grad=True)\n",
      "Loss= tensor(1.2356, grad_fn=<DivBackward0>)\n",
      "i= 769\n",
      "w.grad= tensor([[-0.0977, -0.8038,  2.3216],\n",
      "        [-1.9114,  0.8434, -3.9745]])\n",
      "b.grad= tensor([-48.8058, 306.7091])\n",
      "new w tensor([[-0.4242,  0.8389,  0.6950],\n",
      "        [-0.1867,  0.8070,  0.9695]], requires_grad=True)\n",
      "new b tensor([  2.4301, -14.1996], requires_grad=True)\n",
      "Loss= tensor(1.2366, grad_fn=<DivBackward0>)\n",
      "i= 770\n",
      "w.grad= tensor([[-0.0950, -0.7992,  2.3127],\n",
      "        [-1.9119,  0.8405, -3.9646]])\n",
      "b.grad= tensor([-48.7580, 306.5702])\n",
      "new w tensor([[-0.4242,  0.8390,  0.6949],\n",
      "        [-0.1866,  0.8070,  0.9697]], requires_grad=True)\n",
      "new b tensor([  2.4325, -14.2149], requires_grad=True)\n",
      "Loss= tensor(1.2375, grad_fn=<DivBackward0>)\n",
      "i= 771\n",
      "w.grad= tensor([[-0.0939, -0.7964,  2.3028],\n",
      "        [-1.9131,  0.8368, -3.9552]])\n",
      "b.grad= tensor([-48.7102, 306.4312])\n",
      "new w tensor([[-0.4242,  0.8390,  0.6948],\n",
      "        [-0.1865,  0.8069,  0.9699]], requires_grad=True)\n",
      "new b tensor([  2.4349, -14.2303], requires_grad=True)\n",
      "Loss= tensor(1.2384, grad_fn=<DivBackward0>)\n",
      "i= 772\n",
      "w.grad= tensor([[-0.0917, -0.7923,  2.2937],\n",
      "        [-1.9141,  0.8337, -3.9456]])\n",
      "b.grad= tensor([-48.6624, 306.2921])\n",
      "new w tensor([[-0.4242,  0.8391,  0.6947],\n",
      "        [-0.1864,  0.8069,  0.9700]], requires_grad=True)\n",
      "new b tensor([  2.4374, -14.2456], requires_grad=True)\n",
      "Loss= tensor(1.2394, grad_fn=<DivBackward0>)\n",
      "i= 773\n",
      "w.grad= tensor([[-0.0896, -0.7885,  2.2845],\n",
      "        [-1.9154,  0.8297, -3.9364]])\n",
      "b.grad= tensor([-48.6147, 306.1530])\n",
      "new w tensor([[-0.4242,  0.8391,  0.6945],\n",
      "        [-0.1863,  0.8068,  0.9702]], requires_grad=True)\n",
      "new b tensor([  2.4398, -14.2609], requires_grad=True)\n",
      "Loss= tensor(1.2404, grad_fn=<DivBackward0>)\n",
      "i= 774\n",
      "w.grad= tensor([[-0.0881, -0.7851,  2.2751],\n",
      "        [-1.9159,  0.8269, -3.9266]])\n",
      "b.grad= tensor([-48.5669, 306.0137])\n",
      "new w tensor([[-0.4242,  0.8391,  0.6944],\n",
      "        [-0.1862,  0.8068,  0.9704]], requires_grad=True)\n",
      "new b tensor([  2.4422, -14.2762], requires_grad=True)\n",
      "Loss= tensor(1.2413, grad_fn=<DivBackward0>)\n",
      "i= 775\n",
      "w.grad= tensor([[-0.0860, -0.7813,  2.2660],\n",
      "        [-1.9175,  0.8230, -3.9177]])\n",
      "b.grad= tensor([-48.5193, 305.8744])\n",
      "new w tensor([[-0.4242,  0.8392,  0.6943],\n",
      "        [-0.1861,  0.8068,  0.9706]], requires_grad=True)\n",
      "new b tensor([  2.4447, -14.2915], requires_grad=True)\n",
      "Loss= tensor(1.2423, grad_fn=<DivBackward0>)\n",
      "i= 776\n",
      "w.grad= tensor([[-0.0849, -0.7785,  2.2563],\n",
      "        [-1.9179,  0.8202, -3.9081]])\n",
      "b.grad= tensor([-48.4716, 305.7350])\n",
      "new w tensor([[-0.4242,  0.8392,  0.6942],\n",
      "        [-0.1860,  0.8067,  0.9708]], requires_grad=True)\n",
      "new b tensor([  2.4471, -14.3068], requires_grad=True)\n",
      "Loss= tensor(1.2433, grad_fn=<DivBackward0>)\n",
      "i= 777\n",
      "w.grad= tensor([[-0.0830, -0.7748,  2.2473],\n",
      "        [-1.9192,  0.8166, -3.8990]])\n",
      "b.grad= tensor([-48.4240, 305.5956])\n",
      "new w tensor([[-0.4242,  0.8393,  0.6941],\n",
      "        [-0.1859,  0.8067,  0.9710]], requires_grad=True)\n",
      "new b tensor([  2.4495, -14.3220], requires_grad=True)\n",
      "Loss= tensor(1.2443, grad_fn=<DivBackward0>)\n",
      "i= 778\n",
      "w.grad= tensor([[-0.0811, -0.7712,  2.2382],\n",
      "        [-1.9200,  0.8134, -3.8897]])\n",
      "b.grad= tensor([-48.3764, 305.4561])\n",
      "new w tensor([[-0.4242,  0.8393,  0.6940],\n",
      "        [-0.1858,  0.8066,  0.9712]], requires_grad=True)\n",
      "new b tensor([  2.4519, -14.3373], requires_grad=True)\n",
      "Loss= tensor(1.2452, grad_fn=<DivBackward0>)\n",
      "i= 779\n",
      "w.grad= tensor([[-0.0795, -0.7678,  2.2290],\n",
      "        [-1.9206,  0.8104, -3.8803]])\n",
      "b.grad= tensor([-48.3288, 305.3165])\n",
      "new w tensor([[-0.4242,  0.8393,  0.6939],\n",
      "        [-0.1857,  0.8066,  0.9714]], requires_grad=True)\n",
      "new b tensor([  2.4543, -14.3526], requires_grad=True)\n",
      "Loss= tensor(1.2462, grad_fn=<DivBackward0>)\n",
      "i= 780\n",
      "w.grad= tensor([[-0.0778, -0.7644,  2.2200],\n",
      "        [-1.9218,  0.8067, -3.8713]])\n",
      "b.grad= tensor([-48.2812, 305.1769])\n",
      "new w tensor([[-0.4242,  0.8394,  0.6938],\n",
      "        [-0.1856,  0.8066,  0.9716]], requires_grad=True)\n",
      "new b tensor([  2.4568, -14.3678], requires_grad=True)\n",
      "Loss= tensor(1.2472, grad_fn=<DivBackward0>)\n",
      "i= 781\n",
      "w.grad= tensor([[-0.0757, -0.7606,  2.2112],\n",
      "        [-1.9232,  0.8030, -3.8624]])\n",
      "b.grad= tensor([-48.2337, 305.0371])\n",
      "new w tensor([[-0.4242,  0.8394,  0.6937],\n",
      "        [-0.1856,  0.8065,  0.9718]], requires_grad=True)\n",
      "new b tensor([  2.4592, -14.3831], requires_grad=True)\n",
      "Loss= tensor(1.2482, grad_fn=<DivBackward0>)\n",
      "i= 782\n",
      "w.grad= tensor([[-0.0742, -0.7574,  2.2020],\n",
      "        [-1.9239,  0.7999, -3.8533]])\n",
      "b.grad= tensor([-48.1862, 304.8973])\n",
      "new w tensor([[-0.4242,  0.8395,  0.6935],\n",
      "        [-0.1855,  0.8065,  0.9720]], requires_grad=True)\n",
      "new b tensor([  2.4616, -14.3983], requires_grad=True)\n",
      "Loss= tensor(1.2492, grad_fn=<DivBackward0>)\n",
      "i= 783\n",
      "w.grad= tensor([[-0.0726, -0.7540,  2.1931],\n",
      "        [-1.9242,  0.7975, -3.8437]])\n",
      "b.grad= tensor([-48.1387, 304.7575])\n",
      "new w tensor([[-0.4242,  0.8395,  0.6934],\n",
      "        [-0.1854,  0.8064,  0.9722]], requires_grad=True)\n",
      "new b tensor([  2.4640, -14.4136], requires_grad=True)\n",
      "Loss= tensor(1.2503, grad_fn=<DivBackward0>)\n",
      "i= 784\n",
      "w.grad= tensor([[-0.0708, -0.7506,  2.1842],\n",
      "        [-1.9255,  0.7936, -3.8350]])\n",
      "b.grad= tensor([-48.0913, 304.6175])\n",
      "new w tensor([[-0.4242,  0.8395,  0.6933],\n",
      "        [-0.1853,  0.8064,  0.9724]], requires_grad=True)\n",
      "new b tensor([  2.4664, -14.4288], requires_grad=True)\n",
      "Loss= tensor(1.2513, grad_fn=<DivBackward0>)\n",
      "i= 785\n",
      "w.grad= tensor([[-0.0689, -0.7469,  2.1756],\n",
      "        [-1.9265,  0.7903, -3.8261]])\n",
      "b.grad= tensor([-48.0439, 304.4775])\n",
      "new w tensor([[-0.4242,  0.8396,  0.6932],\n",
      "        [-0.1852,  0.8064,  0.9726]], requires_grad=True)\n",
      "new b tensor([  2.4688, -14.4440], requires_grad=True)\n",
      "Loss= tensor(1.2523, grad_fn=<DivBackward0>)\n",
      "i= 786\n",
      "w.grad= tensor([[-0.0671, -0.7432,  2.1669],\n",
      "        [-1.9274,  0.7871, -3.8172]])\n",
      "b.grad= tensor([-47.9965, 304.3374])\n",
      "new w tensor([[-0.4242,  0.8396,  0.6931],\n",
      "        [-0.1851,  0.8063,  0.9728]], requires_grad=True)\n",
      "new b tensor([  2.4712, -14.4592], requires_grad=True)\n",
      "Loss= tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "i= 787\n",
      "w.grad= tensor([[-0.0663, -0.7411,  2.1574],\n",
      "        [-1.9274,  0.7847, -3.8077]])\n",
      "b.grad= tensor([-47.9491, 304.1973])\n",
      "new w tensor([[-0.4241,  0.8396,  0.6930],\n",
      "        [-0.1850,  0.8063,  0.9730]], requires_grad=True)\n",
      "new b tensor([  2.4736, -14.4745], requires_grad=True)\n",
      "Loss= tensor(1.2544, grad_fn=<DivBackward0>)\n",
      "i= 788\n",
      "w.grad= tensor([[-0.0642, -0.7372,  2.1491],\n",
      "        [-1.9290,  0.7808, -3.7993]])\n",
      "b.grad= tensor([-47.9018, 304.0571])\n",
      "new w tensor([[-0.4241,  0.8397,  0.6929],\n",
      "        [-0.1849,  0.8062,  0.9731]], requires_grad=True)\n",
      "new b tensor([  2.4760, -14.4897], requires_grad=True)\n",
      "Loss= tensor(1.2554, grad_fn=<DivBackward0>)\n",
      "i= 789\n",
      "w.grad= tensor([[-0.0628, -0.7342,  2.1402],\n",
      "        [-1.9293,  0.7783, -3.7901]])\n",
      "b.grad= tensor([-47.8545, 303.9168])\n",
      "new w tensor([[-0.4241,  0.8397,  0.6928],\n",
      "        [-0.1848,  0.8062,  0.9733]], requires_grad=True)\n",
      "new b tensor([  2.4784, -14.5049], requires_grad=True)\n",
      "Loss= tensor(1.2565, grad_fn=<DivBackward0>)\n",
      "i= 790\n",
      "w.grad= tensor([[-0.0606, -0.7302,  2.1319],\n",
      "        [-1.9305,  0.7747, -3.7816]])\n",
      "b.grad= tensor([-47.8072, 303.7764])\n",
      "new w tensor([[-0.4241,  0.8397,  0.6927],\n",
      "        [-0.1847,  0.8062,  0.9735]], requires_grad=True)\n",
      "new b tensor([  2.4808, -14.5200], requires_grad=True)\n",
      "Loss= tensor(1.2575, grad_fn=<DivBackward0>)\n",
      "i= 791\n",
      "w.grad= tensor([[-0.0588, -0.7267,  2.1234],\n",
      "        [-1.9314,  0.7715, -3.7729]])\n",
      "b.grad= tensor([-47.7599, 303.6360])\n",
      "new w tensor([[-0.4241,  0.8398,  0.6926],\n",
      "        [-0.1846,  0.8061,  0.9737]], requires_grad=True)\n",
      "new b tensor([  2.4832, -14.5352], requires_grad=True)\n",
      "Loss= tensor(1.2586, grad_fn=<DivBackward0>)\n",
      "i= 792\n",
      "w.grad= tensor([[-0.0571, -0.7234,  2.1149],\n",
      "        [-1.9319,  0.7687, -3.7639]])\n",
      "b.grad= tensor([-47.7126, 303.4954])\n",
      "new w tensor([[-0.4241,  0.8398,  0.6925],\n",
      "        [-0.1845,  0.8061,  0.9739]], requires_grad=True)\n",
      "new b tensor([  2.4855, -14.5504], requires_grad=True)\n",
      "Loss= tensor(1.2597, grad_fn=<DivBackward0>)\n",
      "i= 793\n",
      "w.grad= tensor([[-0.0557, -0.7204,  2.1062],\n",
      "        [-1.9331,  0.7653, -3.7555]])\n",
      "b.grad= tensor([-47.6654, 303.3548])\n",
      "new w tensor([[-0.4241,  0.8399,  0.6924],\n",
      "        [-0.1844,  0.8060,  0.9741]], requires_grad=True)\n",
      "new b tensor([  2.4879, -14.5656], requires_grad=True)\n",
      "Loss= tensor(1.2607, grad_fn=<DivBackward0>)\n",
      "i= 794\n",
      "w.grad= tensor([[-0.0545, -0.7175,  2.0975],\n",
      "        [-1.9336,  0.7625, -3.7467]])\n",
      "b.grad= tensor([-47.6182, 303.2142])\n",
      "new w tensor([[-0.4241,  0.8399,  0.6923],\n",
      "        [-0.1843,  0.8060,  0.9743]], requires_grad=True)\n",
      "new b tensor([  2.4903, -14.5807], requires_grad=True)\n",
      "Loss= tensor(1.2618, grad_fn=<DivBackward0>)\n",
      "i= 795\n",
      "w.grad= tensor([[-0.0527, -0.7139,  2.0892],\n",
      "        [-1.9348,  0.7589, -3.7383]])\n",
      "b.grad= tensor([-47.5710, 303.0734])\n",
      "new w tensor([[-0.4241,  0.8399,  0.6921],\n",
      "        [-0.1842,  0.8060,  0.9745]], requires_grad=True)\n",
      "new b tensor([  2.4927, -14.5959], requires_grad=True)\n",
      "Loss= tensor(1.2629, grad_fn=<DivBackward0>)\n",
      "i= 796\n",
      "w.grad= tensor([[-0.0515, -0.7111,  2.0805],\n",
      "        [-1.9357,  0.7556, -3.7299]])\n",
      "b.grad= tensor([-47.5239, 302.9326])\n",
      "new w tensor([[-0.4241,  0.8400,  0.6920],\n",
      "        [-0.1841,  0.8059,  0.9746]], requires_grad=True)\n",
      "new b tensor([  2.4951, -14.6110], requires_grad=True)\n",
      "Loss= tensor(1.2640, grad_fn=<DivBackward0>)\n",
      "i= 797\n",
      "w.grad= tensor([[-0.0497, -0.7077,  2.0723],\n",
      "        [-1.9354,  0.7539, -3.7207]])\n",
      "b.grad= tensor([-47.4768, 302.7917])\n",
      "new w tensor([[-0.4241,  0.8400,  0.6919],\n",
      "        [-0.1840,  0.8059,  0.9748]], requires_grad=True)\n",
      "new b tensor([  2.4974, -14.6262], requires_grad=True)\n",
      "Loss= tensor(1.2651, grad_fn=<DivBackward0>)\n",
      "i= 798\n",
      "w.grad= tensor([[-0.0476, -0.7039,  2.0642],\n",
      "        [-1.9363,  0.7506, -3.7122]])\n",
      "b.grad= tensor([-47.4296, 302.6508])\n",
      "new w tensor([[-0.4241,  0.8400,  0.6918],\n",
      "        [-0.1839,  0.8059,  0.9750]], requires_grad=True)\n",
      "new b tensor([  2.4998, -14.6413], requires_grad=True)\n",
      "Loss= tensor(1.2662, grad_fn=<DivBackward0>)\n",
      "i= 799\n",
      "w.grad= tensor([[-0.0470, -0.7019,  2.0553],\n",
      "        [-1.9379,  0.7467, -3.7044]])\n",
      "b.grad= tensor([-47.3826, 302.5097])\n",
      "new w tensor([[-0.4241,  0.8401,  0.6917],\n",
      "        [-0.1838,  0.8058,  0.9752]], requires_grad=True)\n",
      "new b tensor([  2.5022, -14.6564], requires_grad=True)\n",
      "Loss= tensor(1.2673, grad_fn=<DivBackward0>)\n",
      "i= 800\n",
      "w.grad= tensor([[-0.0451, -0.6983,  2.0473],\n",
      "        [-1.9375,  0.7448, -3.6951]])\n",
      "b.grad= tensor([-47.3355, 302.3686])\n",
      "new w tensor([[-0.4241,  0.8401,  0.6916],\n",
      "        [-0.1837,  0.8058,  0.9754]], requires_grad=True)\n",
      "new b tensor([  2.5045, -14.6715], requires_grad=True)\n",
      "Loss= tensor(1.2684, grad_fn=<DivBackward0>)\n",
      "i= 801\n",
      "w.grad= tensor([[-0.0437, -0.6953,  2.0390],\n",
      "        [-1.9388,  0.7414, -3.6871]])\n",
      "b.grad= tensor([-47.2885, 302.2274])\n",
      "new w tensor([[-0.4241,  0.8401,  0.6915],\n",
      "        [-0.1836,  0.8057,  0.9756]], requires_grad=True)\n",
      "new b tensor([  2.5069, -14.6867], requires_grad=True)\n",
      "Loss= tensor(1.2695, grad_fn=<DivBackward0>)\n",
      "i= 802\n",
      "w.grad= tensor([[-0.0419, -0.6918,  2.0310],\n",
      "        [-1.9397,  0.7382, -3.6788]])\n",
      "b.grad= tensor([-47.2415, 302.0862])\n",
      "new w tensor([[-0.4241,  0.8402,  0.6914],\n",
      "        [-0.1835,  0.8057,  0.9758]], requires_grad=True)\n",
      "new b tensor([  2.5093, -14.7018], requires_grad=True)\n",
      "Loss= tensor(1.2706, grad_fn=<DivBackward0>)\n",
      "i= 803\n",
      "w.grad= tensor([[-0.0405, -0.6889,  2.0227],\n",
      "        [-1.9403,  0.7353, -3.6704]])\n",
      "b.grad= tensor([-47.1945, 301.9449])\n",
      "new w tensor([[-0.4241,  0.8402,  0.6913],\n",
      "        [-0.1834,  0.8057,  0.9759]], requires_grad=True)\n",
      "new b tensor([  2.5116, -14.7169], requires_grad=True)\n",
      "Loss= tensor(1.2717, grad_fn=<DivBackward0>)\n",
      "i= 804\n",
      "w.grad= tensor([[-0.0392, -0.6860,  2.0144],\n",
      "        [-1.9404,  0.7331, -3.6618]])\n",
      "b.grad= tensor([-47.1475, 301.8035])\n",
      "new w tensor([[-0.4241,  0.8402,  0.6912],\n",
      "        [-0.1833,  0.8056,  0.9761]], requires_grad=True)\n",
      "new b tensor([  2.5140, -14.7319], requires_grad=True)\n",
      "Loss= tensor(1.2729, grad_fn=<DivBackward0>)\n",
      "i= 805\n",
      "w.grad= tensor([[-0.0377, -0.6830,  2.0063],\n",
      "        [-1.9414,  0.7299, -3.6536]])\n",
      "b.grad= tensor([-47.1006, 301.6620])\n",
      "new w tensor([[-0.4241,  0.8403,  0.6911],\n",
      "        [-0.1832,  0.8056,  0.9763]], requires_grad=True)\n",
      "new b tensor([  2.5163, -14.7470], requires_grad=True)\n",
      "Loss= tensor(1.2740, grad_fn=<DivBackward0>)\n",
      "i= 806\n",
      "w.grad= tensor([[-0.0359, -0.6796,  1.9984],\n",
      "        [-1.9425,  0.7266, -3.6457]])\n",
      "b.grad= tensor([-47.0537, 301.5204])\n",
      "new w tensor([[-0.4241,  0.8403,  0.6910],\n",
      "        [-0.1831,  0.8056,  0.9765]], requires_grad=True)\n",
      "new b tensor([  2.5187, -14.7621], requires_grad=True)\n",
      "Loss= tensor(1.2752, grad_fn=<DivBackward0>)\n",
      "i= 807\n",
      "w.grad= tensor([[-0.0345, -0.6766,  1.9904],\n",
      "        [-1.9428,  0.7241, -3.6372]])\n",
      "b.grad= tensor([-47.0068, 301.3788])\n",
      "new w tensor([[-0.4241,  0.8403,  0.6909],\n",
      "        [-0.1830,  0.8055,  0.9767]], requires_grad=True)\n",
      "new b tensor([  2.5210, -14.7772], requires_grad=True)\n",
      "Loss= tensor(1.2763, grad_fn=<DivBackward0>)\n",
      "i= 808\n",
      "w.grad= tensor([[-0.0330, -0.6735,  1.9824],\n",
      "        [-1.9433,  0.7212, -3.6290]])\n",
      "b.grad= tensor([-46.9599, 301.2371])\n",
      "new w tensor([[-0.4241,  0.8404,  0.6908],\n",
      "        [-0.1829,  0.8055,  0.9769]], requires_grad=True)\n",
      "new b tensor([  2.5234, -14.7922], requires_grad=True)\n",
      "Loss= tensor(1.2774, grad_fn=<DivBackward0>)\n",
      "i= 809\n",
      "w.grad= tensor([[-0.0317, -0.6706,  1.9744],\n",
      "        [-1.9442,  0.7180, -3.6211]])\n",
      "b.grad= tensor([-46.9130, 301.0953])\n",
      "new w tensor([[-0.4241,  0.8404,  0.6907],\n",
      "        [-0.1828,  0.8055,  0.9770]], requires_grad=True)\n",
      "new b tensor([  2.5257, -14.8073], requires_grad=True)\n",
      "Loss= tensor(1.2786, grad_fn=<DivBackward0>)\n",
      "i= 810\n",
      "w.grad= tensor([[-0.0295, -0.6667,  1.9670],\n",
      "        [-1.9445,  0.7157, -3.6127]])\n",
      "b.grad= tensor([-46.8662, 300.9535])\n",
      "new w tensor([[-0.4241,  0.8404,  0.6906],\n",
      "        [-0.1827,  0.8054,  0.9772]], requires_grad=True)\n",
      "new b tensor([  2.5281, -14.8223], requires_grad=True)\n",
      "Loss= tensor(1.2798, grad_fn=<DivBackward0>)\n",
      "i= 811\n",
      "w.grad= tensor([[-0.0291, -0.6650,  1.9584],\n",
      "        [-1.9449,  0.7131, -3.6045]])\n",
      "b.grad= tensor([-46.8194, 300.8115])\n",
      "new w tensor([[-0.4241,  0.8405,  0.6905],\n",
      "        [-0.1826,  0.8054,  0.9774]], requires_grad=True)\n",
      "new b tensor([  2.5304, -14.8374], requires_grad=True)\n",
      "Loss= tensor(1.2809, grad_fn=<DivBackward0>)\n",
      "i= 812\n",
      "w.grad= tensor([[-0.0272, -0.6614,  1.9509],\n",
      "        [-1.9460,  0.7096, -3.5968]])\n",
      "b.grad= tensor([-46.7726, 300.6695])\n",
      "new w tensor([[-0.4241,  0.8405,  0.6904],\n",
      "        [-0.1826,  0.8053,  0.9776]], requires_grad=True)\n",
      "new b tensor([  2.5328, -14.8524], requires_grad=True)\n",
      "Loss= tensor(1.2821, grad_fn=<DivBackward0>)\n",
      "i= 813\n",
      "w.grad= tensor([[-0.0261, -0.6589,  1.9429],\n",
      "        [-1.9467,  0.7068, -3.5888]])\n",
      "b.grad= tensor([-46.7258, 300.5274])\n",
      "new w tensor([[-0.4241,  0.8405,  0.6903],\n",
      "        [-0.1825,  0.8053,  0.9778]], requires_grad=True)\n",
      "new b tensor([  2.5351, -14.8674], requires_grad=True)\n",
      "Loss= tensor(1.2833, grad_fn=<DivBackward0>)\n",
      "i= 814\n",
      "w.grad= tensor([[-0.0245, -0.6558,  1.9352],\n",
      "        [-1.9470,  0.7043, -3.5807]])\n",
      "b.grad= tensor([-46.6791, 300.3853])\n",
      "new w tensor([[-0.4241,  0.8406,  0.6902],\n",
      "        [-0.1824,  0.8053,  0.9779]], requires_grad=True)\n",
      "new b tensor([  2.5374, -14.8825], requires_grad=True)\n",
      "Loss= tensor(1.2844, grad_fn=<DivBackward0>)\n",
      "i= 815\n",
      "w.grad= tensor([[-0.0227, -0.6524,  1.9277],\n",
      "        [-1.9476,  0.7017, -3.5727]])\n",
      "b.grad= tensor([-46.6324, 300.2430])\n",
      "new w tensor([[-0.4241,  0.8406,  0.6901],\n",
      "        [-0.1823,  0.8052,  0.9781]], requires_grad=True)\n",
      "new b tensor([  2.5398, -14.8975], requires_grad=True)\n",
      "Loss= tensor(1.2856, grad_fn=<DivBackward0>)\n",
      "i= 816\n",
      "w.grad= tensor([[-0.0215, -0.6497,  1.9199],\n",
      "        [-1.9486,  0.6985, -3.5651]])\n",
      "b.grad= tensor([-46.5857, 300.1007])\n",
      "new w tensor([[-0.4241,  0.8406,  0.6901],\n",
      "        [-0.1822,  0.8052,  0.9783]], requires_grad=True)\n",
      "new b tensor([  2.5421, -14.9125], requires_grad=True)\n",
      "Loss= tensor(1.2868, grad_fn=<DivBackward0>)\n",
      "i= 817\n",
      "w.grad= tensor([[-0.0205, -0.6473,  1.9120],\n",
      "        [-1.9490,  0.6959, -3.5571]])\n",
      "b.grad= tensor([-46.5390, 299.9583])\n",
      "new w tensor([[-0.4241,  0.8407,  0.6900],\n",
      "        [-0.1821,  0.8052,  0.9785]], requires_grad=True)\n",
      "new b tensor([  2.5444, -14.9275], requires_grad=True)\n",
      "Loss= tensor(1.2880, grad_fn=<DivBackward0>)\n",
      "i= 818\n",
      "w.grad= tensor([[-0.0187, -0.6439,  1.9046],\n",
      "        [-1.9498,  0.6929, -3.5494]])\n",
      "b.grad= tensor([-46.4923, 299.8159])\n",
      "new w tensor([[-0.4241,  0.8407,  0.6899],\n",
      "        [-0.1820,  0.8051,  0.9786]], requires_grad=True)\n",
      "new b tensor([  2.5467, -14.9425], requires_grad=True)\n",
      "Loss= tensor(1.2892, grad_fn=<DivBackward0>)\n",
      "i= 819\n",
      "w.grad= tensor([[-0.0177, -0.6414,  1.8967],\n",
      "        [-1.9502,  0.6903, -3.5416]])\n",
      "b.grad= tensor([-46.4457, 299.6733])\n",
      "new w tensor([[-0.4241,  0.8407,  0.6898],\n",
      "        [-0.1819,  0.8051,  0.9788]], requires_grad=True)\n",
      "new b tensor([  2.5491, -14.9574], requires_grad=True)\n",
      "Loss= tensor(1.2904, grad_fn=<DivBackward0>)\n",
      "i= 820\n",
      "w.grad= tensor([[-0.0162, -0.6384,  1.8893],\n",
      "        [-1.9508,  0.6875, -3.5338]])\n",
      "b.grad= tensor([-46.3991, 299.5307])\n",
      "new w tensor([[-0.4241,  0.8408,  0.6897],\n",
      "        [-0.1818,  0.8051,  0.9790]], requires_grad=True)\n",
      "new b tensor([  2.5514, -14.9724], requires_grad=True)\n",
      "Loss= tensor(1.2916, grad_fn=<DivBackward0>)\n",
      "i= 821\n",
      "w.grad= tensor([[-0.0146, -0.6353,  1.8819],\n",
      "        [-1.9514,  0.6847, -3.5261]])\n",
      "b.grad= tensor([-46.3525, 299.3880])\n",
      "new w tensor([[-0.4241,  0.8408,  0.6896],\n",
      "        [-0.1817,  0.8050,  0.9792]], requires_grad=True)\n",
      "new b tensor([  2.5537, -14.9874], requires_grad=True)\n",
      "Loss= tensor(1.2928, grad_fn=<DivBackward0>)\n",
      "i= 822\n",
      "w.grad= tensor([[-0.0131, -0.6322,  1.8745],\n",
      "        [-1.9519,  0.6822, -3.5183]])\n",
      "b.grad= tensor([-46.3059, 299.2453])\n",
      "new w tensor([[-0.4241,  0.8408,  0.6895],\n",
      "        [-0.1816,  0.8050,  0.9793]], requires_grad=True)\n",
      "new b tensor([  2.5560, -15.0024], requires_grad=True)\n",
      "Loss= tensor(1.2940, grad_fn=<DivBackward0>)\n",
      "i= 823\n",
      "w.grad= tensor([[-0.0119, -0.6297,  1.8669],\n",
      "        [-1.9524,  0.6793, -3.5107]])\n",
      "b.grad= tensor([-46.2593, 299.1024])\n",
      "new w tensor([[-0.4241,  0.8409,  0.6894],\n",
      "        [-0.1815,  0.8050,  0.9795]], requires_grad=True)\n",
      "new b tensor([  2.5583, -15.0173], requires_grad=True)\n",
      "Loss= tensor(1.2952, grad_fn=<DivBackward0>)\n",
      "i= 824\n",
      "w.grad= tensor([[-0.0104, -0.6267,  1.8596],\n",
      "        [-1.9524,  0.6775, -3.5025]])\n",
      "b.grad= tensor([-46.2128, 298.9595])\n",
      "new w tensor([[-0.4241,  0.8409,  0.6893],\n",
      "        [-0.1814,  0.8049,  0.9797]], requires_grad=True)\n",
      "new b tensor([  2.5606, -15.0323], requires_grad=True)\n",
      "Loss= tensor(1.2965, grad_fn=<DivBackward0>)\n",
      "i= 825\n",
      "w.grad= tensor([[-0.0094, -0.6244,  1.8520],\n",
      "        [-1.9533,  0.6744, -3.4952]])\n",
      "b.grad= tensor([-46.1663, 298.8165])\n",
      "new w tensor([[-0.4241,  0.8409,  0.6892],\n",
      "        [-0.1813,  0.8049,  0.9799]], requires_grad=True)\n",
      "new b tensor([  2.5630, -15.0472], requires_grad=True)\n",
      "Loss= tensor(1.2977, grad_fn=<DivBackward0>)\n",
      "i= 826\n",
      "w.grad= tensor([[-0.0077, -0.6210,  1.8449],\n",
      "        [-1.9543,  0.6711, -3.4880]])\n",
      "b.grad= tensor([-46.1198, 298.6735])\n",
      "new w tensor([[-0.4241,  0.8410,  0.6891],\n",
      "        [-0.1812,  0.8049,  0.9800]], requires_grad=True)\n",
      "new b tensor([  2.5653, -15.0621], requires_grad=True)\n",
      "Loss= tensor(1.2989, grad_fn=<DivBackward0>)\n",
      "i= 827\n",
      "w.grad= tensor([[-0.0068, -0.6188,  1.8372],\n",
      "        [-1.9542,  0.6692, -3.4800]])\n",
      "b.grad= tensor([-46.0733, 298.5303])\n",
      "new w tensor([[-0.4241,  0.8410,  0.6890],\n",
      "        [-0.1811,  0.8048,  0.9802]], requires_grad=True)\n",
      "new b tensor([  2.5676, -15.0771], requires_grad=True)\n",
      "Loss= tensor(1.3001, grad_fn=<DivBackward0>)\n",
      "i= 828\n",
      "w.grad= tensor([[-0.0052, -0.6157,  1.8301],\n",
      "        [-1.9548,  0.6665, -3.4726]])\n",
      "b.grad= tensor([-46.0268, 298.3871])\n",
      "new w tensor([[-0.4241,  0.8410,  0.6889],\n",
      "        [-0.1810,  0.8048,  0.9804]], requires_grad=True)\n",
      "new b tensor([  2.5699, -15.0920], requires_grad=True)\n",
      "Loss= tensor(1.3014, grad_fn=<DivBackward0>)\n",
      "i= 829\n",
      "w.grad= tensor([[-0.0043, -0.6134,  1.8226],\n",
      "        [-1.9551,  0.6641, -3.4650]])\n",
      "b.grad= tensor([-45.9804, 298.2438])\n",
      "new w tensor([[-0.4241,  0.8411,  0.6888],\n",
      "        [-0.1809,  0.8048,  0.9806]], requires_grad=True)\n",
      "new b tensor([  2.5722, -15.1069], requires_grad=True)\n",
      "Loss= tensor(1.3026, grad_fn=<DivBackward0>)\n",
      "i= 830\n",
      "w.grad= tensor([[-2.8009e-03, -6.1044e-01,  1.8155e+00],\n",
      "        [-1.9559e+00,  6.6109e-01, -3.4578e+00]])\n",
      "b.grad= tensor([-45.9339, 298.1005])\n",
      "new w tensor([[-0.4241,  0.8411,  0.6887],\n",
      "        [-0.1808,  0.8047,  0.9807]], requires_grad=True)\n",
      "new b tensor([  2.5745, -15.1218], requires_grad=True)\n",
      "Loss= tensor(1.3039, grad_fn=<DivBackward0>)\n",
      "i= 831\n",
      "w.grad= tensor([[-1.1339e-03, -6.0729e-01,  1.8086e+00],\n",
      "        [-1.9566e+00,  6.5826e-01, -3.4505e+00]])\n",
      "b.grad= tensor([-45.8875, 297.9570])\n",
      "new w tensor([[-0.4241,  0.8411,  0.6887],\n",
      "        [-0.1807,  0.8047,  0.9809]], requires_grad=True)\n",
      "new b tensor([  2.5768, -15.1367], requires_grad=True)\n",
      "Loss= tensor(1.3051, grad_fn=<DivBackward0>)\n",
      "i= 832\n",
      "w.grad= tensor([[ 2.1935e-05, -6.0465e-01,  1.8013e+00],\n",
      "        [-1.9566e+00,  6.5607e-01, -3.4428e+00]])\n",
      "b.grad= tensor([-45.8411, 297.8135])\n",
      "new w tensor([[-0.4241,  0.8411,  0.6886],\n",
      "        [-0.1806,  0.8047,  0.9811]], requires_grad=True)\n",
      "new b tensor([  2.5790, -15.1516], requires_grad=True)\n",
      "Loss= tensor(1.3064, grad_fn=<DivBackward0>)\n",
      "i= 833\n",
      "w.grad= tensor([[ 1.2913e-03, -6.0199e-01,  1.7942e+00],\n",
      "        [-1.9571e+00,  6.5343e-01, -3.4355e+00]])\n",
      "b.grad= tensor([-45.7948, 297.6699])\n",
      "new w tensor([[-0.4241,  0.8412,  0.6885],\n",
      "        [-0.1805,  0.8046,  0.9813]], requires_grad=True)\n",
      "new b tensor([  2.5813, -15.1665], requires_grad=True)\n",
      "Loss= tensor(1.3076, grad_fn=<DivBackward0>)\n",
      "i= 834\n",
      "w.grad= tensor([[ 2.3737e-03, -5.9952e-01,  1.7870e+00],\n",
      "        [-1.9574e+00,  6.5109e-01, -3.4280e+00]])\n",
      "b.grad= tensor([-45.7484, 297.5262])\n",
      "new w tensor([[-0.4241,  0.8412,  0.6884],\n",
      "        [-0.1804,  0.8046,  0.9814]], requires_grad=True)\n",
      "new b tensor([  2.5836, -15.1813], requires_grad=True)\n",
      "Loss= tensor(1.3089, grad_fn=<DivBackward0>)\n",
      "i= 835\n",
      "w.grad= tensor([[ 0.0036, -0.5969,  1.7798],\n",
      "        [-1.9582,  0.6480, -3.4210]])\n",
      "b.grad= tensor([-45.7021, 297.3824])\n",
      "new w tensor([[-0.4241,  0.8412,  0.6883],\n",
      "        [-0.1803,  0.8046,  0.9816]], requires_grad=True)\n",
      "new b tensor([  2.5859, -15.1962], requires_grad=True)\n",
      "Loss= tensor(1.3102, grad_fn=<DivBackward0>)\n",
      "i= 836\n",
      "w.grad= tensor([[ 0.0053, -0.5937,  1.7732],\n",
      "        [-1.9579,  0.6465, -3.4131]])\n",
      "b.grad= tensor([-45.6557, 297.2386])\n",
      "new w tensor([[-0.4241,  0.8413,  0.6882],\n",
      "        [-0.1802,  0.8045,  0.9818]], requires_grad=True)\n",
      "new b tensor([  2.5882, -15.2111], requires_grad=True)\n",
      "Loss= tensor(1.3114, grad_fn=<DivBackward0>)\n",
      "i= 837\n",
      "w.grad= tensor([[ 0.0063, -0.5913,  1.7659],\n",
      "        [-1.9584,  0.6439, -3.4059]])\n",
      "b.grad= tensor([-45.6094, 297.0947])\n",
      "new w tensor([[-0.4241,  0.8413,  0.6881],\n",
      "        [-0.1801,  0.8045,  0.9819]], requires_grad=True)\n",
      "new b tensor([  2.5905, -15.2259], requires_grad=True)\n",
      "Loss= tensor(1.3127, grad_fn=<DivBackward0>)\n",
      "i= 838\n",
      "w.grad= tensor([[ 0.0073, -0.5889,  1.7588],\n",
      "        [-1.9600,  0.6401, -3.3995]])\n",
      "b.grad= tensor([-45.5632, 296.9507])\n",
      "new w tensor([[-0.4241,  0.8413,  0.6880],\n",
      "        [-0.1800,  0.8045,  0.9821]], requires_grad=True)\n",
      "new b tensor([  2.5927, -15.2408], requires_grad=True)\n",
      "Loss= tensor(1.3140, grad_fn=<DivBackward0>)\n",
      "i= 839\n",
      "w.grad= tensor([[ 0.0090, -0.5858,  1.7522],\n",
      "        [-1.9592,  0.6388, -3.3915]])\n",
      "b.grad= tensor([-45.5169, 296.8066])\n",
      "new w tensor([[-0.4241,  0.8413,  0.6879],\n",
      "        [-0.1799,  0.8044,  0.9823]], requires_grad=True)\n",
      "new b tensor([  2.5950, -15.2556], requires_grad=True)\n",
      "Loss= tensor(1.3153, grad_fn=<DivBackward0>)\n",
      "i= 840\n",
      "w.grad= tensor([[ 0.0097, -0.5837,  1.7449],\n",
      "        [-1.9599,  0.6363, -3.3845]])\n",
      "b.grad= tensor([-45.4707, 296.6625])\n",
      "new w tensor([[-0.4241,  0.8414,  0.6879],\n",
      "        [-0.1798,  0.8044,  0.9825]], requires_grad=True)\n",
      "new b tensor([  2.5973, -15.2705], requires_grad=True)\n",
      "Loss= tensor(1.3166, grad_fn=<DivBackward0>)\n",
      "i= 841\n",
      "w.grad= tensor([[ 0.0113, -0.5806,  1.7384],\n",
      "        [-1.9609,  0.6328, -3.3779]])\n",
      "b.grad= tensor([-45.4244, 296.5183])\n",
      "new w tensor([[-0.4241,  0.8414,  0.6878],\n",
      "        [-0.1797,  0.8044,  0.9826]], requires_grad=True)\n",
      "new b tensor([  2.5996, -15.2853], requires_grad=True)\n",
      "Loss= tensor(1.3179, grad_fn=<DivBackward0>)\n",
      "i= 842\n",
      "w.grad= tensor([[ 0.0125, -0.5782,  1.7314],\n",
      "        [-1.9604,  0.6314, -3.3701]])\n",
      "b.grad= tensor([-45.3782, 296.3740])\n",
      "new w tensor([[-0.4241,  0.8414,  0.6877],\n",
      "        [-0.1796,  0.8043,  0.9828]], requires_grad=True)\n",
      "new b tensor([  2.6018, -15.3001], requires_grad=True)\n",
      "Loss= tensor(1.3191, grad_fn=<DivBackward0>)\n",
      "i= 843\n",
      "w.grad= tensor([[ 0.0137, -0.5756,  1.7246],\n",
      "        [-1.9616,  0.6280, -3.3637]])\n",
      "b.grad= tensor([-45.3320, 296.2296])\n",
      "new w tensor([[-0.4241,  0.8415,  0.6876],\n",
      "        [-0.1795,  0.8043,  0.9830]], requires_grad=True)\n",
      "new b tensor([  2.6041, -15.3149], requires_grad=True)\n",
      "Loss= tensor(1.3204, grad_fn=<DivBackward0>)\n",
      "i= 844\n",
      "w.grad= tensor([[ 0.0144, -0.5736,  1.7175],\n",
      "        [-1.9615,  0.6261, -3.3563]])\n",
      "b.grad= tensor([-45.2858, 296.0852])\n",
      "new w tensor([[-0.4241,  0.8415,  0.6875],\n",
      "        [-0.1794,  0.8043,  0.9831]], requires_grad=True)\n",
      "new b tensor([  2.6064, -15.3297], requires_grad=True)\n",
      "Loss= tensor(1.3217, grad_fn=<DivBackward0>)\n",
      "i= 845\n",
      "w.grad= tensor([[ 0.0162, -0.5703,  1.7112],\n",
      "        [-1.9616,  0.6241, -3.3490]])\n",
      "b.grad= tensor([-45.2397, 295.9406])\n",
      "new w tensor([[-0.4241,  0.8415,  0.6874],\n",
      "        [-0.1793,  0.8043,  0.9833]], requires_grad=True)\n",
      "new b tensor([  2.6086, -15.3445], requires_grad=True)\n",
      "Loss= tensor(1.3231, grad_fn=<DivBackward0>)\n",
      "i= 846\n",
      "w.grad= tensor([[ 0.0167, -0.5686,  1.7040],\n",
      "        [-1.9629,  0.6206, -3.3427]])\n",
      "b.grad= tensor([-45.1935, 295.7960])\n",
      "new w tensor([[-0.4241,  0.8416,  0.6873],\n",
      "        [-0.1792,  0.8042,  0.9835]], requires_grad=True)\n",
      "new b tensor([  2.6109, -15.3593], requires_grad=True)\n",
      "Loss= tensor(1.3244, grad_fn=<DivBackward0>)\n",
      "i= 847\n",
      "w.grad= tensor([[ 0.0186, -0.5652,  1.6977],\n",
      "        [-1.9622,  0.6193, -3.3351]])\n",
      "b.grad= tensor([-45.1474, 295.6513])\n",
      "new w tensor([[-0.4241,  0.8416,  0.6873],\n",
      "        [-0.1791,  0.8042,  0.9836]], requires_grad=True)\n",
      "new b tensor([  2.6131, -15.3741], requires_grad=True)\n",
      "Loss= tensor(1.3257, grad_fn=<DivBackward0>)\n",
      "i= 848\n",
      "w.grad= tensor([[ 0.0192, -0.5633,  1.6907],\n",
      "        [-1.9636,  0.6157, -3.3288]])\n",
      "b.grad= tensor([-45.1013, 295.5066])\n",
      "new w tensor([[-0.4241,  0.8416,  0.6872],\n",
      "        [-0.1790,  0.8042,  0.9838]], requires_grad=True)\n",
      "new b tensor([  2.6154, -15.3889], requires_grad=True)\n",
      "Loss= tensor(1.3270, grad_fn=<DivBackward0>)\n",
      "i= 849\n",
      "w.grad= tensor([[ 0.0211, -0.5600,  1.6846],\n",
      "        [-1.9631,  0.6143, -3.3214]])\n",
      "b.grad= tensor([-45.0552, 295.3617])\n",
      "new w tensor([[-0.4241,  0.8416,  0.6871],\n",
      "        [-0.1789,  0.8041,  0.9840]], requires_grad=True)\n",
      "new b tensor([  2.6177, -15.4036], requires_grad=True)\n",
      "Loss= tensor(1.3283, grad_fn=<DivBackward0>)\n",
      "i= 850\n",
      "w.grad= tensor([[ 0.0218, -0.5580,  1.6776],\n",
      "        [-1.9627,  0.6126, -3.3140]])\n",
      "b.grad= tensor([-45.0091, 295.2168])\n",
      "new w tensor([[-0.4241,  0.8417,  0.6870],\n",
      "        [-0.1788,  0.8041,  0.9841]], requires_grad=True)\n",
      "new b tensor([  2.6199, -15.4184], requires_grad=True)\n",
      "Loss= tensor(1.3296, grad_fn=<DivBackward0>)\n",
      "i= 851\n",
      "w.grad= tensor([[ 0.0232, -0.5552,  1.6712],\n",
      "        [-1.9641,  0.6091, -3.3079]])\n",
      "b.grad= tensor([-44.9630, 295.0718])\n",
      "new w tensor([[-0.4241,  0.8417,  0.6869],\n",
      "        [-0.1787,  0.8041,  0.9843]], requires_grad=True)\n",
      "new b tensor([  2.6222, -15.4331], requires_grad=True)\n",
      "Loss= tensor(1.3310, grad_fn=<DivBackward0>)\n",
      "i= 852\n",
      "w.grad= tensor([[ 0.0241, -0.5530,  1.6645],\n",
      "        [-1.9643,  0.6069, -3.3010]])\n",
      "b.grad= tensor([-44.9170, 294.9268])\n",
      "new w tensor([[-0.4241,  0.8417,  0.6868],\n",
      "        [-0.1786,  0.8040,  0.9845]], requires_grad=True)\n",
      "new b tensor([  2.6244, -15.4479], requires_grad=True)\n",
      "Loss= tensor(1.3323, grad_fn=<DivBackward0>)\n",
      "i= 853\n",
      "w.grad= tensor([[ 0.0254, -0.5504,  1.6581],\n",
      "        [-1.9641,  0.6051, -3.2939]])\n",
      "b.grad= tensor([-44.8710, 294.7816])\n",
      "new w tensor([[-0.4241,  0.8417,  0.6868],\n",
      "        [-0.1785,  0.8040,  0.9846]], requires_grad=True)\n",
      "new b tensor([  2.6266, -15.4626], requires_grad=True)\n",
      "Loss= tensor(1.3336, grad_fn=<DivBackward0>)\n",
      "i= 854\n",
      "w.grad= tensor([[ 0.0260, -0.5485,  1.6513],\n",
      "        [-1.9651,  0.6021, -3.2876]])\n",
      "b.grad= tensor([-44.8249, 294.6364])\n",
      "new w tensor([[-0.4241,  0.8418,  0.6867],\n",
      "        [-0.1784,  0.8040,  0.9848]], requires_grad=True)\n",
      "new b tensor([  2.6289, -15.4774], requires_grad=True)\n",
      "Loss= tensor(1.3350, grad_fn=<DivBackward0>)\n",
      "i= 855\n",
      "w.grad= tensor([[ 0.0275, -0.5456,  1.6450],\n",
      "        [-1.9643,  0.6009, -3.2801]])\n",
      "b.grad= tensor([-44.7789, 294.4911])\n",
      "new w tensor([[-0.4241,  0.8418,  0.6866],\n",
      "        [-0.1783,  0.8039,  0.9849]], requires_grad=True)\n",
      "new b tensor([  2.6311, -15.4921], requires_grad=True)\n",
      "Loss= tensor(1.3363, grad_fn=<DivBackward0>)\n",
      "i= 856\n",
      "w.grad= tensor([[ 0.0291, -0.5427,  1.6390],\n",
      "        [-1.9651,  0.5981, -3.2736]])\n",
      "b.grad= tensor([-44.7330, 294.3457])\n",
      "new w tensor([[-0.4241,  0.8418,  0.6865],\n",
      "        [-0.1782,  0.8039,  0.9851]], requires_grad=True)\n",
      "new b tensor([  2.6334, -15.5068], requires_grad=True)\n",
      "Loss= tensor(1.3376, grad_fn=<DivBackward0>)\n",
      "i= 857\n",
      "w.grad= tensor([[ 0.0295, -0.5409,  1.6321],\n",
      "        [-1.9657,  0.5954, -3.2672]])\n",
      "b.grad= tensor([-44.6870, 294.2002])\n",
      "new w tensor([[-0.4241,  0.8419,  0.6864],\n",
      "        [-0.1781,  0.8039,  0.9853]], requires_grad=True)\n",
      "new b tensor([  2.6356, -15.5215], requires_grad=True)\n",
      "Loss= tensor(1.3390, grad_fn=<DivBackward0>)\n",
      "i= 858\n",
      "w.grad= tensor([[ 0.0304, -0.5387,  1.6256],\n",
      "        [-1.9660,  0.5932, -3.2605]])\n",
      "b.grad= tensor([-44.6410, 294.0547])\n",
      "new w tensor([[-0.4241,  0.8419,  0.6863],\n",
      "        [-0.1780,  0.8039,  0.9854]], requires_grad=True)\n",
      "new b tensor([  2.6378, -15.5362], requires_grad=True)\n",
      "Loss= tensor(1.3403, grad_fn=<DivBackward0>)\n",
      "i= 859\n",
      "w.grad= tensor([[ 0.0318, -0.5360,  1.6195],\n",
      "        [-1.9656,  0.5916, -3.2535]])\n",
      "b.grad= tensor([-44.5951, 293.9091])\n",
      "new w tensor([[-0.4241,  0.8419,  0.6863],\n",
      "        [-0.1780,  0.8038,  0.9856]], requires_grad=True)\n",
      "new b tensor([  2.6401, -15.5509], requires_grad=True)\n",
      "Loss= tensor(1.3417, grad_fn=<DivBackward0>)\n",
      "i= 860\n",
      "w.grad= tensor([[ 0.0327, -0.5341,  1.6129],\n",
      "        [-1.9662,  0.5889, -3.2472]])\n",
      "b.grad= tensor([-44.5492, 293.7634])\n",
      "new w tensor([[-0.4241,  0.8419,  0.6862],\n",
      "        [-0.1779,  0.8038,  0.9858]], requires_grad=True)\n",
      "new b tensor([  2.6423, -15.5656], requires_grad=True)\n",
      "Loss= tensor(1.3431, grad_fn=<DivBackward0>)\n",
      "i= 861\n",
      "w.grad= tensor([[ 0.0338, -0.5315,  1.6067],\n",
      "        [-1.9665,  0.5866, -3.2406]])\n",
      "b.grad= tensor([-44.5033, 293.6176])\n",
      "new w tensor([[-0.4241,  0.8420,  0.6861],\n",
      "        [-0.1778,  0.8038,  0.9859]], requires_grad=True)\n",
      "new b tensor([  2.6445, -15.5803], requires_grad=True)\n",
      "Loss= tensor(1.3444, grad_fn=<DivBackward0>)\n",
      "i= 862\n",
      "w.grad= tensor([[ 0.0352, -0.5288,  1.6006],\n",
      "        [-1.9668,  0.5842, -3.2341]])\n",
      "b.grad= tensor([-44.4574, 293.4717])\n",
      "new w tensor([[-0.4241,  0.8420,  0.6860],\n",
      "        [-0.1777,  0.8037,  0.9861]], requires_grad=True)\n",
      "new b tensor([  2.6467, -15.5950], requires_grad=True)\n",
      "Loss= tensor(1.3458, grad_fn=<DivBackward0>)\n",
      "i= 863\n",
      "w.grad= tensor([[ 0.0363, -0.5264,  1.5944],\n",
      "        [-1.9672,  0.5817, -3.2277]])\n",
      "b.grad= tensor([-44.4115, 293.3258])\n",
      "new w tensor([[-0.4241,  0.8420,  0.6859],\n",
      "        [-0.1776,  0.8037,  0.9862]], requires_grad=True)\n",
      "new b tensor([  2.6490, -15.6096], requires_grad=True)\n",
      "Loss= tensor(1.3472, grad_fn=<DivBackward0>)\n",
      "i= 864\n",
      "w.grad= tensor([[ 0.0371, -0.5244,  1.5880],\n",
      "        [-1.9668,  0.5803, -3.2207]])\n",
      "b.grad= tensor([-44.3656, 293.1797])\n",
      "new w tensor([[-0.4241,  0.8420,  0.6859],\n",
      "        [-0.1775,  0.8037,  0.9864]], requires_grad=True)\n",
      "new b tensor([  2.6512, -15.6243], requires_grad=True)\n",
      "Loss= tensor(1.3485, grad_fn=<DivBackward0>)\n",
      "i= 865\n",
      "w.grad= tensor([[ 0.0385, -0.5217,  1.5821],\n",
      "        [-1.9672,  0.5779, -3.2143]])\n",
      "b.grad= tensor([-44.3198, 293.0337])\n",
      "new w tensor([[-0.4241,  0.8421,  0.6858],\n",
      "        [-0.1774,  0.8037,  0.9866]], requires_grad=True)\n",
      "new b tensor([  2.6534, -15.6389], requires_grad=True)\n",
      "Loss= tensor(1.3499, grad_fn=<DivBackward0>)\n",
      "i= 866\n",
      "w.grad= tensor([[ 0.0397, -0.5192,  1.5761],\n",
      "        [-1.9677,  0.5754, -3.2081]])\n",
      "b.grad= tensor([-44.2739, 292.8875])\n",
      "new w tensor([[-0.4241,  0.8421,  0.6857],\n",
      "        [-0.1773,  0.8036,  0.9867]], requires_grad=True)\n",
      "new b tensor([  2.6556, -15.6536], requires_grad=True)\n",
      "Loss= tensor(1.3513, grad_fn=<DivBackward0>)\n",
      "i= 867\n",
      "w.grad= tensor([[ 0.0399, -0.5177,  1.5694],\n",
      "        [-1.9677,  0.5735, -3.2014]])\n",
      "b.grad= tensor([-44.2281, 292.7412])\n",
      "new w tensor([[-0.4241,  0.8421,  0.6856],\n",
      "        [-0.1772,  0.8036,  0.9869]], requires_grad=True)\n",
      "new b tensor([  2.6578, -15.6682], requires_grad=True)\n",
      "Loss= tensor(1.3527, grad_fn=<DivBackward0>)\n",
      "i= 868\n",
      "w.grad= tensor([[ 0.0414, -0.5148,  1.5636],\n",
      "        [-1.9680,  0.5712, -3.1952]])\n",
      "b.grad= tensor([-44.1823, 292.5949])\n",
      "new w tensor([[-0.4241,  0.8421,  0.6855],\n",
      "        [-0.1771,  0.8036,  0.9870]], requires_grad=True)\n",
      "new b tensor([  2.6600, -15.6828], requires_grad=True)\n",
      "Loss= tensor(1.3540, grad_fn=<DivBackward0>)\n",
      "i= 869\n",
      "w.grad= tensor([[ 0.0419, -0.5132,  1.5572],\n",
      "        [-1.9682,  0.5690, -3.1888]])\n",
      "b.grad= tensor([-44.1365, 292.4485])\n",
      "new w tensor([[-0.4241,  0.8422,  0.6855],\n",
      "        [-0.1770,  0.8035,  0.9872]], requires_grad=True)\n",
      "new b tensor([  2.6622, -15.6975], requires_grad=True)\n",
      "Loss= tensor(1.3554, grad_fn=<DivBackward0>)\n",
      "i= 870\n",
      "w.grad= tensor([[ 0.0440, -0.5098,  1.5518],\n",
      "        [-1.9683,  0.5668, -3.1824]])\n",
      "b.grad= tensor([-44.0907, 292.3020])\n",
      "new w tensor([[-0.4241,  0.8422,  0.6854],\n",
      "        [-0.1769,  0.8035,  0.9874]], requires_grad=True)\n",
      "new b tensor([  2.6644, -15.7121], requires_grad=True)\n",
      "Loss= tensor(1.3568, grad_fn=<DivBackward0>)\n",
      "i= 871\n",
      "w.grad= tensor([[ 0.0442, -0.5085,  1.5452],\n",
      "        [-1.9678,  0.5655, -3.1756]])\n",
      "b.grad= tensor([-44.0450, 292.1554])\n",
      "new w tensor([[-0.4241,  0.8422,  0.6853],\n",
      "        [-0.1768,  0.8035,  0.9875]], requires_grad=True)\n",
      "new b tensor([  2.6666, -15.7267], requires_grad=True)\n",
      "Loss= tensor(1.3582, grad_fn=<DivBackward0>)\n",
      "i= 872\n",
      "w.grad= tensor([[ 0.0457, -0.5056,  1.5396],\n",
      "        [-1.9688,  0.5625, -3.1699]])\n",
      "b.grad= tensor([-43.9992, 292.0088])\n",
      "new w tensor([[-0.4241,  0.8422,  0.6852],\n",
      "        [-0.1767,  0.8035,  0.9877]], requires_grad=True)\n",
      "new b tensor([  2.6688, -15.7413], requires_grad=True)\n",
      "Loss= tensor(1.3596, grad_fn=<DivBackward0>)\n",
      "i= 873\n",
      "w.grad= tensor([[ 0.0463, -0.5039,  1.5333],\n",
      "        [-1.9687,  0.5607, -3.1633]])\n",
      "b.grad= tensor([-43.9535, 291.8620])\n",
      "new w tensor([[-0.4241,  0.8423,  0.6852],\n",
      "        [-0.1766,  0.8034,  0.9878]], requires_grad=True)\n",
      "new b tensor([  2.6710, -15.7559], requires_grad=True)\n",
      "Loss= tensor(1.3610, grad_fn=<DivBackward0>)\n",
      "i= 874\n",
      "w.grad= tensor([[ 0.0474, -0.5015,  1.5275],\n",
      "        [-1.9685,  0.5589, -3.1569]])\n",
      "b.grad= tensor([-43.9078, 291.7152])\n",
      "new w tensor([[-0.4241,  0.8423,  0.6851],\n",
      "        [-0.1765,  0.8034,  0.9880]], requires_grad=True)\n",
      "new b tensor([  2.6732, -15.7705], requires_grad=True)\n",
      "Loss= tensor(1.3624, grad_fn=<DivBackward0>)\n",
      "i= 875\n",
      "w.grad= tensor([[ 0.0489, -0.4986,  1.5219],\n",
      "        [-1.9689,  0.5566, -3.1507]])\n",
      "b.grad= tensor([-43.8620, 291.5683])\n",
      "new w tensor([[-0.4241,  0.8423,  0.6850],\n",
      "        [-0.1764,  0.8034,  0.9882]], requires_grad=True)\n",
      "new b tensor([  2.6754, -15.7850], requires_grad=True)\n",
      "Loss= tensor(1.3638, grad_fn=<DivBackward0>)\n",
      "i= 876\n",
      "w.grad= tensor([[ 0.0493, -0.4970,  1.5157],\n",
      "        [-1.9695,  0.5539, -3.1449]])\n",
      "b.grad= tensor([-43.8163, 291.4214])\n",
      "new w tensor([[-0.4241,  0.8423,  0.6849],\n",
      "        [-0.1763,  0.8033,  0.9883]], requires_grad=True)\n",
      "new b tensor([  2.6776, -15.7996], requires_grad=True)\n",
      "Loss= tensor(1.3652, grad_fn=<DivBackward0>)\n",
      "i= 877\n",
      "w.grad= tensor([[ 0.0502, -0.4949,  1.5098],\n",
      "        [-1.9702,  0.5512, -3.1391]])\n",
      "b.grad= tensor([-43.7707, 291.2743])\n",
      "new w tensor([[-0.4241,  0.8424,  0.6849],\n",
      "        [-0.1762,  0.8033,  0.9885]], requires_grad=True)\n",
      "new b tensor([  2.6798, -15.8142], requires_grad=True)\n",
      "Loss= tensor(1.3666, grad_fn=<DivBackward0>)\n",
      "i= 878\n",
      "w.grad= tensor([[ 0.0516, -0.4923,  1.5042],\n",
      "        [-1.9688,  0.5509, -3.1319]])\n",
      "b.grad= tensor([-43.7250, 291.1272])\n",
      "new w tensor([[-0.4241,  0.8424,  0.6848],\n",
      "        [-0.1761,  0.8033,  0.9886]], requires_grad=True)\n",
      "new b tensor([  2.6820, -15.8287], requires_grad=True)\n",
      "Loss= tensor(1.3681, grad_fn=<DivBackward0>)\n",
      "i= 879\n",
      "w.grad= tensor([[ 0.0519, -0.4909,  1.4979],\n",
      "        [-1.9701,  0.5475, -3.1266]])\n",
      "b.grad= tensor([-43.6793, 290.9799])\n",
      "new w tensor([[-0.4241,  0.8424,  0.6847],\n",
      "        [-0.1760,  0.8033,  0.9888]], requires_grad=True)\n",
      "new b tensor([  2.6842, -15.8433], requires_grad=True)\n",
      "Loss= tensor(1.3695, grad_fn=<DivBackward0>)\n",
      "i= 880\n",
      "w.grad= tensor([[ 0.0535, -0.4880,  1.4926],\n",
      "        [-1.9689,  0.5469, -3.1196]])\n",
      "b.grad= tensor([-43.6337, 290.8327])\n",
      "new w tensor([[-0.4241,  0.8424,  0.6846],\n",
      "        [-0.1759,  0.8032,  0.9889]], requires_grad=True)\n",
      "new b tensor([  2.6864, -15.8578], requires_grad=True)\n",
      "Loss= tensor(1.3709, grad_fn=<DivBackward0>)\n",
      "i= 881\n",
      "w.grad= tensor([[ 0.0536, -0.4868,  1.4863],\n",
      "        [-1.9700,  0.5438, -3.1142]])\n",
      "b.grad= tensor([-43.5881, 290.6853])\n",
      "new w tensor([[-0.4242,  0.8425,  0.6846],\n",
      "        [-0.1758,  0.8032,  0.9891]], requires_grad=True)\n",
      "new b tensor([  2.6885, -15.8724], requires_grad=True)\n",
      "Loss= tensor(1.3723, grad_fn=<DivBackward0>)\n",
      "i= 882\n",
      "w.grad= tensor([[ 0.0551, -0.4840,  1.4809],\n",
      "        [-1.9702,  0.5416, -3.1082]])\n",
      "b.grad= tensor([-43.5424, 290.5378])\n",
      "new w tensor([[-0.4242,  0.8425,  0.6845],\n",
      "        [-0.1757,  0.8032,  0.9893]], requires_grad=True)\n",
      "new b tensor([  2.6907, -15.8869], requires_grad=True)\n",
      "Loss= tensor(1.3737, grad_fn=<DivBackward0>)\n",
      "i= 883\n",
      "w.grad= tensor([[ 0.0563, -0.4816,  1.4754],\n",
      "        [-1.9698,  0.5402, -3.1019]])\n",
      "b.grad= tensor([-43.4968, 290.3903])\n",
      "new w tensor([[-0.4242,  0.8425,  0.6844],\n",
      "        [-0.1756,  0.8032,  0.9894]], requires_grad=True)\n",
      "new b tensor([  2.6929, -15.9014], requires_grad=True)\n",
      "Loss= tensor(1.3752, grad_fn=<DivBackward0>)\n",
      "i= 884\n",
      "w.grad= tensor([[ 0.0571, -0.4795,  1.4696],\n",
      "        [-1.9696,  0.5384, -3.0956]])\n",
      "b.grad= tensor([-43.4512, 290.2427])\n",
      "new w tensor([[-0.4242,  0.8425,  0.6843],\n",
      "        [-0.1755,  0.8031,  0.9896]], requires_grad=True)\n",
      "new b tensor([  2.6951, -15.9159], requires_grad=True)\n",
      "Loss= tensor(1.3766, grad_fn=<DivBackward0>)\n",
      "i= 885\n",
      "w.grad= tensor([[ 0.0576, -0.4780,  1.4636],\n",
      "        [-1.9703,  0.5358, -3.0900]])\n",
      "b.grad= tensor([-43.4056, 290.0950])\n",
      "new w tensor([[-0.4242,  0.8426,  0.6843],\n",
      "        [-0.1754,  0.8031,  0.9897]], requires_grad=True)\n",
      "new b tensor([  2.6972, -15.9304], requires_grad=True)\n",
      "Loss= tensor(1.3780, grad_fn=<DivBackward0>)\n",
      "i= 886\n",
      "w.grad= tensor([[ 0.0588, -0.4755,  1.4582],\n",
      "        [-1.9700,  0.5344, -3.0837]])\n",
      "b.grad= tensor([-43.3601, 289.9473])\n",
      "new w tensor([[-0.4242,  0.8426,  0.6842],\n",
      "        [-0.1753,  0.8031,  0.9899]], requires_grad=True)\n",
      "new b tensor([  2.6994, -15.9449], requires_grad=True)\n",
      "Loss= tensor(1.3795, grad_fn=<DivBackward0>)\n",
      "i= 887\n",
      "w.grad= tensor([[ 0.0598, -0.4735,  1.4526],\n",
      "        [-1.9703,  0.5319, -3.0780]])\n",
      "b.grad= tensor([-43.3145, 289.7994])\n",
      "new w tensor([[-0.4242,  0.8426,  0.6841],\n",
      "        [-0.1752,  0.8030,  0.9900]], requires_grad=True)\n",
      "new b tensor([  2.7016, -15.9594], requires_grad=True)\n",
      "Loss= tensor(1.3809, grad_fn=<DivBackward0>)\n",
      "i= 888\n",
      "w.grad= tensor([[ 0.0609, -0.4711,  1.4472],\n",
      "        [-1.9707,  0.5297, -3.0723]])\n",
      "b.grad= tensor([-43.2690, 289.6515])\n",
      "new w tensor([[-0.4242,  0.8426,  0.6840],\n",
      "        [-0.1751,  0.8030,  0.9902]], requires_grad=True)\n",
      "new b tensor([  2.7037, -15.9739], requires_grad=True)\n",
      "Loss= tensor(1.3824, grad_fn=<DivBackward0>)\n",
      "i= 889\n",
      "w.grad= tensor([[ 0.0615, -0.4694,  1.4414],\n",
      "        [-1.9699,  0.5287, -3.0658]])\n",
      "b.grad= tensor([-43.2234, 289.5035])\n",
      "new w tensor([[-0.4242,  0.8427,  0.6840],\n",
      "        [-0.1750,  0.8030,  0.9903]], requires_grad=True)\n",
      "new b tensor([  2.7059, -15.9884], requires_grad=True)\n",
      "Loss= tensor(1.3838, grad_fn=<DivBackward0>)\n",
      "i= 890\n",
      "w.grad= tensor([[ 0.0627, -0.4670,  1.4361],\n",
      "        [-1.9709,  0.5255, -3.0606]])\n",
      "b.grad= tensor([-43.1779, 289.3554])\n",
      "new w tensor([[-0.4242,  0.8427,  0.6839],\n",
      "        [-0.1749,  0.8030,  0.9905]], requires_grad=True)\n",
      "new b tensor([  2.7080, -16.0028], requires_grad=True)\n",
      "Loss= tensor(1.3853, grad_fn=<DivBackward0>)\n",
      "i= 891\n",
      "w.grad= tensor([[ 0.0635, -0.4651,  1.4305],\n",
      "        [-1.9707,  0.5238, -3.0546]])\n",
      "b.grad= tensor([-43.1324, 289.2072])\n",
      "new w tensor([[-0.4242,  0.8427,  0.6838],\n",
      "        [-0.1748,  0.8029,  0.9906]], requires_grad=True)\n",
      "new b tensor([  2.7102, -16.0173], requires_grad=True)\n",
      "Loss= tensor(1.3867, grad_fn=<DivBackward0>)\n",
      "i= 892\n",
      "w.grad= tensor([[ 0.0644, -0.4630,  1.4250],\n",
      "        [-1.9703,  0.5225, -3.0484]])\n",
      "b.grad= tensor([-43.0869, 289.0590])\n",
      "new w tensor([[-0.4242,  0.8427,  0.6838],\n",
      "        [-0.1747,  0.8029,  0.9908]], requires_grad=True)\n",
      "new b tensor([  2.7124, -16.0317], requires_grad=True)\n",
      "Loss= tensor(1.3882, grad_fn=<DivBackward0>)\n",
      "i= 893\n",
      "w.grad= tensor([[ 0.0654, -0.4608,  1.4196],\n",
      "        [-1.9703,  0.5205, -3.0425]])\n",
      "b.grad= tensor([-43.0414, 288.9106])\n",
      "new w tensor([[-0.4242,  0.8428,  0.6837],\n",
      "        [-0.1746,  0.8029,  0.9909]], requires_grad=True)\n",
      "new b tensor([  2.7145, -16.0462], requires_grad=True)\n",
      "Loss= tensor(1.3896, grad_fn=<DivBackward0>)\n",
      "i= 894\n",
      "w.grad= tensor([[ 0.0663, -0.4587,  1.4142],\n",
      "        [-1.9709,  0.5181, -3.0372]])\n",
      "b.grad= tensor([-42.9959, 288.7622])\n",
      "new w tensor([[-0.4242,  0.8428,  0.6836],\n",
      "        [-0.1745,  0.8029,  0.9911]], requires_grad=True)\n",
      "new b tensor([  2.7167, -16.0606], requires_grad=True)\n",
      "Loss= tensor(1.3911, grad_fn=<DivBackward0>)\n",
      "i= 895\n",
      "w.grad= tensor([[ 0.0672, -0.4567,  1.4088],\n",
      "        [-1.9708,  0.5162, -3.0312]])\n",
      "b.grad= tensor([-42.9505, 288.6137])\n",
      "new w tensor([[-0.4242,  0.8428,  0.6835],\n",
      "        [-0.1744,  0.8028,  0.9912]], requires_grad=True)\n",
      "new b tensor([  2.7188, -16.0751], requires_grad=True)\n",
      "Loss= tensor(1.3925, grad_fn=<DivBackward0>)\n",
      "i= 896\n",
      "w.grad= tensor([[ 0.0673, -0.4555,  1.4029],\n",
      "        [-1.9707,  0.5146, -3.0255]])\n",
      "b.grad= tensor([-42.9050, 288.4652])\n",
      "new w tensor([[-0.4242,  0.8428,  0.6835],\n",
      "        [-0.1743,  0.8028,  0.9914]], requires_grad=True)\n",
      "new b tensor([  2.7209, -16.0895], requires_grad=True)\n",
      "Loss= tensor(1.3940, grad_fn=<DivBackward0>)\n",
      "i= 897\n",
      "w.grad= tensor([[ 0.0689, -0.4527,  1.3981],\n",
      "        [-1.9705,  0.5128, -3.0196]])\n",
      "b.grad= tensor([-42.8596, 288.3165])\n",
      "new w tensor([[-0.4242,  0.8428,  0.6834],\n",
      "        [-0.1742,  0.8028,  0.9915]], requires_grad=True)\n",
      "new b tensor([  2.7231, -16.1039], requires_grad=True)\n",
      "Loss= tensor(1.3955, grad_fn=<DivBackward0>)\n",
      "i= 898\n",
      "w.grad= tensor([[ 0.0695, -0.4510,  1.3925],\n",
      "        [-1.9708,  0.5105, -3.0142]])\n",
      "b.grad= tensor([-42.8141, 288.1678])\n",
      "new w tensor([[-0.4242,  0.8429,  0.6833],\n",
      "        [-0.1741,  0.8028,  0.9917]], requires_grad=True)\n",
      "new b tensor([  2.7252, -16.1183], requires_grad=True)\n",
      "Loss= tensor(1.3969, grad_fn=<DivBackward0>)\n",
      "i= 899\n",
      "w.grad= tensor([[ 0.0703, -0.4490,  1.3872],\n",
      "        [-1.9706,  0.5089, -3.0084]])\n",
      "b.grad= tensor([-42.7687, 288.0190])\n",
      "new w tensor([[-0.4242,  0.8429,  0.6833],\n",
      "        [-0.1740,  0.8027,  0.9919]], requires_grad=True)\n",
      "new b tensor([  2.7274, -16.1327], requires_grad=True)\n",
      "Loss= tensor(1.3984, grad_fn=<DivBackward0>)\n",
      "i= 900\n",
      "w.grad= tensor([[ 0.0709, -0.4472,  1.3818],\n",
      "        [-1.9702,  0.5075, -3.0024]])\n",
      "b.grad= tensor([-42.7233, 287.8701])\n",
      "new w tensor([[-0.4242,  0.8429,  0.6832],\n",
      "        [-0.1739,  0.8027,  0.9920]], requires_grad=True)\n",
      "new b tensor([  2.7295, -16.1471], requires_grad=True)\n",
      "Loss= tensor(1.3999, grad_fn=<DivBackward0>)\n",
      "i= 901\n",
      "w.grad= tensor([[ 0.0721, -0.4450,  1.3767],\n",
      "        [-1.9704,  0.5052, -2.9970]])\n",
      "b.grad= tensor([-42.6779, 287.7211])\n",
      "new w tensor([[-0.4242,  0.8429,  0.6831],\n",
      "        [-0.1738,  0.8027,  0.9922]], requires_grad=True)\n",
      "new b tensor([  2.7316, -16.1615], requires_grad=True)\n",
      "Loss= tensor(1.4014, grad_fn=<DivBackward0>)\n",
      "i= 902\n",
      "w.grad= tensor([[ 0.0730, -0.4429,  1.3716],\n",
      "        [-1.9712,  0.5026, -2.9919]])\n",
      "b.grad= tensor([-42.6325, 287.5721])\n",
      "new w tensor([[-0.4242,  0.8430,  0.6831],\n",
      "        [-0.1737,  0.8027,  0.9923]], requires_grad=True)\n",
      "new b tensor([  2.7338, -16.1759], requires_grad=True)\n",
      "Loss= tensor(1.4029, grad_fn=<DivBackward0>)\n",
      "i= 903\n",
      "w.grad= tensor([[ 0.0735, -0.4414,  1.3661],\n",
      "        [-1.9701,  0.5019, -2.9856]])\n",
      "b.grad= tensor([-42.5871, 287.4229])\n",
      "new w tensor([[-0.4242,  0.8430,  0.6830],\n",
      "        [-0.1736,  0.8026,  0.9924]], requires_grad=True)\n",
      "new b tensor([  2.7359, -16.1902], requires_grad=True)\n",
      "Loss= tensor(1.4043, grad_fn=<DivBackward0>)\n",
      "i= 904\n",
      "w.grad= tensor([[ 0.0747, -0.4388,  1.3612],\n",
      "        [-1.9703,  0.4997, -2.9801]])\n",
      "b.grad= tensor([-42.5418, 287.2737])\n",
      "new w tensor([[-0.4242,  0.8430,  0.6829],\n",
      "        [-0.1735,  0.8026,  0.9926]], requires_grad=True)\n",
      "new b tensor([  2.7380, -16.2046], requires_grad=True)\n",
      "Loss= tensor(1.4058, grad_fn=<DivBackward0>)\n",
      "i= 905\n",
      "w.grad= tensor([[ 0.0754, -0.4372,  1.3559],\n",
      "        [-1.9703,  0.4980, -2.9746]])\n",
      "b.grad= tensor([-42.4964, 287.1244])\n",
      "new w tensor([[-0.4242,  0.8430,  0.6829],\n",
      "        [-0.1734,  0.8026,  0.9927]], requires_grad=True)\n",
      "new b tensor([  2.7402, -16.2190], requires_grad=True)\n",
      "Loss= tensor(1.4073, grad_fn=<DivBackward0>)\n",
      "i= 906\n",
      "w.grad= tensor([[ 0.0760, -0.4354,  1.3506],\n",
      "        [-1.9708,  0.4956, -2.9694]])\n",
      "b.grad= tensor([-42.4511, 286.9750])\n",
      "new w tensor([[-0.4242,  0.8430,  0.6828],\n",
      "        [-0.1733,  0.8026,  0.9929]], requires_grad=True)\n",
      "new b tensor([  2.7423, -16.2333], requires_grad=True)\n",
      "Loss= tensor(1.4088, grad_fn=<DivBackward0>)\n",
      "i= 907\n",
      "w.grad= tensor([[ 0.0771, -0.4331,  1.3457],\n",
      "        [-1.9703,  0.4943, -2.9637]])\n",
      "b.grad= tensor([-42.4057, 286.8256])\n",
      "new w tensor([[-0.4242,  0.8431,  0.6827],\n",
      "        [-0.1732,  0.8025,  0.9930]], requires_grad=True)\n",
      "new b tensor([  2.7444, -16.2477], requires_grad=True)\n",
      "Loss= tensor(1.4103, grad_fn=<DivBackward0>)\n",
      "i= 908\n",
      "w.grad= tensor([[ 0.0779, -0.4313,  1.3406],\n",
      "        [-1.9703,  0.4922, -2.9582]])\n",
      "b.grad= tensor([-42.3604, 286.6761])\n",
      "new w tensor([[-0.4242,  0.8431,  0.6827],\n",
      "        [-0.1731,  0.8025,  0.9932]], requires_grad=True)\n",
      "new b tensor([  2.7465, -16.2620], requires_grad=True)\n",
      "Loss= tensor(1.4118, grad_fn=<DivBackward0>)\n",
      "i= 909\n",
      "w.grad= tensor([[ 0.0783, -0.4298,  1.3352],\n",
      "        [-1.9702,  0.4906, -2.9526]])\n",
      "b.grad= tensor([-42.3151, 286.5264])\n",
      "new w tensor([[-0.4242,  0.8431,  0.6826],\n",
      "        [-0.1730,  0.8025,  0.9933]], requires_grad=True)\n",
      "new b tensor([  2.7486, -16.2763], requires_grad=True)\n",
      "Loss= tensor(1.4133, grad_fn=<DivBackward0>)\n",
      "i= 910\n",
      "w.grad= tensor([[ 0.0794, -0.4278,  1.3303],\n",
      "        [-1.9699,  0.4891, -2.9471]])\n",
      "b.grad= tensor([-42.2698, 286.3767])\n",
      "new w tensor([[-0.4242,  0.8431,  0.6825],\n",
      "        [-0.1729,  0.8025,  0.9935]], requires_grad=True)\n",
      "new b tensor([  2.7507, -16.2906], requires_grad=True)\n",
      "Loss= tensor(1.4148, grad_fn=<DivBackward0>)\n",
      "i= 911\n",
      "w.grad= tensor([[ 0.0805, -0.4254,  1.3255],\n",
      "        [-1.9707,  0.4863, -2.9423]])\n",
      "b.grad= tensor([-42.2245, 286.2270])\n",
      "new w tensor([[-0.4243,  0.8432,  0.6825],\n",
      "        [-0.1728,  0.8024,  0.9936]], requires_grad=True)\n",
      "new b tensor([  2.7529, -16.3049], requires_grad=True)\n",
      "Loss= tensor(1.4163, grad_fn=<DivBackward0>)\n",
      "i= 912\n",
      "w.grad= tensor([[ 0.0809, -0.4240,  1.3202],\n",
      "        [-1.9694,  0.4858, -2.9360]])\n",
      "b.grad= tensor([-42.1792, 286.0771])\n",
      "new w tensor([[-0.4243,  0.8432,  0.6824],\n",
      "        [-0.1727,  0.8024,  0.9938]], requires_grad=True)\n",
      "new b tensor([  2.7550, -16.3192], requires_grad=True)\n",
      "Loss= tensor(1.4178, grad_fn=<DivBackward0>)\n",
      "i= 913\n",
      "w.grad= tensor([[ 0.0822, -0.4215,  1.3156],\n",
      "        [-1.9699,  0.4834, -2.9311]])\n",
      "b.grad= tensor([-42.1339, 285.9272])\n",
      "new w tensor([[-0.4243,  0.8432,  0.6823],\n",
      "        [-0.1726,  0.8024,  0.9939]], requires_grad=True)\n",
      "new b tensor([  2.7571, -16.3335], requires_grad=True)\n",
      "Loss= tensor(1.4193, grad_fn=<DivBackward0>)\n",
      "i= 914\n",
      "w.grad= tensor([[ 0.0824, -0.4202,  1.3102],\n",
      "        [-1.9699,  0.4816, -2.9258]])\n",
      "b.grad= tensor([-42.0886, 285.7771])\n",
      "new w tensor([[-0.4243,  0.8432,  0.6823],\n",
      "        [-0.1725,  0.8024,  0.9941]], requires_grad=True)\n",
      "new b tensor([  2.7592, -16.3478], requires_grad=True)\n",
      "Loss= tensor(1.4208, grad_fn=<DivBackward0>)\n",
      "i= 915\n",
      "w.grad= tensor([[ 0.0840, -0.4175,  1.3058],\n",
      "        [-1.9695,  0.4803, -2.9202]])\n",
      "b.grad= tensor([-42.0434, 285.6270])\n",
      "new w tensor([[-0.4243,  0.8432,  0.6822],\n",
      "        [-0.1724,  0.8023,  0.9942]], requires_grad=True)\n",
      "new b tensor([  2.7613, -16.3621], requires_grad=True)\n",
      "Loss= tensor(1.4223, grad_fn=<DivBackward0>)\n",
      "i= 916\n",
      "w.grad= tensor([[ 0.0837, -0.4168,  1.3002],\n",
      "        [-1.9693,  0.4786, -2.9148]])\n",
      "b.grad= tensor([-41.9981, 285.4768])\n",
      "new w tensor([[-0.4243,  0.8433,  0.6821],\n",
      "        [-0.1723,  0.8023,  0.9944]], requires_grad=True)\n",
      "new b tensor([  2.7634, -16.3764], requires_grad=True)\n",
      "Loss= tensor(1.4238, grad_fn=<DivBackward0>)\n",
      "i= 917\n",
      "w.grad= tensor([[ 0.0849, -0.4145,  1.2955],\n",
      "        [-1.9695,  0.4765, -2.9098]])\n",
      "b.grad= tensor([-41.9529, 285.3266])\n",
      "new w tensor([[-0.4243,  0.8433,  0.6821],\n",
      "        [-0.1722,  0.8023,  0.9945]], requires_grad=True)\n",
      "new b tensor([  2.7655, -16.3907], requires_grad=True)\n",
      "Loss= tensor(1.4253, grad_fn=<DivBackward0>)\n",
      "i= 918\n",
      "w.grad= tensor([[ 0.0854, -0.4130,  1.2905],\n",
      "        [-1.9696,  0.4745, -2.9046]])\n",
      "b.grad= tensor([-41.9076, 285.1762])\n",
      "new w tensor([[-0.4243,  0.8433,  0.6820],\n",
      "        [-0.1721,  0.8023,  0.9947]], requires_grad=True)\n",
      "new b tensor([  2.7676, -16.4049], requires_grad=True)\n",
      "Loss= tensor(1.4269, grad_fn=<DivBackward0>)\n",
      "i= 919\n",
      "w.grad= tensor([[ 0.0864, -0.4108,  1.2858],\n",
      "        [-1.9686,  0.4740, -2.8987]])\n",
      "b.grad= tensor([-41.8624, 285.0258])\n",
      "new w tensor([[-0.4243,  0.8433,  0.6819],\n",
      "        [-0.1720,  0.8022,  0.9948]], requires_grad=True)\n",
      "new b tensor([  2.7697, -16.4192], requires_grad=True)\n",
      "Loss= tensor(1.4284, grad_fn=<DivBackward0>)\n",
      "i= 920\n",
      "w.grad= tensor([[ 0.0868, -0.4094,  1.2808],\n",
      "        [-1.9693,  0.4713, -2.8941]])\n",
      "b.grad= tensor([-41.8172, 284.8753])\n",
      "new w tensor([[-0.4243,  0.8433,  0.6819],\n",
      "        [-0.1719,  0.8022,  0.9949]], requires_grad=True)\n",
      "new b tensor([  2.7718, -16.4334], requires_grad=True)\n",
      "Loss= tensor(1.4299, grad_fn=<DivBackward0>)\n",
      "i= 921\n",
      "w.grad= tensor([[ 0.0877, -0.4076,  1.2760],\n",
      "        [-1.9692,  0.4694, -2.8888]])\n",
      "b.grad= tensor([-41.7720, 284.7247])\n",
      "new w tensor([[-0.4243,  0.8434,  0.6818],\n",
      "        [-0.1718,  0.8022,  0.9951]], requires_grad=True)\n",
      "new b tensor([  2.7738, -16.4476], requires_grad=True)\n",
      "Loss= tensor(1.4314, grad_fn=<DivBackward0>)\n",
      "i= 922\n",
      "w.grad= tensor([[ 0.0885, -0.4057,  1.2713],\n",
      "        [-1.9690,  0.4679, -2.8835]])\n",
      "b.grad= tensor([-41.7268, 284.5740])\n",
      "new w tensor([[-0.4243,  0.8434,  0.6817],\n",
      "        [-0.1717,  0.8022,  0.9952]], requires_grad=True)\n",
      "new b tensor([  2.7759, -16.4619], requires_grad=True)\n",
      "Loss= tensor(1.4330, grad_fn=<DivBackward0>)\n",
      "i= 923\n",
      "w.grad= tensor([[ 0.0893, -0.4037,  1.2666],\n",
      "        [-1.9685,  0.4666, -2.8782]])\n",
      "b.grad= tensor([-41.6816, 284.4232])\n",
      "new w tensor([[-0.4243,  0.8434,  0.6817],\n",
      "        [-0.1716,  0.8021,  0.9954]], requires_grad=True)\n",
      "new b tensor([  2.7780, -16.4761], requires_grad=True)\n",
      "Loss= tensor(1.4345, grad_fn=<DivBackward0>)\n",
      "i= 924\n",
      "w.grad= tensor([[ 0.0902, -0.4019,  1.2619],\n",
      "        [-1.9682,  0.4652, -2.8729]])\n",
      "b.grad= tensor([-41.6364, 284.2724])\n",
      "new w tensor([[-0.4243,  0.8434,  0.6816],\n",
      "        [-0.1716,  0.8021,  0.9955]], requires_grad=True)\n",
      "new b tensor([  2.7801, -16.4903], requires_grad=True)\n",
      "Loss= tensor(1.4360, grad_fn=<DivBackward0>)\n",
      "i= 925\n",
      "w.grad= tensor([[ 0.0905, -0.4004,  1.2569],\n",
      "        [-1.9686,  0.4628, -2.8681]])\n",
      "b.grad= tensor([-41.5913, 284.1215])\n",
      "new w tensor([[-0.4243,  0.8434,  0.6816],\n",
      "        [-0.1715,  0.8021,  0.9957]], requires_grad=True)\n",
      "new b tensor([  2.7822, -16.5045], requires_grad=True)\n",
      "Loss= tensor(1.4376, grad_fn=<DivBackward0>)\n",
      "i= 926\n",
      "w.grad= tensor([[ 0.0913, -0.3986,  1.2522],\n",
      "        [-1.9678,  0.4618, -2.8626]])\n",
      "b.grad= tensor([-41.5461, 283.9705])\n",
      "new w tensor([[-0.4243,  0.8435,  0.6815],\n",
      "        [-0.1714,  0.8021,  0.9958]], requires_grad=True)\n",
      "new b tensor([  2.7843, -16.5187], requires_grad=True)\n",
      "Loss= tensor(1.4391, grad_fn=<DivBackward0>)\n",
      "i= 927\n",
      "w.grad= tensor([[ 0.0926, -0.3962,  1.2480],\n",
      "        [-1.9677,  0.4602, -2.8575]])\n",
      "b.grad= tensor([-41.5009, 283.8194])\n",
      "new w tensor([[-0.4243,  0.8435,  0.6814],\n",
      "        [-0.1713,  0.8021,  0.9960]], requires_grad=True)\n",
      "new b tensor([  2.7863, -16.5329], requires_grad=True)\n",
      "Loss= tensor(1.4406, grad_fn=<DivBackward0>)\n",
      "i= 928\n",
      "w.grad= tensor([[ 0.0926, -0.3953,  1.2428],\n",
      "        [-1.9679,  0.4583, -2.8526]])\n",
      "b.grad= tensor([-41.4558, 283.6682])\n",
      "new w tensor([[-0.4243,  0.8435,  0.6814],\n",
      "        [-0.1712,  0.8020,  0.9961]], requires_grad=True)\n",
      "new b tensor([  2.7884, -16.5471], requires_grad=True)\n",
      "Loss= tensor(1.4422, grad_fn=<DivBackward0>)\n",
      "i= 929\n",
      "w.grad= tensor([[ 0.0932, -0.3937,  1.2380],\n",
      "        [-1.9675,  0.4567, -2.8475]])\n",
      "b.grad= tensor([-41.4107, 283.5170])\n",
      "new w tensor([[-0.4243,  0.8435,  0.6813],\n",
      "        [-0.1711,  0.8020,  0.9962]], requires_grad=True)\n",
      "new b tensor([  2.7905, -16.5613], requires_grad=True)\n",
      "Loss= tensor(1.4437, grad_fn=<DivBackward0>)\n",
      "i= 930\n",
      "w.grad= tensor([[ 0.0944, -0.3914,  1.2337],\n",
      "        [-1.9675,  0.4549, -2.8425]])\n",
      "b.grad= tensor([-41.3655, 283.3657])\n",
      "new w tensor([[-0.4243,  0.8435,  0.6812],\n",
      "        [-0.1710,  0.8020,  0.9964]], requires_grad=True)\n",
      "new b tensor([  2.7925, -16.5754], requires_grad=True)\n",
      "Loss= tensor(1.4453, grad_fn=<DivBackward0>)\n",
      "i= 931\n",
      "w.grad= tensor([[ 0.0948, -0.3901,  1.2289],\n",
      "        [-1.9671,  0.4537, -2.8373]])\n",
      "b.grad= tensor([-41.3204, 283.2143])\n",
      "new w tensor([[-0.4243,  0.8436,  0.6812],\n",
      "        [-0.1709,  0.8020,  0.9965]], requires_grad=True)\n",
      "new b tensor([  2.7946, -16.5896], requires_grad=True)\n",
      "Loss= tensor(1.4468, grad_fn=<DivBackward0>)\n",
      "i= 932\n",
      "w.grad= tensor([[ 0.0951, -0.3888,  1.2240],\n",
      "        [-1.9671,  0.4518, -2.8324]])\n",
      "b.grad= tensor([-41.2753, 283.0628])\n",
      "new w tensor([[-0.4243,  0.8436,  0.6811],\n",
      "        [-0.1708,  0.8019,  0.9967]], requires_grad=True)\n",
      "new b tensor([  2.7967, -16.6037], requires_grad=True)\n",
      "Loss= tensor(1.4484, grad_fn=<DivBackward0>)\n",
      "i= 933\n",
      "w.grad= tensor([[ 0.0962, -0.3867,  1.2198],\n",
      "        [-1.9673,  0.4496, -2.8278]])\n",
      "b.grad= tensor([-41.2302, 282.9112])\n",
      "new w tensor([[-0.4244,  0.8436,  0.6811],\n",
      "        [-0.1707,  0.8019,  0.9968]], requires_grad=True)\n",
      "new b tensor([  2.7987, -16.6179], requires_grad=True)\n",
      "Loss= tensor(1.4499, grad_fn=<DivBackward0>)\n",
      "i= 934\n",
      "w.grad= tensor([[ 0.0967, -0.3851,  1.2151],\n",
      "        [-1.9664,  0.4488, -2.8223]])\n",
      "b.grad= tensor([-41.1851, 282.7596])\n",
      "new w tensor([[-0.4244,  0.8436,  0.6810],\n",
      "        [-0.1706,  0.8019,  0.9969]], requires_grad=True)\n",
      "new b tensor([  2.8008, -16.6320], requires_grad=True)\n",
      "Loss= tensor(1.4515, grad_fn=<DivBackward0>)\n",
      "i= 935\n",
      "w.grad= tensor([[ 0.0975, -0.3833,  1.2106],\n",
      "        [-1.9658,  0.4479, -2.8170]])\n",
      "b.grad= tensor([-41.1400, 282.6078])\n",
      "new w tensor([[-0.4244,  0.8436,  0.6809],\n",
      "        [-0.1705,  0.8019,  0.9971]], requires_grad=True)\n",
      "new b tensor([  2.8028, -16.6462], requires_grad=True)\n",
      "Loss= tensor(1.4530, grad_fn=<DivBackward0>)\n",
      "i= 936\n",
      "w.grad= tensor([[ 0.0986, -0.3811,  1.2065],\n",
      "        [-1.9665,  0.4451, -2.8127]])\n",
      "b.grad= tensor([-41.0950, 282.4560])\n",
      "new w tensor([[-0.4244,  0.8437,  0.6809],\n",
      "        [-0.1704,  0.8019,  0.9972]], requires_grad=True)\n",
      "new b tensor([  2.8049, -16.6603], requires_grad=True)\n",
      "Loss= tensor(1.4546, grad_fn=<DivBackward0>)\n",
      "i= 937\n",
      "w.grad= tensor([[ 0.0988, -0.3799,  1.2017],\n",
      "        [-1.9666,  0.4431, -2.8081]])\n",
      "b.grad= tensor([-41.0499, 282.3041])\n",
      "new w tensor([[-0.4244,  0.8437,  0.6808],\n",
      "        [-0.1703,  0.8018,  0.9974]], requires_grad=True)\n",
      "new b tensor([  2.8070, -16.6744], requires_grad=True)\n",
      "Loss= tensor(1.4561, grad_fn=<DivBackward0>)\n",
      "i= 938\n",
      "w.grad= tensor([[ 0.0998, -0.3780,  1.1974],\n",
      "        [-1.9657,  0.4425, -2.8027]])\n",
      "b.grad= tensor([-41.0048, 282.1521])\n",
      "new w tensor([[-0.4244,  0.8437,  0.6808],\n",
      "        [-0.1702,  0.8018,  0.9975]], requires_grad=True)\n",
      "new b tensor([  2.8090, -16.6885], requires_grad=True)\n",
      "Loss= tensor(1.4577, grad_fn=<DivBackward0>)\n",
      "i= 939\n",
      "w.grad= tensor([[ 0.1007, -0.3760,  1.1931],\n",
      "        [-1.9650,  0.4414, -2.7975]])\n",
      "b.grad= tensor([-40.9598, 282.0001])\n",
      "new w tensor([[-0.4244,  0.8437,  0.6807],\n",
      "        [-0.1701,  0.8018,  0.9976]], requires_grad=True)\n",
      "new b tensor([  2.8111, -16.7026], requires_grad=True)\n",
      "Loss= tensor(1.4592, grad_fn=<DivBackward0>)\n",
      "i= 940\n",
      "w.grad= tensor([[ 0.1003, -0.3756,  1.1879],\n",
      "        [-1.9651,  0.4394, -2.7928]])\n",
      "b.grad= tensor([-40.9147, 281.8479])\n",
      "new w tensor([[-0.4244,  0.8437,  0.6806],\n",
      "        [-0.1700,  0.8018,  0.9978]], requires_grad=True)\n",
      "new b tensor([  2.8131, -16.7167], requires_grad=True)\n",
      "Loss= tensor(1.4608, grad_fn=<DivBackward0>)\n",
      "i= 941\n",
      "w.grad= tensor([[ 0.1014, -0.3734,  1.1838],\n",
      "        [-1.9652,  0.4375, -2.7882]])\n",
      "b.grad= tensor([-40.8697, 281.6957])\n",
      "new w tensor([[-0.4244,  0.8437,  0.6806],\n",
      "        [-0.1699,  0.8017,  0.9979]], requires_grad=True)\n",
      "new b tensor([  2.8151, -16.7308], requires_grad=True)\n",
      "Loss= tensor(1.4624, grad_fn=<DivBackward0>)\n",
      "i= 942\n",
      "w.grad= tensor([[ 0.1019, -0.3719,  1.1794],\n",
      "        [-1.9649,  0.4360, -2.7834]])\n",
      "b.grad= tensor([-40.8246, 281.5434])\n",
      "new w tensor([[-0.4244,  0.8438,  0.6805],\n",
      "        [-0.1698,  0.8017,  0.9981]], requires_grad=True)\n",
      "new b tensor([  2.8172, -16.7448], requires_grad=True)\n",
      "Loss= tensor(1.4640, grad_fn=<DivBackward0>)\n",
      "i= 943\n",
      "w.grad= tensor([[ 0.1033, -0.3694,  1.1755],\n",
      "        [-1.9640,  0.4354, -2.7780]])\n",
      "b.grad= tensor([-40.7796, 281.3910])\n",
      "new w tensor([[-0.4244,  0.8438,  0.6805],\n",
      "        [-0.1697,  0.8017,  0.9982]], requires_grad=True)\n",
      "new b tensor([  2.8192, -16.7589], requires_grad=True)\n",
      "Loss= tensor(1.4655, grad_fn=<DivBackward0>)\n",
      "i= 944\n",
      "w.grad= tensor([[ 0.1036, -0.3682,  1.1709],\n",
      "        [-1.9644,  0.4332, -2.7736]])\n",
      "b.grad= tensor([-40.7346, 281.2386])\n",
      "new w tensor([[-0.4244,  0.8438,  0.6804],\n",
      "        [-0.1696,  0.8017,  0.9983]], requires_grad=True)\n",
      "new b tensor([  2.8213, -16.7730], requires_grad=True)\n",
      "Loss= tensor(1.4671, grad_fn=<DivBackward0>)\n",
      "i= 945\n",
      "w.grad= tensor([[ 0.1038, -0.3670,  1.1663],\n",
      "        [-1.9644,  0.4311, -2.7691]])\n",
      "b.grad= tensor([-40.6896, 281.0860])\n",
      "new w tensor([[-0.4244,  0.8438,  0.6803],\n",
      "        [-0.1695,  0.8017,  0.9985]], requires_grad=True)\n",
      "new b tensor([  2.8233, -16.7870], requires_grad=True)\n",
      "Loss= tensor(1.4687, grad_fn=<DivBackward0>)\n",
      "i= 946\n",
      "w.grad= tensor([[ 0.1051, -0.3646,  1.1625],\n",
      "        [-1.9637,  0.4301, -2.7641]])\n",
      "b.grad= tensor([-40.6446, 280.9334])\n",
      "new w tensor([[-0.4244,  0.8438,  0.6803],\n",
      "        [-0.1694,  0.8016,  0.9986]], requires_grad=True)\n",
      "new b tensor([  2.8253, -16.8011], requires_grad=True)\n",
      "Loss= tensor(1.4702, grad_fn=<DivBackward0>)\n",
      "i= 947\n",
      "w.grad= tensor([[ 0.1052, -0.3637,  1.1578],\n",
      "        [-1.9636,  0.4286, -2.7593]])\n",
      "b.grad= tensor([-40.5996, 280.7807])\n",
      "new w tensor([[-0.4244,  0.8439,  0.6802],\n",
      "        [-0.1693,  0.8016,  0.9988]], requires_grad=True)\n",
      "new b tensor([  2.8274, -16.8151], requires_grad=True)\n",
      "Loss= tensor(1.4718, grad_fn=<DivBackward0>)\n",
      "i= 948\n",
      "w.grad= tensor([[ 0.1057, -0.3621,  1.1535],\n",
      "        [-1.9632,  0.4272, -2.7546]])\n",
      "b.grad= tensor([-40.5546, 280.6279])\n",
      "new w tensor([[-0.4244,  0.8439,  0.6802],\n",
      "        [-0.1692,  0.8016,  0.9989]], requires_grad=True)\n",
      "new b tensor([  2.8294, -16.8291], requires_grad=True)\n",
      "Loss= tensor(1.4734, grad_fn=<DivBackward0>)\n",
      "i= 949\n",
      "w.grad= tensor([[ 0.1067, -0.3601,  1.1494],\n",
      "        [-1.9629,  0.4257, -2.7499]])\n",
      "b.grad= tensor([-40.5096, 280.4750])\n",
      "new w tensor([[-0.4244,  0.8439,  0.6801],\n",
      "        [-0.1691,  0.8016,  0.9990]], requires_grad=True)\n",
      "new b tensor([  2.8314, -16.8432], requires_grad=True)\n",
      "Loss= tensor(1.4750, grad_fn=<DivBackward0>)\n",
      "i= 950\n",
      "w.grad= tensor([[ 0.1066, -0.3595,  1.1446],\n",
      "        [-1.9626,  0.4242, -2.7452]])\n",
      "b.grad= tensor([-40.4646, 280.3221])\n",
      "new w tensor([[-0.4244,  0.8439,  0.6801],\n",
      "        [-0.1690,  0.8015,  0.9992]], requires_grad=True)\n",
      "new b tensor([  2.8334, -16.8572], requires_grad=True)\n",
      "Loss= tensor(1.4766, grad_fn=<DivBackward0>)\n",
      "i= 951\n",
      "w.grad= tensor([[ 0.1082, -0.3567,  1.1412],\n",
      "        [-1.9616,  0.4238, -2.7399]])\n",
      "b.grad= tensor([-40.4197, 280.1690])\n",
      "new w tensor([[-0.4244,  0.8439,  0.6800],\n",
      "        [-0.1689,  0.8015,  0.9993]], requires_grad=True)\n",
      "new b tensor([  2.8355, -16.8712], requires_grad=True)\n",
      "Loss= tensor(1.4781, grad_fn=<DivBackward0>)\n",
      "i= 952\n",
      "w.grad= tensor([[ 0.1080, -0.3560,  1.1364],\n",
      "        [-1.9628,  0.4205, -2.7363]])\n",
      "b.grad= tensor([-40.3747, 280.0159])\n",
      "new w tensor([[-0.4244,  0.8439,  0.6799],\n",
      "        [-0.1688,  0.8015,  0.9994]], requires_grad=True)\n",
      "new b tensor([  2.8375, -16.8852], requires_grad=True)\n",
      "Loss= tensor(1.4797, grad_fn=<DivBackward0>)\n",
      "i= 953\n",
      "w.grad= tensor([[ 0.1091, -0.3538,  1.1326],\n",
      "        [-1.9616,  0.4201, -2.7310]])\n",
      "b.grad= tensor([-40.3297, 279.8627])\n",
      "new w tensor([[-0.4245,  0.8440,  0.6799],\n",
      "        [-0.1687,  0.8015,  0.9996]], requires_grad=True)\n",
      "new b tensor([  2.8395, -16.8992], requires_grad=True)\n",
      "Loss= tensor(1.4813, grad_fn=<DivBackward0>)\n",
      "i= 954\n",
      "w.grad= tensor([[ 0.1094, -0.3527,  1.1282],\n",
      "        [-1.9617,  0.4181, -2.7268]])\n",
      "b.grad= tensor([-40.2848, 279.7094])\n",
      "new w tensor([[-0.4245,  0.8440,  0.6798],\n",
      "        [-0.1686,  0.8015,  0.9997]], requires_grad=True)\n",
      "new b tensor([  2.8415, -16.9132], requires_grad=True)\n",
      "Loss= tensor(1.4829, grad_fn=<DivBackward0>)\n",
      "i= 955\n",
      "w.grad= tensor([[ 0.1102, -0.3509,  1.1242],\n",
      "        [-1.9612,  0.4169, -2.7220]])\n",
      "b.grad= tensor([-40.2398, 279.5560])\n",
      "new w tensor([[-0.4245,  0.8440,  0.6798],\n",
      "        [-0.1685,  0.8014,  0.9999]], requires_grad=True)\n",
      "new b tensor([  2.8435, -16.9271], requires_grad=True)\n",
      "Loss= tensor(1.4845, grad_fn=<DivBackward0>)\n",
      "i= 956\n",
      "w.grad= tensor([[ 0.1102, -0.3500,  1.1197],\n",
      "        [-1.9608,  0.4155, -2.7174]])\n",
      "b.grad= tensor([-40.1949, 279.4026])\n",
      "new w tensor([[-0.4245,  0.8440,  0.6797],\n",
      "        [-0.1684,  0.8014,  1.0000]], requires_grad=True)\n",
      "new b tensor([  2.8455, -16.9411], requires_grad=True)\n",
      "Loss= tensor(1.4861, grad_fn=<DivBackward0>)\n",
      "i= 957\n",
      "w.grad= tensor([[ 0.1112, -0.3480,  1.1158],\n",
      "        [-1.9608,  0.4138, -2.7130]])\n",
      "b.grad= tensor([-40.1500, 279.2491])\n",
      "new w tensor([[-0.4245,  0.8440,  0.6797],\n",
      "        [-0.1683,  0.8014,  1.0001]], requires_grad=True)\n",
      "new b tensor([  2.8475, -16.9551], requires_grad=True)\n",
      "Loss= tensor(1.4877, grad_fn=<DivBackward0>)\n",
      "i= 958\n",
      "w.grad= tensor([[ 0.1121, -0.3460,  1.1120],\n",
      "        [-1.9605,  0.4123, -2.7084]])\n",
      "b.grad= tensor([-40.1050, 279.0954])\n",
      "new w tensor([[-0.4245,  0.8441,  0.6796],\n",
      "        [-0.1682,  0.8014,  1.0003]], requires_grad=True)\n",
      "new b tensor([  2.8495, -16.9690], requires_grad=True)\n",
      "Loss= tensor(1.4893, grad_fn=<DivBackward0>)\n",
      "i= 959\n",
      "w.grad= tensor([[ 0.1114, -0.3460,  1.1070],\n",
      "        [-1.9602,  0.4108, -2.7039]])\n",
      "b.grad= tensor([-40.0601, 278.9417])\n",
      "new w tensor([[-0.4245,  0.8441,  0.6796],\n",
      "        [-0.1681,  0.8014,  1.0004]], requires_grad=True)\n",
      "new b tensor([  2.8515, -16.9830], requires_grad=True)\n",
      "Loss= tensor(1.4909, grad_fn=<DivBackward0>)\n",
      "i= 960\n",
      "w.grad= tensor([[ 0.1130, -0.3433,  1.1037],\n",
      "        [-1.9597,  0.4095, -2.6993]])\n",
      "b.grad= tensor([-40.0152, 278.7880])\n",
      "new w tensor([[-0.4245,  0.8441,  0.6795],\n",
      "        [-0.1680,  0.8013,  1.0005]], requires_grad=True)\n",
      "new b tensor([  2.8535, -16.9969], requires_grad=True)\n",
      "Loss= tensor(1.4925, grad_fn=<DivBackward0>)\n",
      "i= 961\n",
      "w.grad= tensor([[ 0.1131, -0.3424,  1.0993],\n",
      "        [-1.9590,  0.4086, -2.6945]])\n",
      "b.grad= tensor([-39.9703, 278.6341])\n",
      "new w tensor([[-0.4245,  0.8441,  0.6794],\n",
      "        [-0.1679,  0.8013,  1.0007]], requires_grad=True)\n",
      "new b tensor([  2.8555, -17.0108], requires_grad=True)\n",
      "Loss= tensor(1.4941, grad_fn=<DivBackward0>)\n",
      "i= 962\n",
      "w.grad= tensor([[ 0.1136, -0.3410,  1.0952],\n",
      "        [-1.9592,  0.4068, -2.6903]])\n",
      "b.grad= tensor([-39.9254, 278.4802])\n",
      "new w tensor([[-0.4245,  0.8441,  0.6794],\n",
      "        [-0.1678,  0.8013,  1.0008]], requires_grad=True)\n",
      "new b tensor([  2.8575, -17.0248], requires_grad=True)\n",
      "Loss= tensor(1.4957, grad_fn=<DivBackward0>)\n",
      "i= 963\n",
      "w.grad= tensor([[ 0.1143, -0.3393,  1.0914],\n",
      "        [-1.9586,  0.4056, -2.6857]])\n",
      "b.grad= tensor([-39.8805, 278.3261])\n",
      "new w tensor([[-0.4245,  0.8441,  0.6793],\n",
      "        [-0.1677,  0.8013,  1.0009]], requires_grad=True)\n",
      "new b tensor([  2.8595, -17.0387], requires_grad=True)\n",
      "Loss= tensor(1.4973, grad_fn=<DivBackward0>)\n",
      "i= 964\n",
      "w.grad= tensor([[ 0.1151, -0.3376,  1.0875],\n",
      "        [-1.9584,  0.4040, -2.6814]])\n",
      "b.grad= tensor([-39.8356, 278.1720])\n",
      "new w tensor([[-0.4245,  0.8442,  0.6793],\n",
      "        [-0.1676,  0.8013,  1.0011]], requires_grad=True)\n",
      "new b tensor([  2.8615, -17.0526], requires_grad=True)\n",
      "Loss= tensor(1.4989, grad_fn=<DivBackward0>)\n",
      "i= 965\n",
      "w.grad= tensor([[ 0.1152, -0.3366,  1.0832],\n",
      "        [-1.9580,  0.4028, -2.6768]])\n",
      "b.grad= tensor([-39.7907, 278.0179])\n",
      "new w tensor([[-0.4245,  0.8442,  0.6792],\n",
      "        [-0.1675,  0.8012,  1.0012]], requires_grad=True)\n",
      "new b tensor([  2.8635, -17.0665], requires_grad=True)\n",
      "Loss= tensor(1.5005, grad_fn=<DivBackward0>)\n",
      "i= 966\n",
      "w.grad= tensor([[ 0.1159, -0.3350,  1.0794],\n",
      "        [-1.9571,  0.4018, -2.6721]])\n",
      "b.grad= tensor([-39.7459, 277.8636])\n",
      "new w tensor([[-0.4245,  0.8442,  0.6792],\n",
      "        [-0.1674,  0.8012,  1.0013]], requires_grad=True)\n",
      "new b tensor([  2.8655, -17.0804], requires_grad=True)\n",
      "Loss= tensor(1.5021, grad_fn=<DivBackward0>)\n",
      "i= 967\n",
      "w.grad= tensor([[ 0.1166, -0.3333,  1.0756],\n",
      "        [-1.9570,  0.4004, -2.6678]])\n",
      "b.grad= tensor([-39.7010, 277.7092])\n",
      "new w tensor([[-0.4245,  0.8442,  0.6791],\n",
      "        [-0.1673,  0.8012,  1.0015]], requires_grad=True)\n",
      "new b tensor([  2.8675, -17.0943], requires_grad=True)\n",
      "Loss= tensor(1.5037, grad_fn=<DivBackward0>)\n",
      "i= 968\n",
      "w.grad= tensor([[ 0.1180, -0.3308,  1.0722],\n",
      "        [-1.9570,  0.3985, -2.6637]])\n",
      "b.grad= tensor([-39.6561, 277.5548])\n",
      "new w tensor([[-0.4245,  0.8442,  0.6791],\n",
      "        [-0.1672,  0.8012,  1.0016]], requires_grad=True)\n",
      "new b tensor([  2.8695, -17.1082], requires_grad=True)\n",
      "Loss= tensor(1.5054, grad_fn=<DivBackward0>)\n",
      "i= 969\n",
      "w.grad= tensor([[ 0.1176, -0.3303,  1.0678],\n",
      "        [-1.9565,  0.3974, -2.6591]])\n",
      "b.grad= tensor([-39.6113, 277.4003])\n",
      "new w tensor([[-0.4245,  0.8442,  0.6790],\n",
      "        [-0.1671,  0.8012,  1.0017]], requires_grad=True)\n",
      "new b tensor([  2.8714, -17.1220], requires_grad=True)\n",
      "Loss= tensor(1.5070, grad_fn=<DivBackward0>)\n",
      "i= 970\n",
      "w.grad= tensor([[ 0.1177, -0.3294,  1.0635],\n",
      "        [-1.9563,  0.3958, -2.6549]])\n",
      "b.grad= tensor([-39.5664, 277.2457])\n",
      "new w tensor([[-0.4246,  0.8443,  0.6790],\n",
      "        [-0.1670,  0.8011,  1.0019]], requires_grad=True)\n",
      "new b tensor([  2.8734, -17.1359], requires_grad=True)\n",
      "Loss= tensor(1.5086, grad_fn=<DivBackward0>)\n",
      "i= 971\n",
      "w.grad= tensor([[ 0.1186, -0.3277,  1.0599],\n",
      "        [-1.9553,  0.3952, -2.6502]])\n",
      "b.grad= tensor([-39.5216, 277.0910])\n",
      "new w tensor([[-0.4246,  0.8443,  0.6789],\n",
      "        [-0.1669,  0.8011,  1.0020]], requires_grad=True)\n",
      "new b tensor([  2.8754, -17.1497], requires_grad=True)\n",
      "Loss= tensor(1.5102, grad_fn=<DivBackward0>)\n",
      "i= 972\n",
      "w.grad= tensor([[ 0.1192, -0.3262,  1.0561],\n",
      "        [-1.9552,  0.3937, -2.6460]])\n",
      "b.grad= tensor([-39.4767, 276.9362])\n",
      "new w tensor([[-0.4246,  0.8443,  0.6789],\n",
      "        [-0.1668,  0.8011,  1.0021]], requires_grad=True)\n",
      "new b tensor([  2.8774, -17.1636], requires_grad=True)\n",
      "Loss= tensor(1.5118, grad_fn=<DivBackward0>)\n",
      "i= 973\n",
      "w.grad= tensor([[ 0.1197, -0.3247,  1.0523],\n",
      "        [-1.9547,  0.3923, -2.6417]])\n",
      "b.grad= tensor([-39.4319, 276.7814])\n",
      "new w tensor([[-0.4246,  0.8443,  0.6788],\n",
      "        [-0.1667,  0.8011,  1.0023]], requires_grad=True)\n",
      "new b tensor([  2.8793, -17.1774], requires_grad=True)\n",
      "Loss= tensor(1.5134, grad_fn=<DivBackward0>)\n",
      "i= 974\n",
      "w.grad= tensor([[ 0.1201, -0.3234,  1.0484],\n",
      "        [-1.9549,  0.3903, -2.6378]])\n",
      "b.grad= tensor([-39.3870, 276.6265])\n",
      "new w tensor([[-0.4246,  0.8443,  0.6787],\n",
      "        [-0.1666,  0.8011,  1.0024]], requires_grad=True)\n",
      "new b tensor([  2.8813, -17.1913], requires_grad=True)\n",
      "Loss= tensor(1.5151, grad_fn=<DivBackward0>)\n",
      "i= 975\n",
      "w.grad= tensor([[ 0.1209, -0.3217,  1.0447],\n",
      "        [-1.9539,  0.3897, -2.6330]])\n",
      "b.grad= tensor([-39.3422, 276.4715])\n",
      "new w tensor([[-0.4246,  0.8443,  0.6787],\n",
      "        [-0.1665,  0.8010,  1.0025]], requires_grad=True)\n",
      "new b tensor([  2.8833, -17.2051], requires_grad=True)\n",
      "Loss= tensor(1.5167, grad_fn=<DivBackward0>)\n",
      "i= 976\n",
      "w.grad= tensor([[ 0.1213, -0.3204,  1.0409],\n",
      "        [-1.9543,  0.3874, -2.6293]])\n",
      "b.grad= tensor([-39.2974, 276.3164])\n",
      "new w tensor([[-0.4246,  0.8444,  0.6786],\n",
      "        [-0.1664,  0.8010,  1.0027]], requires_grad=True)\n",
      "new b tensor([  2.8852, -17.2189], requires_grad=True)\n",
      "Loss= tensor(1.5183, grad_fn=<DivBackward0>)\n",
      "i= 977\n",
      "w.grad= tensor([[ 0.1217, -0.3191,  1.0370],\n",
      "        [-1.9528,  0.3874, -2.6243]])\n",
      "b.grad= tensor([-39.2526, 276.1612])\n",
      "new w tensor([[-0.4246,  0.8444,  0.6786],\n",
      "        [-0.1664,  0.8010,  1.0028]], requires_grad=True)\n",
      "new b tensor([  2.8872, -17.2327], requires_grad=True)\n",
      "Loss= tensor(1.5199, grad_fn=<DivBackward0>)\n",
      "i= 978\n",
      "w.grad= tensor([[ 0.1222, -0.3178,  1.0333],\n",
      "        [-1.9530,  0.3855, -2.6204]])\n",
      "b.grad= tensor([-39.2077, 276.0060])\n",
      "new w tensor([[-0.4246,  0.8444,  0.6785],\n",
      "        [-0.1663,  0.8010,  1.0029]], requires_grad=True)\n",
      "new b tensor([  2.8892, -17.2465], requires_grad=True)\n",
      "Loss= tensor(1.5216, grad_fn=<DivBackward0>)\n",
      "i= 979\n",
      "w.grad= tensor([[ 0.1227, -0.3164,  1.0295],\n",
      "        [-1.9521,  0.3849, -2.6158]])\n",
      "b.grad= tensor([-39.1629, 275.8506])\n",
      "new w tensor([[-0.4246,  0.8444,  0.6785],\n",
      "        [-0.1662,  0.8010,  1.0031]], requires_grad=True)\n",
      "new b tensor([  2.8911, -17.2603], requires_grad=True)\n",
      "Loss= tensor(1.5232, grad_fn=<DivBackward0>)\n",
      "i= 980\n",
      "w.grad= tensor([[ 0.1227, -0.3155,  1.0255],\n",
      "        [-1.9522,  0.3830, -2.6120]])\n",
      "b.grad= tensor([-39.1181, 275.6952])\n",
      "new w tensor([[-0.4246,  0.8444,  0.6784],\n",
      "        [-0.1661,  0.8009,  1.0032]], requires_grad=True)\n",
      "new b tensor([  2.8931, -17.2741], requires_grad=True)\n",
      "Loss= tensor(1.5248, grad_fn=<DivBackward0>)\n",
      "i= 981\n",
      "w.grad= tensor([[ 0.1239, -0.3133,  1.0223],\n",
      "        [-1.9516,  0.3818, -2.6077]])\n",
      "b.grad= tensor([-39.0733, 275.5397])\n",
      "new w tensor([[-0.4246,  0.8444,  0.6784],\n",
      "        [-0.1660,  0.8009,  1.0033]], requires_grad=True)\n",
      "new b tensor([  2.8950, -17.2879], requires_grad=True)\n",
      "Loss= tensor(1.5265, grad_fn=<DivBackward0>)\n",
      "i= 982\n",
      "w.grad= tensor([[ 0.1238, -0.3126,  1.0182],\n",
      "        [-1.9505,  0.3811, -2.6031]])\n",
      "b.grad= tensor([-39.0285, 275.3842])\n",
      "new w tensor([[-0.4246,  0.8444,  0.6783],\n",
      "        [-0.1659,  0.8009,  1.0034]], requires_grad=True)\n",
      "new b tensor([  2.8970, -17.3016], requires_grad=True)\n",
      "Loss= tensor(1.5281, grad_fn=<DivBackward0>)\n",
      "i= 983\n",
      "w.grad= tensor([[ 0.1243, -0.3113,  1.0145],\n",
      "        [-1.9509,  0.3790, -2.5994]])\n",
      "b.grad= tensor([-38.9837, 275.2285])\n",
      "new w tensor([[-0.4246,  0.8445,  0.6783],\n",
      "        [-0.1658,  0.8009,  1.0036]], requires_grad=True)\n",
      "new b tensor([  2.8989, -17.3154], requires_grad=True)\n",
      "Loss= tensor(1.5297, grad_fn=<DivBackward0>)\n",
      "i= 984\n",
      "w.grad= tensor([[ 0.1251, -0.3094,  1.0112],\n",
      "        [-1.9506,  0.3776, -2.5954]])\n",
      "b.grad= tensor([-38.9390, 275.0727])\n",
      "new w tensor([[-0.4246,  0.8445,  0.6782],\n",
      "        [-0.1657,  0.8009,  1.0037]], requires_grad=True)\n",
      "new b tensor([  2.9009, -17.3291], requires_grad=True)\n",
      "Loss= tensor(1.5314, grad_fn=<DivBackward0>)\n",
      "i= 985\n",
      "w.grad= tensor([[ 0.1256, -0.3081,  1.0075],\n",
      "        [-1.9499,  0.3767, -2.5911]])\n",
      "b.grad= tensor([-38.8942, 274.9169])\n",
      "new w tensor([[-0.4246,  0.8445,  0.6782],\n",
      "        [-0.1656,  0.8009,  1.0038]], requires_grad=True)\n",
      "new b tensor([  2.9028, -17.3429], requires_grad=True)\n",
      "Loss= tensor(1.5330, grad_fn=<DivBackward0>)\n",
      "i= 986\n",
      "w.grad= tensor([[ 0.1255, -0.3074,  1.0035],\n",
      "        [-1.9491,  0.3757, -2.5867]])\n",
      "b.grad= tensor([-38.8494, 274.7610])\n",
      "new w tensor([[-0.4246,  0.8445,  0.6781],\n",
      "        [-0.1655,  0.8008,  1.0040]], requires_grad=True)\n",
      "new b tensor([  2.9048, -17.3566], requires_grad=True)\n",
      "Loss= tensor(1.5347, grad_fn=<DivBackward0>)\n",
      "i= 987\n",
      "w.grad= tensor([[ 0.1259, -0.3061,  0.9999],\n",
      "        [-1.9484,  0.3751, -2.5823]])\n",
      "b.grad= tensor([-38.8046, 274.6050])\n",
      "new w tensor([[-0.4247,  0.8445,  0.6781],\n",
      "        [-0.1654,  0.8008,  1.0041]], requires_grad=True)\n",
      "new b tensor([  2.9067, -17.3704], requires_grad=True)\n",
      "Loss= tensor(1.5363, grad_fn=<DivBackward0>)\n",
      "i= 988\n",
      "w.grad= tensor([[ 0.1270, -0.3041,  0.9967],\n",
      "        [-1.9496,  0.3719, -2.5794]])\n",
      "b.grad= tensor([-38.7599, 274.4489])\n",
      "new w tensor([[-0.4247,  0.8445,  0.6780],\n",
      "        [-0.1653,  0.8008,  1.0042]], requires_grad=True)\n",
      "new b tensor([  2.9087, -17.3841], requires_grad=True)\n",
      "Loss= tensor(1.5380, grad_fn=<DivBackward0>)\n",
      "i= 989\n",
      "w.grad= tensor([[ 0.1273, -0.3029,  0.9931],\n",
      "        [-1.9479,  0.3720, -2.5745]])\n",
      "b.grad= tensor([-38.7151, 274.2928])\n",
      "new w tensor([[-0.4247,  0.8446,  0.6780],\n",
      "        [-0.1652,  0.8008,  1.0043]], requires_grad=True)\n",
      "new b tensor([  2.9106, -17.3978], requires_grad=True)\n",
      "Loss= tensor(1.5396, grad_fn=<DivBackward0>)\n",
      "i= 990\n",
      "w.grad= tensor([[ 0.1272, -0.3022,  0.9891],\n",
      "        [-1.9469,  0.3713, -2.5701]])\n",
      "b.grad= tensor([-38.6703, 274.1365])\n",
      "new w tensor([[-0.4247,  0.8446,  0.6779],\n",
      "        [-0.1651,  0.8008,  1.0045]], requires_grad=True)\n",
      "new b tensor([  2.9125, -17.4115], requires_grad=True)\n",
      "Loss= tensor(1.5412, grad_fn=<DivBackward0>)\n",
      "i= 991\n",
      "w.grad= tensor([[ 0.1284, -0.3001,  0.9860],\n",
      "        [-1.9476,  0.3688, -2.5669]])\n",
      "b.grad= tensor([-38.6256, 273.9802])\n",
      "new w tensor([[-0.4247,  0.8446,  0.6779],\n",
      "        [-0.1650,  0.8007,  1.0046]], requires_grad=True)\n",
      "new b tensor([  2.9145, -17.4252], requires_grad=True)\n",
      "Loss= tensor(1.5429, grad_fn=<DivBackward0>)\n",
      "i= 992\n",
      "w.grad= tensor([[ 0.1289, -0.2988,  0.9825],\n",
      "        [-1.9463,  0.3685, -2.5622]])\n",
      "b.grad= tensor([-38.5808, 273.8238])\n",
      "new w tensor([[-0.4247,  0.8446,  0.6778],\n",
      "        [-0.1649,  0.8007,  1.0047]], requires_grad=True)\n",
      "new b tensor([  2.9164, -17.4389], requires_grad=True)\n",
      "Loss= tensor(1.5445, grad_fn=<DivBackward0>)\n",
      "i= 993\n",
      "w.grad= tensor([[ 0.1290, -0.2978,  0.9788],\n",
      "        [-1.9460,  0.3671, -2.5583]])\n",
      "b.grad= tensor([-38.5361, 273.6673])\n",
      "new w tensor([[-0.4247,  0.8446,  0.6778],\n",
      "        [-0.1648,  0.8007,  1.0049]], requires_grad=True)\n",
      "new b tensor([  2.9183, -17.4526], requires_grad=True)\n",
      "Loss= tensor(1.5462, grad_fn=<DivBackward0>)\n",
      "i= 994\n",
      "w.grad= tensor([[ 0.1294, -0.2967,  0.9751],\n",
      "        [-1.9453,  0.3663, -2.5541]])\n",
      "b.grad= tensor([-38.4913, 273.5108])\n",
      "new w tensor([[-0.4247,  0.8446,  0.6777],\n",
      "        [-0.1647,  0.8007,  1.0050]], requires_grad=True)\n",
      "new b tensor([  2.9202, -17.4663], requires_grad=True)\n",
      "Loss= tensor(1.5478, grad_fn=<DivBackward0>)\n",
      "i= 995\n",
      "w.grad= tensor([[ 0.1303, -0.2948,  0.9721],\n",
      "        [-1.9454,  0.3645, -2.5505]])\n",
      "b.grad= tensor([-38.4466, 273.3541])\n",
      "new w tensor([[-0.4247,  0.8446,  0.6777],\n",
      "        [-0.1646,  0.8007,  1.0051]], requires_grad=True)\n",
      "new b tensor([  2.9222, -17.4799], requires_grad=True)\n",
      "Loss= tensor(1.5495, grad_fn=<DivBackward0>)\n",
      "i= 996\n",
      "w.grad= tensor([[ 0.1297, -0.2946,  0.9679],\n",
      "        [-1.9448,  0.3634, -2.5464]])\n",
      "b.grad= tensor([-38.4019, 273.1974])\n",
      "new w tensor([[-0.4247,  0.8447,  0.6776],\n",
      "        [-0.1645,  0.8006,  1.0052]], requires_grad=True)\n",
      "new b tensor([  2.9241, -17.4936], requires_grad=True)\n",
      "Loss= tensor(1.5511, grad_fn=<DivBackward0>)\n",
      "i= 997\n",
      "w.grad= tensor([[ 0.1305, -0.2930,  0.9646],\n",
      "        [-1.9446,  0.3620, -2.5426]])\n",
      "b.grad= tensor([-38.3571, 273.0406])\n",
      "new w tensor([[-0.4247,  0.8447,  0.6776],\n",
      "        [-0.1644,  0.8006,  1.0054]], requires_grad=True)\n",
      "new b tensor([  2.9260, -17.5072], requires_grad=True)\n",
      "Loss= tensor(1.5528, grad_fn=<DivBackward0>)\n",
      "i= 998\n",
      "w.grad= tensor([[ 0.1312, -0.2913,  0.9615],\n",
      "        [-1.9442,  0.3605, -2.5388]])\n",
      "b.grad= tensor([-38.3124, 272.8837])\n",
      "new w tensor([[-0.4247,  0.8447,  0.6775],\n",
      "        [-0.1643,  0.8006,  1.0055]], requires_grad=True)\n",
      "new b tensor([  2.9279, -17.5209], requires_grad=True)\n",
      "Loss= tensor(1.5545, grad_fn=<DivBackward0>)\n",
      "i= 999\n",
      "w.grad= tensor([[ 0.1311, -0.2908,  0.9576],\n",
      "        [-1.9428,  0.3602, -2.5342]])\n",
      "b.grad= tensor([-38.2677, 272.7267])\n",
      "new w tensor([[-0.4247,  0.8447,  0.6775],\n",
      "        [-0.1642,  0.8006,  1.0056]], requires_grad=True)\n",
      "new b tensor([  2.9298, -17.5345], requires_grad=True)\n",
      "Loss= tensor(1.5561, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = torch.tensor ([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]],)\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = torch.tensor ([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype=torch.float32)\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "print('inputs=', inputs)\n",
    "print('targets=', targets)\n",
    "\n",
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print('w=', w)\n",
    "print('b=', b)\n",
    "\n",
    "# We can define the model as follows:\n",
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "# MSE Cost function\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "#@ represents matrix multiplication in PyTorch, and the .t method returns the transpose of a tensor.\n",
    "# The matrix obtained by passing the input data into the model is a set of predictions for the target variables.\n",
    "\n",
    "for i in range(1000):\n",
    "    print('i=', i)\n",
    "\n",
    "    # Generate predictions\n",
    "    preds = model(inputs.float())\n",
    "    \n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = mse(preds, targets.float())\n",
    "    \n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    print('w.grad=', w.grad)\n",
    "    print('b.grad=', b.grad)\n",
    "\n",
    "    # Adjust weights & reset gradients\n",
    "    # We multiply the gradients with a very small number (10^-5 in this case) to ensure that we don't modify the weights by a very large amount. We want to take a small step in the downhill direction of the gradient, not a giant leap. This number is called the learning rate of the algorithm.\n",
    "    # We use torch.no_grad to indicate to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases.    \n",
    "    # Before we proceed, we reset the gradients to zero by invoking the .zero_() method. We need to do this because PyTorch accumulates gradients. Otherwise, the next time we invoke .backward on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results.   \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= 5e-5 * w.grad \n",
    "        b -= 5e-5 * b.grad  \n",
    "        w.grad.zero_()\n",
    "        w.grad.zero_()\n",
    "    \n",
    "    print('new w', w)\n",
    "    print('new b', b)\n",
    "\n",
    "    print('Loss=', loss)\n",
    "    w.requires_grad = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415a3a7",
   "metadata": {},
   "source": [
    "* First we will create a more complex set of numbers. We can do this by using the **torch.randn()** function that will create 50 numbers that are randomly generated in the interval between 0 an 1. Then, we will stretch the values of x to the values in the interval between 0 to 10 by multiplying x with 10.\n",
    "* Now, we need to create the variable y. For example, we can say that one y is equal to x times 3 minus 4. We will also add some noise to the y. To do this we will use the **torch.randn()** function again, create 50 random numbers and add them to our equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35f6db0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.9592e+00, -1.1559e+00, -1.8130e+01,  5.1026e+00,  9.8498e+00,\n",
      "         4.1219e+00,  3.5523e+00, -1.2832e+01,  1.9814e+01,  3.7433e+00,\n",
      "        -5.3122e-01, -6.9003e-01, -1.2752e+00, -8.9274e+00, -1.0192e+00,\n",
      "         9.1375e+00,  1.6884e-02, -1.3735e+01, -1.6743e+01,  3.1011e+00,\n",
      "        -7.4373e+00,  2.6078e+00, -1.1220e+01,  3.3638e+01,  3.2329e+00,\n",
      "        -1.1468e+00, -4.6981e+00, -7.6215e-01,  1.0173e+01, -8.0256e+00,\n",
      "         1.7872e+01,  3.7482e+00,  1.2960e+01,  2.5794e+00,  2.1208e+00,\n",
      "         1.0533e+01, -1.2189e+01,  8.8070e+00,  4.1140e+00,  3.1796e+00,\n",
      "        -1.6276e+00, -2.7042e+00, -5.7163e+00,  7.3928e+00,  6.5555e+00,\n",
      "         9.9304e+00,  7.1724e+00, -6.6134e+00,  2.1841e+00, -2.9298e+00])\n",
      "tensor([ 25.9278,  -7.3948, -56.3655,  12.0803,  24.8346,   7.6864,   7.4171,\n",
      "        -42.7607,  55.7748,   6.1029,  -5.8227,  -7.5651,  -8.1346, -29.6330,\n",
      "         -6.4996,  24.8029,  -5.6461, -44.7517, -55.6080,   5.2027, -25.4190,\n",
      "          3.7784, -38.9737,  97.0918,   5.0969,  -8.5215, -17.5823,  -6.1356,\n",
      "         27.3798, -29.0243,  51.3679,   5.3943,  34.5484,   3.9468,   2.6537,\n",
      "         27.3734, -42.2652,  21.1750,   9.6370,   5.1339,  -9.1098, -12.2219,\n",
      "        -18.9374,  18.9105,  13.9456,  27.5773,  18.0568, -24.5473,   4.1989,\n",
      "        -10.3521])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = torch.randn(50)\n",
    "x = x * 10\n",
    "y = x * 3 - 4\n",
    "y += torch.randn(50)\n",
    "print (x)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad6107a",
   "metadata": {},
   "source": [
    "* Next step is to create a class called **LinearModel()**. To be able to use it as a PyTorch model, we will pass **torch. nn.Module** as a parameter. Now, we will define the **init function** or the constructor by passing parameter **self**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2650b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(LinearModel, self).__init__()\n",
    "    self.linear = torch.nn.Linear(1, 1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795eacc2",
   "metadata": {},
   "source": [
    "* Then we will call the **super()** function of the linear model. Note, that because this is a linear regression model, so it will have only **one layer**. That is why we will create a **self**. linear variable in which we‚Äôll call the **torch.nn.Linear()** function. This function takes two input parameters. The first one is the **size** of each input sample. This parameter will be equal to 1 because we have only numbers. The second parameter is the shape of the **output** which will also be equal to 1. \n",
    "* Now, we will create the **forward()** function which will take **self** and **x** as inputs. Finally, we will return **self. linear** and we‚Äôll pass in **x** as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "689af23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.9592e+00],\n",
      "        [-1.1559e+00],\n",
      "        [-1.8130e+01],\n",
      "        [ 5.1026e+00],\n",
      "        [ 9.8498e+00],\n",
      "        [ 4.1219e+00],\n",
      "        [ 3.5523e+00],\n",
      "        [-1.2832e+01],\n",
      "        [ 1.9814e+01],\n",
      "        [ 3.7433e+00],\n",
      "        [-5.3122e-01],\n",
      "        [-6.9003e-01],\n",
      "        [-1.2752e+00],\n",
      "        [-8.9274e+00],\n",
      "        [-1.0192e+00],\n",
      "        [ 9.1375e+00],\n",
      "        [ 1.6884e-02],\n",
      "        [-1.3735e+01],\n",
      "        [-1.6743e+01],\n",
      "        [ 3.1011e+00],\n",
      "        [-7.4373e+00],\n",
      "        [ 2.6078e+00],\n",
      "        [-1.1220e+01],\n",
      "        [ 3.3638e+01],\n",
      "        [ 3.2329e+00],\n",
      "        [-1.1468e+00],\n",
      "        [-4.6981e+00],\n",
      "        [-7.6215e-01],\n",
      "        [ 1.0173e+01],\n",
      "        [-8.0256e+00],\n",
      "        [ 1.7872e+01],\n",
      "        [ 3.7482e+00],\n",
      "        [ 1.2960e+01],\n",
      "        [ 2.5794e+00],\n",
      "        [ 2.1208e+00],\n",
      "        [ 1.0533e+01],\n",
      "        [-1.2189e+01],\n",
      "        [ 8.8070e+00],\n",
      "        [ 4.1140e+00],\n",
      "        [ 3.1796e+00],\n",
      "        [-1.6276e+00],\n",
      "        [-2.7042e+00],\n",
      "        [-5.7163e+00],\n",
      "        [ 7.3928e+00],\n",
      "        [ 6.5555e+00],\n",
      "        [ 9.9304e+00],\n",
      "        [ 7.1724e+00],\n",
      "        [-6.6134e+00],\n",
      "        [ 2.1841e+00],\n",
      "        [-2.9298e+00]])\n"
     ]
    }
   ],
   "source": [
    "x_torch = torch.FloatTensor(x).reshape(-1, 1)\n",
    "y_torch = torch.FloatTensor(y).reshape(-1, 1)\n",
    "\n",
    "print(x_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7671db9",
   "metadata": {},
   "source": [
    "* Now when we have defined our data, we can start coding the training part. We will start by creating a variable **model** which will be used for calling our **LinearModel class**. Then, we will calculate the loss by using the function **torch.nn.MSELoss()**. With this function we will calculate the mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8406a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005) # We will talk about this optimzer in next lexture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b69d5",
   "metadata": {},
   "source": [
    "* The next step is to train of our model. First we will create a for loop that will iterate in the range from 0 to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1383407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[741.083984375, 18.929676055908203, 12.738565444946289, 12.465985298156738, 12.246581077575684, 12.031825065612793, 11.821233749389648, 11.614725112915039, 11.412219047546387, 11.213630676269531, 11.018896102905273, 10.827932357788086, 10.640666961669922, 10.457030296325684, 10.276952743530273, 10.100364685058594, 9.927202224731445, 9.757390975952148, 9.59086799621582, 9.42757511138916, 9.267443656921387, 9.110419273376465, 8.956433296203613, 8.805432319641113, 8.657356262207031, 8.512152671813965, 8.369758605957031, 8.23012638092041, 8.0931978225708, 7.95892333984375, 7.827250957489014, 7.6981306076049805, 7.571511268615723, 7.447343826293945, 7.325583457946777, 7.206182479858398, 7.089095592498779, 6.974278450012207, 6.8616838455200195, 6.751272201538086, 6.642999172210693, 6.536825180053711, 6.432708740234375, 6.330606460571289, 6.230486869812012, 6.132303237915039, 6.0360236167907715, 5.9416117668151855, 5.849025726318359, 5.758236885070801, 5.669206142425537, 5.581898212432861, 5.496285438537598, 5.412330150604248, 5.330002307891846, 5.249269008636475, 5.170098304748535, 5.092464923858643, 5.016331672668457, 4.941678047180176, 4.868467807769775, 4.796677112579346, 4.726278781890869, 4.657241344451904, 4.589544773101807, 4.523159503936768, 4.458059787750244, 4.394223213195801, 4.331619739532471, 4.270232677459717, 4.2100324630737305, 4.150999546051025, 4.093109607696533, 4.0363450050354, 3.9806761741638184, 3.926088571548462, 3.87255859375, 3.820064067840576, 3.768589496612549, 3.7181100845336914, 3.6686086654663086, 3.620067834854126, 3.572465181350708, 3.525787115097046, 3.480013132095337, 3.435126304626465, 3.391108512878418, 3.3479440212249756, 3.305616855621338, 3.2641074657440186, 3.2234036922454834, 3.1834890842437744, 3.144348382949829, 3.1059658527374268, 3.068324089050293, 3.031414747238159, 2.9952194690704346, 2.959726572036743, 2.9249210357666016, 2.8907885551452637, 2.8573174476623535, 2.824497699737549, 2.7923104763031006, 2.7607498168945312, 2.729797601699829, 2.699446678161621, 2.669684410095215, 2.6404993534088135, 2.61187744140625, 2.5838122367858887, 2.5562899112701416, 2.5293009281158447, 2.5028364658355713, 2.476882219314575, 2.451432704925537, 2.426476001739502, 2.402001142501831, 2.378002882003784, 2.35446834564209, 2.331389904022217, 2.3087587356567383, 2.286566972732544, 2.2648041248321533, 2.2434635162353516, 2.222534418106079, 2.2020139694213867, 2.181889295578003, 2.1621549129486084, 2.1428022384643555, 2.1238255500793457, 2.105217218399048, 2.0869686603546143, 2.06907320022583, 2.0515248775482178, 2.034316301345825, 2.017442226409912, 2.000894546508789, 1.9846673011779785, 1.9687542915344238, 1.953150749206543, 1.9378482103347778, 1.9228428602218628, 1.9081284999847412, 1.8936978578567505, 1.8795477151870728, 1.8656730651855469, 1.852064609527588, 1.8387210369110107, 1.8256372213363647, 1.81280517578125, 1.8002221584320068, 1.7878836393356323, 1.775783658027649, 1.763918399810791, 1.752282738685608, 1.7408734560012817, 1.729683518409729, 1.7187113761901855, 1.707952857017517, 1.6974018812179565, 1.6870553493499756, 1.6769088506698608, 1.6669590473175049, 1.6572022438049316, 1.6476352214813232, 1.6382522583007812, 1.629051923751831, 1.6200295686721802, 1.611182689666748, 1.6025059223175049, 1.593998670578003, 1.585654854774475, 1.577473759651184, 1.5694513320922852, 1.5615841150283813, 1.5538685321807861, 1.546303153038025, 1.5388846397399902, 1.5316087007522583, 1.5244754552841187, 1.5174793004989624, 1.5106183290481567, 1.503890872001648, 1.497294306755066, 1.4908252954483032, 1.4844810962677002, 1.4782606363296509, 1.4721603393554688, 1.4661774635314941, 1.4603112936019897, 1.4545581340789795, 1.4489175081253052, 1.443385362625122, 1.4379608631134033, 1.4326411485671997, 1.427424669265747, 1.4223088026046753, 1.417292594909668, 1.4123740196228027, 1.4075508117675781, 1.4028195142745972, 1.3981815576553345, 1.3936326503753662, 1.3891719579696655, 1.3847975730895996, 1.3805079460144043, 1.3763022422790527, 1.3721767663955688, 1.3681318759918213, 1.364165186882019, 1.3602756261825562, 1.3564616441726685, 1.3527212142944336, 1.3490535020828247, 1.3454563617706299, 1.3419291973114014, 1.3384708166122437, 1.3350790739059448, 1.3317534923553467, 1.3284908533096313, 1.325292706489563, 1.3221560716629028, 1.3190802335739136, 1.316064476966858, 1.3131070137023926, 1.3102062940597534, 1.3073621988296509, 1.3045735359191895, 1.3018383979797363, 1.2991565465927124, 1.2965264320373535, 1.2939471006393433, 1.2914177179336548, 1.288938045501709, 1.286505937576294, 1.2841212749481201, 1.2817825078964233, 1.2794890403747559, 1.2772399187088013, 1.2750352621078491, 1.272871971130371, 1.2707509994506836, 1.2686717510223389, 1.266632318496704, 1.2646325826644897, 1.2626709938049316, 1.2607483863830566, 1.2588626146316528, 1.2570134401321411, 1.2552003860473633, 1.253421425819397, 1.2516777515411377, 1.2499678134918213, 1.2482904195785522, 1.246646523475647, 1.2450343370437622, 1.2434523105621338, 1.2419016361236572, 1.2403812408447266, 1.2388895750045776, 1.2374281883239746, 1.2359936237335205, 1.2345874309539795, 1.2332088947296143, 1.2318569421768188, 1.2305306196212769, 1.2292299270629883, 1.2279552221298218, 1.2267048358917236, 1.2254786491394043, 1.2242759466171265, 1.2230969667434692, 1.2219411134719849, 1.2208073139190674, 1.2196950912475586, 1.2186050415039062, 1.2175357341766357, 1.2164872884750366, 1.2154589891433716, 1.214450716972351, 1.2134618759155273, 1.2124929428100586, 1.2115412950515747, 1.2106093168258667, 1.2096948623657227, 1.2087985277175903, 1.2079188823699951, 1.2070567607879639, 1.2062113285064697, 1.2053829431533813, 1.204569935798645, 1.2037723064422607, 1.2029906511306763, 1.2022240161895752, 1.201472282409668, 1.2007347345352173, 1.2000120878219604, 1.1993027925491333, 1.1986078023910522, 1.1979255676269531, 1.1972575187683105, 1.1966019868850708, 1.1959589719772339, 1.1953282356262207, 1.1947096586227417, 1.1941041946411133, 1.1935096979141235, 1.1929270029067993, 1.1923549175262451, 1.1917943954467773, 1.1912446022033691, 1.1907051801681519, 1.1901769638061523, 1.1896581649780273, 1.189150094985962, 1.1886519193649292, 1.1881626844406128, 1.1876835823059082, 1.1872130632400513, 1.186752438545227, 1.1863001585006714, 1.1858574151992798, 1.1854225397109985, 1.184996247291565, 1.1845782995224, 1.1841679811477661, 1.1837669610977173, 1.1833720207214355, 1.1829861402511597, 1.1826066970825195, 1.1822353601455688, 1.1818708181381226, 1.1815130710601807, 1.1811628341674805, 1.1808189153671265, 1.1804817914962769, 1.1801514625549316, 1.1798268556594849, 1.1795094013214111, 1.1791976690292358, 1.1788921356201172, 1.178592324256897, 1.1782983541488647, 1.1780102252960205, 1.1777279376983643, 1.1774506568908691, 1.1771787405014038, 1.1769118309020996, 1.1766505241394043, 1.1763942241668701, 1.1761431694030762, 1.175896406173706, 1.1756552457809448, 1.1754177808761597, 1.1751857995986938, 1.1749577522277832, 1.174734115600586, 1.1745150089263916, 1.1742994785308838, 1.1740894317626953, 1.1738827228546143, 1.1736797094345093, 1.1734811067581177, 1.1732864379882812, 1.173095464706421, 1.172907829284668, 1.1727238893508911, 1.1725435256958008, 1.1723672151565552, 1.1721936464309692, 1.172024130821228, 1.171857237815857, 1.1716939210891724, 1.171533465385437, 1.1713767051696777, 1.171222448348999, 1.171071171760559, 1.1709234714508057, 1.1707777976989746, 1.1706359386444092, 1.170495629310608, 1.170358419418335, 1.1702245473861694, 1.1700925827026367, 1.1699633598327637, 1.169836401939392, 1.169711709022522, 1.1695903539657593, 1.1694709062576294, 1.16935396194458, 1.1692386865615845, 1.169126033782959, 1.1690157651901245, 1.1689072847366333, 1.1688015460968018, 1.168696403503418, 1.1685948371887207, 1.1684941053390503, 1.1683963537216187, 1.1683002710342407, 1.1682052612304688, 1.168112874031067, 1.1680220365524292, 1.1679329872131348, 1.1678458452224731, 1.1677600145339966, 1.1676760911941528, 1.1675935983657837, 1.16751229763031, 1.167433261871338, 1.1673552989959717, 1.1672797203063965, 1.1672053337097168, 1.1671319007873535, 1.1670597791671753, 1.1669888496398926, 1.1669198274612427, 1.166852593421936, 1.1667863130569458, 1.166720986366272, 1.1666570901870728, 1.1665942668914795, 1.1665329933166504, 1.1664725542068481, 1.1664135456085205, 1.1663556098937988, 1.1662986278533936, 1.166243076324463, 1.1661884784698486, 1.1661348342895508, 1.1660828590393066, 1.1660305261611938, 1.1659802198410034, 1.1659306287765503, 1.1658822298049927, 1.1658343076705933, 1.165787935256958, 1.1657423973083496, 1.1656972169876099, 1.1656533479690552, 1.1656098365783691, 1.1655675172805786, 1.1655261516571045, 1.1654847860336304, 1.1654454469680786, 1.1654062271118164, 1.165367841720581, 1.165330171585083, 1.165292739868164, 1.1652562618255615, 1.1652212142944336, 1.1651862859725952, 1.1651519536972046, 1.16511869430542, 1.165085792541504, 1.1650539636611938, 1.165022373199463, 1.1649914979934692, 1.1649609804153442, 1.164931058883667, 1.164901614189148, 1.164872646331787, 1.1648451089859009, 1.1648179292678833, 1.1647907495498657, 1.1647636890411377, 1.1647378206253052, 1.16471266746521, 1.164687991142273, 1.1646629571914673, 1.1646397113800049, 1.164615511894226, 1.1645925045013428, 1.1645694971084595, 1.1645479202270508, 1.1645258665084839, 1.164504885673523, 1.1644841432571411, 1.1644634008407593, 1.164443016052246, 1.164422869682312, 1.1644034385681152, 1.164385199546814, 1.1643662452697754, 1.1643481254577637, 1.1643303632736206, 1.164312481880188, 1.1642951965332031, 1.1642783880233765, 1.1642614603042603, 1.1642459630966187, 1.164229393005371, 1.1642143726348877, 1.164198875427246, 1.1641833782196045, 1.1641689538955688, 1.164154291152954, 1.164140224456787, 1.1641265153884888, 1.1641128063201904, 1.1640994548797607, 1.1640866994857788, 1.1640739440917969, 1.1640610694885254, 1.164048671722412, 1.1640361547470093, 1.1640244722366333, 1.1640132665634155, 1.1640011072158813, 1.1639901399612427, 1.163979411125183, 1.1639683246612549, 1.163957953453064, 1.1639482975006104, 1.1639376878738403, 1.1639277935028076, 1.1639177799224854, 1.1639081239700317, 1.1638985872268677, 1.1638895273208618, 1.1638809442520142, 1.16387140750885, 1.163862943649292, 1.1638543605804443, 1.1638461351394653, 1.1638377904891968, 1.1638303995132446, 1.1638216972351074, 1.1638141870498657, 1.1638069152832031, 1.16379976272583, 1.1637920141220093, 1.1637849807739258, 1.1637779474258423, 1.1637712717056274, 1.1637645959854126, 1.1637574434280396, 1.163751482963562, 1.1637449264526367, 1.1637392044067383, 1.163732647895813, 1.1637271642684937, 1.1637206077575684, 1.1637152433395386, 1.1637096405029297, 1.1637033224105835, 1.163698673248291, 1.1636929512023926, 1.1636883020401, 1.1636825799942017, 1.16367769241333, 1.1636732816696167, 1.1636680364608765, 1.1636635065078735, 1.163658857345581, 1.1636539697647095, 1.163649559020996, 1.1636455059051514, 1.1636412143707275, 1.1636370420455933, 1.1636323928833008, 1.1636285781860352, 1.1636250019073486, 1.163620948791504, 1.1636171340942383, 1.163612961769104, 1.1636096239089966, 1.16360604763031, 1.1636021137237549, 1.1635985374450684, 1.1635955572128296, 1.1635923385620117, 1.1635892391204834, 1.1635855436325073, 1.1635823249816895, 1.1635798215866089, 1.1635768413543701, 1.1635733842849731, 1.1635708808898926, 1.1635677814483643, 1.1635652780532837, 1.1635621786117554, 1.1635596752166748, 1.1635568141937256, 1.1635545492172241, 1.163551688194275, 1.163549542427063, 1.1635470390319824, 1.1635441780090332, 1.1635422706604004, 1.1635397672653198, 1.163537621498108, 1.1635349988937378, 1.1635332107543945, 1.1635315418243408, 1.1635291576385498, 1.1635268926620483, 1.163524866104126, 1.1635230779647827, 1.1635210514068604, 1.1635196208953857, 1.1635173559188843, 1.163515567779541, 1.1635137796401978, 1.1635119915008545, 1.1635105609893799, 1.1635088920593262, 1.1635069847106934, 1.16350519657135, 1.1635040044784546, 1.1635023355484009, 1.1635013818740845, 1.1634994745254517, 1.1634979248046875, 1.1634960174560547, 1.1634947061538696, 1.1634937524795532, 1.1634918451309204, 1.163490653038025, 1.1634899377822876, 1.163488507270813, 1.1634869575500488, 1.1634857654571533, 1.1634844541549683, 1.1634831428527832, 1.1634820699691772, 1.1634811162948608, 1.1634798049926758, 1.1634795665740967, 1.1634777784347534, 1.1634767055511475, 1.1634759902954102, 1.1634750366210938, 1.1634740829467773, 1.1634727716445923, 1.1634718179702759, 1.1634711027145386, 1.1634702682495117, 1.1634689569473267, 1.1634681224822998, 1.1634678840637207, 1.163466453552246, 1.1634660959243774, 1.1634647846221924, 1.163464069366455, 1.1634637117385864, 1.163462519645691, 1.1634618043899536, 1.1634609699249268, 1.163460373878479, 1.1634602546691895, 1.1634591817855835, 1.1634583473205566, 1.1634575128555298, 1.163456916809082, 1.1634562015533447, 1.1634559631347656, 1.1634546518325806, 1.1634544134140015, 1.1634544134140015, 1.163453459739685, 1.163452386856079, 1.1634525060653687, 1.1634517908096313, 1.163451075553894, 1.1634502410888672, 1.1634501218795776, 1.1634494066238403, 1.1634490489959717, 1.163448691368103, 1.1634478569030762, 1.1634474992752075, 1.1634472608566284, 1.1634465456008911, 1.1634465456008911, 1.1634455919265747, 1.1634455919265747, 1.163444995880127, 1.1634447574615479, 1.1634441614151, 1.1634435653686523, 1.1634433269500732, 1.163442850112915, 1.163442611694336, 1.1634422540664673, 1.1634420156478882, 1.1634416580200195, 1.1634410619735718, 1.1634410619735718, 1.163440227508545, 1.1634399890899658, 1.1634401082992554, 1.1634395122528076, 1.163439393043518, 1.163439154624939, 1.163438320159912, 1.1634382009506226, 1.1634379625320435, 1.163437843322754, 1.1634374856948853, 1.1634374856948853, 1.163436770439148, 1.1634368896484375, 1.163436770439148, 1.1634358167648315, 1.1634358167648315, 1.1634360551834106, 1.1634351015090942, 1.163435459136963, 1.1634351015090942, 1.163434624671936, 1.163434624671936, 1.1634345054626465, 1.1634336709976196, 1.1634340286254883, 1.1634337902069092, 1.1634331941604614, 1.16343355178833, 1.16343355178833, 1.1634330749511719, 1.1634325981140137, 1.1634330749511719, 1.1634325981140137, 1.1634324789047241, 1.1634323596954346, 1.1634321212768555, 1.1634318828582764, 1.1634315252304077, 1.1634317636489868, 1.1634316444396973, 1.1634312868118286, 1.1634310483932495, 1.1634310483932495, 1.1634310483932495, 1.1634304523468018, 1.1634306907653809, 1.1634306907653809, 1.1634302139282227, 1.1634303331375122, 1.163429617881775, 1.1634299755096436, 1.163429856300354, 1.1634297370910645, 1.163429856300354, 1.1634297370910645, 1.163429617881775, 1.1634292602539062, 1.1634291410446167, 1.1634291410446167, 1.163428783416748, 1.1634284257888794, 1.163428544998169, 1.1634286642074585, 1.163428544998169, 1.163428544998169, 1.1634286642074585, 1.1634283065795898, 1.1634281873703003, 1.1634283065795898, 1.1634283065795898, 1.163427710533142, 1.1634278297424316, 1.1634278297424316, 1.1634271144866943, 1.1634281873703003, 1.163427710533142, 1.163427710533142, 1.163427710533142, 1.1634271144866943, 1.1634268760681152, 1.1634271144866943, 1.1634273529052734, 1.1634269952774048, 1.1634272336959839, 1.1634266376495361, 1.1634267568588257, 1.1634266376495361, 1.1634269952774048, 1.1634266376495361, 1.1634269952774048, 1.1634267568588257, 1.163426399230957, 1.1634265184402466, 1.163426399230957, 1.1634265184402466, 1.163426160812378, 1.1634259223937988, 1.1634266376495361, 1.163426160812378, 1.1634260416030884, 1.163426160812378, 1.163426160812378, 1.1634259223937988, 1.1634260416030884, 1.1634258031845093, 1.1634260416030884, 1.163425326347351, 1.1634255647659302, 1.1634254455566406, 1.1634258031845093, 1.1634254455566406, 1.163425087928772, 1.1634258031845093, 1.1634255647659302, 1.1634254455566406, 1.163425326347351, 1.163425087928772, 1.163425326347351, 1.163425087928772, 1.163425326347351, 1.1634254455566406, 1.1634254455566406, 1.1634252071380615, 1.1634255647659302, 1.163425087928772, 1.1634249687194824, 1.1634246110916138, 1.163425087928772, 1.163425087928772, 1.163425087928772, 1.1634249687194824, 1.1634248495101929, 1.1634248495101929, 1.163425326347351, 1.163425326347351, 1.1634248495101929, 1.1634248495101929, 1.1634242534637451, 1.1634246110916138, 1.1634249687194824, 1.1634249687194824, 1.1634248495101929, 1.1634247303009033, 1.1634249687194824, 1.163425087928772, 1.1634247303009033, 1.1634247303009033, 1.1634247303009033, 1.1634246110916138, 1.1634248495101929, 1.1634249687194824, 1.1634244918823242, 1.1634246110916138, 1.1634244918823242, 1.1634247303009033, 1.1634242534637451, 1.1634244918823242, 1.1634246110916138, 1.1634247303009033, 1.1634244918823242, 1.1634243726730347, 1.1634242534637451, 1.1634243726730347, 1.1634247303009033, 1.1634247303009033, 1.1634242534637451, 1.1634244918823242, 1.1634244918823242, 1.1634243726730347, 1.1634243726730347, 1.1634241342544556, 1.163424015045166, 1.1634243726730347, 1.1634243726730347, 1.1634242534637451, 1.1634243726730347, 1.1634242534637451, 1.1634242534637451, 1.1634243726730347, 1.1634242534637451, 1.1634242534637451, 1.163424015045166, 1.1634238958358765, 1.1634243726730347, 1.1634242534637451, 1.1634244918823242, 1.163424015045166, 1.163424015045166, 1.1634244918823242, 1.1634243726730347, 1.1634242534637451, 1.1634243726730347, 1.1634247303009033, 1.1634243726730347, 1.1634243726730347, 1.163424015045166, 1.1634238958358765, 1.1634238958358765, 1.1634242534637451, 1.1634238958358765, 1.1634238958358765, 1.1634241342544556, 1.163423776626587, 1.1634238958358765, 1.163423776626587, 1.1634236574172974, 1.1634238958358765, 1.1634242534637451, 1.1634238958358765, 1.163424015045166, 1.163424015045166, 1.1634238958358765, 1.163424015045166, 1.163424015045166, 1.1634238958358765, 1.1634235382080078, 1.1634236574172974, 1.1634236574172974, 1.1634236574172974, 1.1634234189987183, 1.163424015045166, 1.163423776626587, 1.163424015045166, 1.1634241342544556, 1.1634234189987183, 1.163423776626587, 1.163424015045166, 1.1634238958358765, 1.163423776626587, 1.163423776626587, 1.1634238958358765, 1.163424015045166, 1.1634241342544556, 1.1634238958358765, 1.163424015045166, 1.1634242534637451, 1.1634242534637451, 1.1634242534637451, 1.1634241342544556, 1.1634241342544556, 1.1634242534637451, 1.1634241342544556, 1.1634242534637451, 1.1634241342544556, 1.163424015045166, 1.1634238958358765, 1.163423776626587, 1.1634238958358765, 1.1634236574172974, 1.1634235382080078, 1.1634236574172974, 1.1634236574172974, 1.163423776626587, 1.1634241342544556, 1.1634236574172974, 1.1634236574172974, 1.1634238958358765, 1.1634243726730347, 1.163424015045166, 1.1634238958358765, 1.1634243726730347, 1.1634238958358765, 1.1634238958358765, 1.1634238958358765, 1.1634238958358765, 1.1634238958358765, 1.1634238958358765, 1.1634241342544556, 1.1634238958358765, 1.1634241342544556, 1.1634238958358765, 1.1634238958358765, 1.1634236574172974, 1.163424015045166, 1.1634236574172974, 1.1634238958358765, 1.1634238958358765, 1.1634241342544556, 1.1634238958358765, 1.163423776626587, 1.1634236574172974, 1.1634234189987183, 1.1634235382080078, 1.1634235382080078, 1.1634232997894287, 1.1634235382080078, 1.1634234189987183, 1.1634234189987183, 1.1634236574172974, 1.1634234189987183, 1.1634234189987183, 1.1634234189987183, 1.1634236574172974, 1.1634235382080078, 1.1634236574172974, 1.1634236574172974, 1.163424015045166, 1.1634236574172974, 1.1634236574172974, 1.163423776626587, 1.1634238958358765, 1.1634238958358765, 1.163424015045166, 1.1634238958358765, 1.1634238958358765, 1.1634238958358765, 1.163423776626587, 1.1634238958358765, 1.1634238958358765, 1.1634238958358765]\n"
     ]
    }
   ],
   "source": [
    "all_loss = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "  y_hat = model(x_torch)\n",
    "\n",
    "  loss = criterion(y_hat, y_torch)\n",
    "  loss.backward()\n",
    "  all_loss.append(loss.item())\n",
    "\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "    \n",
    "print (all_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c09565",
   "metadata": {},
   "source": [
    "* Now, let‚Äôs make some predictions with the model.forward() function and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "552eb94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 26.0181],\n",
      "        [ -7.4451],\n",
      "        [-58.5462],\n",
      "        [ 11.3970],\n",
      "        [ 25.6888],\n",
      "        [  8.4443],\n",
      "        [  6.7296],\n",
      "        [-42.5964],\n",
      "        [ 55.6877],\n",
      "        [  7.3047],\n",
      "        [ -5.5644],\n",
      "        [ -6.0425],\n",
      "        [ -7.8044],\n",
      "        [-30.8420],\n",
      "        [ -7.0335],\n",
      "        [ 23.5444],\n",
      "        [ -3.9143],\n",
      "        [-45.3156],\n",
      "        [-54.3713],\n",
      "        [  5.3711],\n",
      "        [-26.3561],\n",
      "        [  3.8860],\n",
      "        [-37.7447],\n",
      "        [ 97.3053],\n",
      "        [  5.7679],\n",
      "        [ -7.4177],\n",
      "        [-18.1093],\n",
      "        [ -6.2596],\n",
      "        [ 26.6616],\n",
      "        [-28.1271],\n",
      "        [ 49.8394],\n",
      "        [  7.3193],\n",
      "        [ 35.0524],\n",
      "        [  3.8005],\n",
      "        [  2.4200],\n",
      "        [ 27.7455],\n",
      "        [-40.6621],\n",
      "        [ 22.5493],\n",
      "        [  8.4205],\n",
      "        [  5.6074],\n",
      "        [ -8.8651],\n",
      "        [-12.1065],\n",
      "        [-21.1747],\n",
      "        [ 18.2919],\n",
      "        [ 15.7708],\n",
      "        [ 25.9316],\n",
      "        [ 17.6282],\n",
      "        [-23.8754],\n",
      "        [  2.6103],\n",
      "        [-12.7855]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.forward(x_torch)\n",
    "print (y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d9446",
   "metadata": {},
   "source": [
    "## Multivariable linear regression\n",
    "* Given a dataset ${\\cal D} = \\{(x^{(i)},y^{(i)})\\}_{i=1}^N$, Each observation $i$ includes a scalar response $y_{i}$ and a column vector ${x}_{i}$ of $p$ parameters (regressors), $x^{(i)}=\\left\\{ x_{i1},x_{i2},\\cdots,x_{im}\\right\\} ^{T}$\n",
    "* Data $x^{(i)} \\in \\mathbb{R}^d$\n",
    "\n",
    "* Want to predict a scalar $\\hat y$ as a function of a scalar $x$<br>\n",
    "* Model: $\\hat y$ is a linear function of $x$:<br>\n",
    "> $\\it\\hat y = wx + b$<br>\n",
    "> $\\hat y$ is the prediction<br>\n",
    "> $w$ is the weight<br>\n",
    "> $b$ is the bias<br>\n",
    "* $w$ and $b$ together are the parameters<br>\n",
    "* In linear regression square loss is the most common loss function:\n",
    ">$L_s(y,\\hat y)=(y^{(i)}-\\hat y^{(i)})^{2}$\n",
    "* In a linear regression model, the response variable, $y_{i}$, is a linear function of the regressors and can be presented in the form of a general equation as follows::\n",
    "$y^{(i)}=w_{1}x_{i1}+w_{2}x_{i2}+\\cdots+w_{m}x_{im}+e_{i}$ \n",
    "* The general equation can be written in vector form  \n",
    "$\\left[\\begin{array}{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}\n",
    "\\end{array}\\right]$=$\\left[\\begin{array}{ccccc}\n",
    "x_{11} & x_{12} & x_{13} & \\cdots & x_{1m}\\\\\n",
    "x_{21} & x_{22} & x_{23} & \\cdots & x_{2m}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{n2} & x_{n3} & \\cdots & x_{nm}\n",
    "\\end{array}\\right]$$\\left[\\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "\\vdots\\\\\n",
    "w_{n}\n",
    "\\end{array}\\right]+\\left[\\begin{array}{c}\n",
    "e_{1}\\\\\n",
    "e_{2}\\\\\n",
    "\\vdots\\\\\n",
    "e_{n}\n",
    "\\end{array}\\right]$\n",
    "* It is equal to $\\boldsymbol{Y}=\\boldsymbol{\\boldsymbol{X}W}+\\boldsymbol{E}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9b252",
   "metadata": {},
   "source": [
    "* regression coefficients are obtained by minimizing the residual squares, so that the normal equation is obtained as follows:<br>\n",
    "$nw_{0}+w_{1}\\sum x_{1j}+w_{2}\\sum x_{2j}+\\cdots+w_{m}\\sum x_{m}=\\sum y_{j}$ <br>\n",
    "$w_{0}\\sum x_{1j}+w_{1}\\sum x_{2j}^{2}+w_{2}\\sum x_{1j}x_{2j}+\\cdots+w_{m}\\sum x_{1j}x_{mj}=\\sum x_{1j}y_{j}$<br>\n",
    "$w_{0}\\sum x_{2j}+w_{1}\\sum x_{1j}x_{2j}+w_{2}\\sum x_{2j}^{2}+\\cdots+w_{m}\\sum x_{2j}x_{mj}=\\sum x_{2j}y_{j}$<br>\n",
    "$\\vdots$<br>\n",
    "$w_{0}\\sum x_{mj}+w_{1}\\sum x_{1j}x_{mj}+w_{2}\\sum x_{2j}x_{mj}+\\cdots+w_{m}\\sum x_{mj}^{2}=\\sum x_{mj}y_{j}$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f560241",
   "metadata": {},
   "source": [
    "* Equations can be written in matrix notation as follows:  \n",
    "$\\left[\\begin{array}{ccccc}\n",
    "n & \\sum x_{1j} & \\sum x_{2j} & \\cdots & \\sum x_{mj}\\\\\n",
    "\\sum x_{1j} & \\sum x_{1j}^{2} & \\sum x_{1j}x_{2j} & \\cdots & \\sum x_{1j}x_{mj}\\\\\n",
    "\\sum x_{2j} & \\sum x_{1j}x_{2j} & \\sum x_{2j}^{2} & \\cdots & \\sum x_{2j}x_{mj}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\sum x_{mj} & \\sum x_{1j}x_{mj} & \\sum x_{2j}x_{mj} & \\cdots & \\sum x_{mj}^{2}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "\\vdots\\\\\n",
    "w_{m}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\sum y_{j}\\\\\n",
    "\\sum x_{1j}y_{j}\\\\\n",
    "\\sum x_{2j}y_{j}\\\\\n",
    "\\vdots\\\\\n",
    "\\sum x_{mj}y_{j}\n",
    "\\end{array}\\right]$\n",
    "* It is equal to  $\\boldsymbol{XW}=\\boldsymbol{Y}$  \n",
    "* where $\\boldsymbol{X}$ matrix is called the covariance matrix."
   ]
  },
  {
   "attachments": {
    "Cholesky.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a3f1edd2",
   "metadata": {},
   "source": [
    "* Cholesky Decomposition:\n",
    "$\\boldsymbol{X}$ is a definite symmetric and positive\n",
    "matrix, $X_{ij}=X_{ji}$, then $\\boldsymbol{X}$ can be written  \n",
    "$\\boldsymbol{X}=\\boldsymbol{L}\\boldsymbol{L}^{T}$  \n",
    "Where $\\boldsymbol{L}$ is the bottom triangle matrix which is defined as follows:  \n",
    "$\\boldsymbol{L}=\\left[\\begin{array}{cccc}\n",
    "l_{11} & 0 & \\cdots & 0\\\\\n",
    "l_{12} & l_{22} & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "l_{n1} & l_{n2} & \\cdots & l_{nn}\n",
    "\\end{array}\\right]$ <br>\n",
    "Where  \n",
    "<img src=\"attachment:Cholesky.png\" width= \"300\"/> </div> <br>\n",
    "\n",
    "* To solve $\\boldsymbol{XW}=\\boldsymbol{Y}$ then solution $\\boldsymbol{W}$ is\n",
    "obtained by: (a) forward substitution: solution $d$ uses $\\boldsymbol{Ld}=\\boldsymbol{W}$, then (b) back substitution:\n",
    "solution $\\boldsymbol{W}$ uses $\\boldsymbol{L^{T}W}=\\boldsymbol{d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76646b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
