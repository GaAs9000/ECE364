{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847696bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ECE 364 Lecture 12 Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597c77e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Linear Regression (LR)?  \n",
    "* Linear regression (LR) models the linear relationship between the independent (X) variable with that of the dependent variable (y).\n",
    "For example, how the likelihood of blood pressure is influenced by a person‚Äôs age and weight. This relationship can be explained using linear regression.\n",
    "* In LR, the y variable should be continuous, whereas the X variable can be continuous or categorical. If both X and y are continuous, the linear relationship can be estimated using correlation coefficient (r) or the coefficient of determination (R-Squared)\n",
    "* LR is useful if the relationships between the X and y variables are linear\n",
    "* LR is helpful to predict the value of y based on the value of the X variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa07317f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iinear Regression<br>\n",
    "* Given a dataset ${\\cal D} = \\{(x^{(i)},y^{(i)})\\}_{i=1}^N$\n",
    "* Data $x^{(i)} \\in \\mathbb{R}^d$\n",
    "* **Question:** Given new unseen data $ùë•$ how to predict its label $ùë¶$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebffd03",
   "metadata": {},
   "source": [
    "* For example: likelihood of blood pressure is influenced by a person‚Äôs age.<br>\n",
    "    This relationship can be explained using linear regression.\n",
    "* We have a data set of blood pressure vs ages from 20 patients. They can be ploted as following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04620623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEmCAYAAACEQCxyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsy0lEQVR4nO3de1xUdf4/8NeB4SK3SVAZUDQvCIsoomiipSKIioqt5qXykqWrbmkqWWkZauuqmWLlpqIkulq23y5u7boi7jcvhUnAkoB3RUWFB19BAZGbzPn9wY/ZRkTnwBnmcl7Px2MeD85lzrw5D50353N7C6IoiiAiIkWyMXUARERkOkwCREQKxiRARKRgTAJERArGJEBEpGBMAkRECsYkQESkYEwCREQKpjJ1AOZAq9Xi5s2bcHV1hSAIpg6HiKjZRFFEWVkZvL29YWPT+N/7TAIAbt68CR8fH1OHQUQku7y8PHTo0KHR40wCAFxdXQHU3Sw3NzcTR0NE1HylpaXw8fHRfb81hkkA0DUBubm5MQkQkVV5XBM3O4aJiBSMSYCISMHYHEREZKZqtSJSc4tRWFaJdq6O6N/ZHbY28o5gZBIgIjJDB7PzsfL708gvqdTt81I7InZsAEYGesn2OWwOIiIyMwez8zFvT4ZeAgCAgpJKzNuTgYPZ+bJ9FpMAEZEZqdWKWPn9aTys5GP9vpXfn0atVp6ikEwCRERmJDW3uMETwG+JAPJLKpGaWyzL5zEJEBGZkcKyxhNAU857HCYBIiIz0s7VUdbzHodJgIjIjPTv7A4vtSMaGwgqoG6UUP/O7rJ8HpMAEZEZsbUREDs2AAAaJIL67dixAbLNF2ASICIyMyMDvbBlah9o1PpNPhq1I7ZM7SPrPAGDJot9/PHHki88c+bMx65eR0REDzcy0AvDAzRGnzEsiKL42MGmNjY26NChA2xtbQ26aF5eHs6fP48uXbo0O8CWUFpaCrVajZKSEq4iSkRWwdDvNYOXjUhLS0O7du0MOpdPAERElsGgPoHY2Fi4uLgYfNFly5bB3V2enmsiIjIeg5qDrB2bg4jI2hj6vcbRQURECiZ5Keng4OCHlisTBAGOjo7o1q0bXnrpJYSFhckSIBERGY/kJ4GRI0fi8uXLcHZ2RlhYGIYOHQoXFxdcunQJ/fr1Q35+PiIiIvD3v//dGPESEZGMJD8J3Lp1CzExMVi+fLne/j/96U+4evUqDh06hNjYWLz//vsYN26cbIESEZH8JHcMq9VqpKeno1u3bnr7L168iL59+6KkpARnz55Fv379UFZWJmuwxsKOYSKyNkbrGHZ0dERKSkqD/SkpKXB0rJvirNVq4eDgIPXSRETUwiQ3B82fPx9z585Feno6+vXrB0EQkJqaih07dmDZsmUAgKSkJAQHB8seLBERyatJ8wT27t2LzZs349y5cwAAPz8/zJ8/Hy+88AIAoKKiQjdayBKwOYiIrI2h32ucLAYmASKyPpwsRkREj2Vwn0Dr1q0fOknsQcXF8hQ/JiIi4zM4CWzatEn3syiKmDdvHlatWmXwyqJERGR+mtwn4Orqil9//dViagY8CvsEiMjasE+AiIgei0mAiEjBTJoEjh07hrFjx8Lb2xuCIGD//v2NnjtnzhwIgqDXNwEAQ4cOhSAIeq8pU6YYN3AiIithcMfw4sWL9barq6uxevVqqNVqvf0bN240+MPLy8sRFBSEmTNnYsKECY2et3//fpw8eRLe3t4PPT579mysWrVKt92qVSuDYyAiUjKDk8B//vMfve2BAwfi8uXLevsMGUL6W6NGjcKoUaMeec6NGzfw2muvISkpCaNHj37oOU5OTtBoNJI+m4iIJCSBH374wZhxPJRWq8W0adOwZMkS9OjRo9Hz9u7diz179sDT0xOjRo1CbGzsI4vdV1VVoaqqSrddWloqa9xERJZC8gJyLWndunVQqVRYsGBBo+e8+OKL6Ny5MzQaDbKzs7F06VL8+uuvSE5ObvQ9a9aswcqVK40RMhGRRTHbJJCeno6PPvoIGRkZj2xmmj17tu7nwMBA+Pr6IiQkBBkZGejTp89D37N06VK9Po7S0lL4+PjIFzwRkYUw2yGix48fR2FhITp27AiVSgWVSoWrV68iJiYGTz75ZKPv69OnD+zs7HDhwoVGz3FwcICbm5vei4hIicz2SWDatGmIiIjQ2zdixAhMmzYNM2fObPR9OTk5qKmpgZeXl7FDJCKyeCZNAnfv3sXFixd127m5ucjMzIS7uzs6duwIDw8PvfPt7Oyg0Wjg5+cHALh06RL27t2LqKgotGnTBqdPn0ZMTAyCg4MxaNCgFv1diIgsUZOSQGVlJU6dOoXCwkJotVq9Y9HR0QZfJy0tDWFhYbrt+nb6GTNmIDEx8bHvt7e3x7///W989NFHuHv3Lnx8fDB69GjExsbC1tbW4DiIiJRK8gJyBw8exPTp03Hr1q2GFxME1NbWyhZcS+ECckRkbYy2gNxrr72GiRMnIj8/H1qtVu9liQmAiEjJJCeBwsJCLF68GJ6ensaIh4iIWpDkJPDcc8/hyJEjRgiFiIhamuQ+gXv37mHixIlo27YtevbsCTs7O73jj5rda67YJ0BE1sbQ7zXJo4M+//xzJCUloVWrVjhy5IjebF5BECwyCRARKZXkJPDuu+9i1apVePvtt2FjY7YTjomIyACSv8Wrq6sxefJkJgAiIisg+Zt8xowZ+PLLL40RCxERtTDJzUG1tbX44IMPkJSUhF69ejXoGJZSWYyIiExLchLIyspCcHAwACA7O1vvmNTKYkREZFqSk4ApKowREZFxsHeXiEjBJD8JVFZW4pNPPsEPP/zw0FVEMzIyZAuOiIiMS3ISePnll5GcnIznnnsO/fv3Zz8AEZEFk5wE/vnPf+LAgQMs2kJEilGrFZGaW4zCskq0c3VE/87usLWxjj+AJSeB9u3bw9XV1RixEBGZnYPZ+Vj5/Wnkl1Tq9nmpHRE7NgAjAy2/jK3kjuENGzbgrbfewtWrV40RDxGR2TiYnY95ezL0EgAAFJRUYt6eDBzMzjdRZPKR/CQQEhKCyspKdOnSBU5OTg0mixUXF8sWHBGRqdRqRaz8/jQetsyyCEAAsPL70xgeoLHopiHJSeD555/HjRs38Oc//xmenp7sGCYiiyGlbT81t7jBE8BviQDySyqRmluM0K4eRorY+CQngZSUFJw4cQJBQUHGiIeIyCiktu0XljWeAJpynrmS3Cfg7++PiooKY8RCRGQUTWnbb+fqaNC1DT3PXElOAmvXrkVMTAyOHDmCoqIilJaW6r2IiMzJ49r2gbq2/Vqt/hn9O7vDS+2Ixhq8BdQ9SfTv7C5jtC1PcnPQyJEjAQDh4eF6+0VRhCAIqK2tlScyIiIZNLVt39ZGQOzYAMzbkwEB0Esi9YkhdmyARXcKA1xAjoisXHPa9kcGemHL1D4N+hI0VjRPQHISGDJkiDHiICIyiua27Y8M9MLwAI3Vzhg2qE/g1KlTDRaKe5ScnBzcv3+/yUEREclFjrZ9WxsBoV09MK53e4R29bCaBAAYmASCg4NRVFRk8EVDQ0Nx7dq1JgdFRCSX+rZ9AA9NBCKso22/qQxqDhJFEcuXL4eTk5NBF62urm5WUEREcqpv23/7myzcuVejd+wJJ7tG3qUMBiWBwYMH49y5cwZfNDQ0FK1atWpyUERExlDyQAKo3zdvTwa2TO1jFR29UhmUBI4cOWLkMIiIjEcp6wA1BctLEpHVkzJXQGmYBIjI6illHaCmkDxPgIioJchZzUsp6wA1BZMAEZkduat51c8VKCipfGi/gIC6WcCWvg5QU0huDqqqqkJ5ebkxYiEiMko1r0fNFbCmdYCawuAkcOvWLYwePRouLi5wc3PDwIEDcfnyZWPGRkQK09QVPw1RP1dAo9Zv8tGoHRU7PBSQ0By0dOlSpKenY+XKlXB0dMTWrVsxZ84cJCcnGzM+IlIQY1fzsvZ1gJrC4CSQlJSEzz77DFFRUQCAqKgoBAYGoqampkGdYSKipmiJUTz16wBRHYObg27evIng4GDdtr+/P+zt7XHz5k2jBEZE1qNWK+LEpSL8PfMGTlwqarQ5h6N4Wp7BTwKiKEKl0j9dpVJJWl2UiJRHykgfjuJpeZKSQHh4uF4iuHfvHsaOHQt7e3vdvoyMDHkjJCKLVT/S58Ev9PqRPg92yCqlmpc5MTgJxMbGNtg3btw4WYMhIuvR1PV6lFDNy5wIoihKH2tlZUpLS6FWq1FSUgI3NzdTh0NkFU5cKsLz239+7HlfzB7w0I5aOWcMK5Gh32vNnjFcXV2N6upquLi4NPdSRGRFmjvSh6N4WoakGcM7d+7E/PnzsXfvXgB1cwdcXV2hVqsxfPhwSdXHiMi6caSPZTA4CaxevRqvvvoqzpw5gwULFmDevHlITEzEqlWrsHbtWpw9exbvvvuuMWMlIgsiR21fMj6Dk0BiYiISEhJw+PBhJCUlIT4+Hp988gneeustLFmyBPHx8Thw4ICkDz927BjGjh0Lb29vCIKA/fv3N3runDlzIAgCNm3apLe/qqoK8+fPR5s2beDs7Izo6Ghcv35dUhxEJD+u12MZDE4C165dw9NPPw0ACAkJgUqlQs+ePXXHe/Xqhfx8aQs7lZeXIygoCJs3b37kefv378fJkyfh7e3d4NjChQvx7bffYt++ffjxxx9x9+5djBkzBrW1tZJiISL5cb0e82dwx3BNTQ0cHBx02/b29nrLRahUKslfvKNGjcKoUaMeec6NGzfw2muvISkpCaNHj9Y7VlJSgoSEBPz1r39FREQEAGDPnj3w8fHB4cOHMWLECEnxEJH8uF6PeZM0Ouj06dMoKCgAUDd57OzZs7h79y6AulVG5abVajFt2jQsWbIEPXr0aHA8PT0dNTU1iIyM1O3z9vZGYGAgUlJSGk0CVVVVqKqq0m2XlpbKHjsR/RdH+pgvSUkgPDwcv51WMGbMGACAIAgQRRGCIG9mX7duHVQqFRYsWPDQ4wUFBbC3t0fr1q319nt6euqS1cOsWbMGK1eulDVWIiJLZHASyM3NNWYcDaSnp+Ojjz5CRkaG5OTyuIS0dOlSLF68WLddWloKHx+fJsdKRGSpDE4CnTp1MmYcDRw/fhyFhYXo2LGjbl9tbS1iYmKwadMmXLlyBRqNBtXV1bh9+7be00BhYSEGDhzY6LUdHBz0+jeIiJTK4NFBxcXFDYZe5uTkYObMmZg0aRI+//xzWQObNm0aTp06hczMTN3L29sbS5YsQVJSEgCgb9++sLOz0ytsk5+fj+zs7EcmASIiqmPwk8Crr74KLy8vbNy4EUDdX9vPPPMMvL290bVrV7z00kuora3FtGnTDP7wu3fv4uLFi7rt3NxcZGZmwt3dHR07doSHh35Hkp2dHTQaDfz8/AAAarUar7zyCmJiYuDh4QF3d3e88cYb6Nmzp260EBE1DdfuUQaDk8DPP/+MnTt36rZ3794Nd3d3ZGZmQqVS4cMPP8Rf/vIXSUkgLS0NYWFhuu36dvoZM2YgMTHRoGvExcVBpVJh0qRJqKioQHh4OBITE2Fra2twHESkT0oNALJsBq8i2qpVK5w9e1bXNxAVFYUePXpg/fr1AIDz588jNDTUItcP4iqiRP/VWA2A+mcATvKyDIZ+rxncJ+Dm5oY7d+7otlNTUzFgwADdtiAIemPvicjyPK4GAFBXA6Cx8pBkeQxOAv3798fHH38MrVaLr776CmVlZRg2bJju+Pnz5znMksjCpeYW6zUBPUgEkF9SidTc4pYLiozK4D6B999/HxEREdizZw/u37+PZcuW6Q3L3LdvH4YMGWKUIImoZTS3BgBZHoOTQO/evXHmzBmkpKRAo9Hgqaee0js+ZcoUBAQEyB4gEbUc1gBQHknLRrRt27bRusIPLu5GRJanvgZAQUnlQ/sFBNStAMoaANZDUmUxIrJurAGgPEwCRKSHNQCUpdmF5onI+rAGgHIwCRDRQ7EGgDKwOYiISMFkTQI2NjYYNmwY0tPT5bwsEREZiaxJ4LPPPsOQIUMarQRGRETmxeAF5KwZF5AjImsj+wJyD7p48SKSkpJQUVEBAGAuISKyPJKTQFFRESIiItC9e3dERUUhPz8fADBr1izExMTIHiARERmP5CSwaNEiqFQqXLt2DU5OTrr9kydPxsGDB2UNjkhparUiTlwqwt8zb+DEpSIu2UxGJ3mewKFDh5CUlIQOHTro7ff19cXVq1dlC4xIaVjNi0xB8pNAeXm53hNAvVu3bsHBwUGWoIiUpr6a14Nr+ReUVGLengwczM43UWRk7SQngcGDB2P37t26bUEQoNVqsX79er16wURK0dwmHFbzIlOS3By0fv16DB06FGlpaaiursabb76JnJwcFBcX46effjJGjERmS44mHCnVvLiMA8lN8pNAQEAATp06hf79+2P48OEoLy/H+PHj8Z///Addu3Y1RoxEZkmuJhxW8yJTkvQkUFNTg8jISGzbtg0rV640VkxEZu9xTTgC6ppwhgdoHrvyJqt5kSlJehKws7NDdnY2BIHLyZKyyVmQvb6aV2P/qwTUNTGxmhcZg+TmoOnTpyMhIcEYsRBZDDmbcFjNi0xJcsdwdXU1duzYgeTkZISEhMDZ2Vnv+MaNG2ULjshcyd2EU1/N68FOZg3nCZCRSU4C2dnZ6NOnDwDg/PnzesfYTERKYYyC7KzmRaYgOQn88MMPxoiDyKLUN+HM25MBAdBLBM1pwmE1L2pprCxG1EQsyE7WQPKTQFhY2CObff73f/+3WQERWRI24ZClk5wEevfurbddU1ODzMxMZGdnY8aMGXLFRWQx2IRDlkxyEoiLi3vo/hUrVuDu3bvNDoiIiFqObH0CU6dOxWeffSbX5YisCusEkLmS/CTQmBMnTsDRkdPaiR7EOgFkziQngfHjx+tti6KI/Px8pKWlYfny5bIFRmQN6heZe/Dv/vpF5jiKiExNchJQq9V62zY2NvDz88OqVasQGRkpW2BElk7OReaIjEVyEti5c6cx4iCyOqwTQJZAcsdwXl4erl+/rttOTU3FwoULER8fL2tgRJaOdQLIEkhOAi+88IJu6YiCggJEREQgNTUVy5Ytw6pVq2QPkMhSsU4AWQLJSSA7Oxv9+/cHAPztb39Dz549kZKSgs8//xyJiYlyx0dksVgngCyB5CRQU1MDBwcHAMDhw4cRHR0NAPD390d+vmHl9IiUgHUCyBJITgI9evTA1q1bcfz4cSQnJ2PkyJEAgJs3b8LDg51bRL/FRebI3EkeHbRu3Tr8/ve/x/r16zFjxgwEBQUBAL777jtdMxER/RcXmSNzJoiiKHn+em1tLUpLS9G6dWvdvitXrsDJyQnt2rWTNcCWUFpaCrVajZKSEri5uZk6HCKiZjP0e01yc1BFRQWqqqp0CeDq1avYtGkTzp07Z5EJgIhIySQngXHjxmH37t0AgDt37uCpp57Chg0b8Oyzz2LLli2yB0hERMYjOQlkZGTgmWeeAQB89dVX8PT0xNWrV7F79258/PHHsgdIRETGIzkJ3Lt3D66urgCAQ4cOYfz48bCxscGAAQNw9epV2QMkIiLjkZwEunXrhv379yMvLw9JSUm6ReMKCwsld6oeO3YMY8eOhbe3NwRBwP79+/WOr1ixAv7+/nB2dkbr1q0RERGBkydP6p0zdOhQCIKg95oyZYrUX4ssCNfmJ5KP5CGi7733Hl544QUsWrQIw4YNQ2hoKIC6p4Lg4GBJ1yovL0dQUBBmzpyJCRMmNDjevXt3bN68GV26dEFFRQXi4uIQGRmJixcvom3btrrzZs+erbdkRatWraT+WmQh5Fqbv1YrcsgmEZo4RLSgoAD5+fkICgqCjU3dw0Rqairc3Nzg7+/ftEAEAd9++y2effbZRs+pH/J0+PBhhIeHA6h7Eujduzc2bdrUpM/97XU5RNS8NbY2f/1Xt6GTr1jkhZTAaENEAUCj0cDV1RXJycmoqKgAAPTr16/JCcAQ1dXViI+Ph1qt1k1Qq7d37160adMGPXr0wBtvvIGysrJHXquqqgqlpaV6LzJvj1ubH6hbm/9xTUP1ieTBJZ7ri7wczObSJ6QskpNAUVERwsPD0b17d0RFRenWC5o1axZiYmJkD/Af//gHXFxc4OjoiLi4OCQnJ6NNmza64y+++CK++OILHDlyBMuXL8fXX3/doPrZg9asWQO1Wq17+fj4yB63EhmzrV7K2vyPik+OREJkTST3CSxatAh2dna4du0afve73+n2T548GYsWLcKGDRtkDTAsLAyZmZm4desWtm/fjkmTJuHkyZO6iWmzZ8/WnRsYGAhfX1+EhIQgIyMDffr0eeg1ly5disWLF+u2S0tLmQiaydhNLHKszc8iL0QNSX4SOHToENatW4cOHTro7ff19TXKEFFnZ2d069YNAwYMQEJCAlQqFRISEho9v0+fPrCzs8OFCxcaPcfBwQFubm56L2q6lmhikWNtfhZ5IWpIchIoLy+Hk5NTg/23bt3SLTFtTKIooqqqqtHjOTk5qKmpgZcXO/haQks1sRi6Nn/fTq0bbZJikReihiQ3Bw0ePBi7d+/G+++/D6BuVI9Wq8X69esRFhYm6Vp3797FxYsXddu5ubnIzMyEu7s7PDw8sHr1akRHR8PLywtFRUX49NNPcf36dUycOBEAcOnSJezduxdRUVFo06YNTp8+jZiYGAQHB2PQoEFSfzVqgpZqYqlfm3/engwIgF7SqU8M0UFeGLL+h0abpOoTSUFJ5UOTloC6JZ5Z5IWURPKTwPr167Ft2zaMGjUK1dXVePPNNxEYGIhjx45h3bp1kq6VlpaG4OBg3fyCxYsXIzg4GO+99x5sbW1x9uxZTJgwAd27d8eYMWPwf//3fzh+/Dh69OgBALC3t8e///1vjBgxAn5+fliwYAEiIyNx+PBh2NraSv3VqAlasonlUWvz/2FwZ8Qfy31kkxSLvBA11OR5Alu2bEF6ejq0Wi369OmDV1991WKbYDhPoOlOXCrC89t/fux5X8weIFtn64MTvfp2at3gCeC36v/C//GtYbC1EThPgBTB0O81Sc1BNTU1iIyMxLZt27By5cpmB0mWzxRNLLY2gl5COXGpSFKTFIu8EP2XpCRgZ2eH7OxsCAL/s1AdQ9rqjd3E0pQmqQcTCZFSSe4TmD59+iOHaJLymLqOLkf9EDWd5NFB1dXV2LFjB5KTkxESEgJnZ2e94xs3bpQtOLIcpmxi4agfoqaTnASys7N1M3HPnz+vd4zNRMrysJU4TdHEYg5NUkSWqkmjg6wNRwdJZ44jbMwxJiJTMfR7TVIS+J//+R/s378fNTU1iIiIwB/+8AdZgjU1JgFp5FrS2RhYJ4CojuxDROPj4zF37lz4+vrC0dERX3/9NXJzc7FmzRpZAibL8LhlIgTULRMxPEBjki9fjvohksbg0UGffPIJ3nnnHZw7dw6//vorEhISsHnzZmPGRmZIjiWdich8GJwELl++jJkzZ+q2p02bhqqqKhQUFBglMDJPXImTyLoYnAQqKirg4uKi27a1tYWDgwPu3btnlMDIPHFMPpF1kTREdMeOHXqJ4P79+0hMTNSr9LVgwQL5oiOzwzH5RNbF4NFBTz755GPnAQiCgMuXL8sSWEvi6CBp6kcHAQ8fk2/K0UFEVEf20UFXrlyRIy6yAvXLRDw4Jl8j05h8DvMkajmSZwwTAcZbJoITvohaFmcMg81B5sKcJ6ERWRpDv9ckryJKZAwtVauYiPQxCZBZ4CQ0ItNgEiCzwEloRKZhUMdwaWmpwRdkmzo1BSehEZmGQUngiSeeMLhWQG1tbbMCImXiJDQi0zAoCfzwww+6n69cuYK3334bL730EkJDQwEAJ06cwK5du7iiKDUZC8MQmYbkIaLh4eGYNWsWnn/+eb39n3/+OeLj43HkyBE542sRHCJqPjhPgEgeRikqAwBOTk749ddf4evrq7f//Pnz6N27t0UuKMckII2xZ/RyxjBR88m+bEQ9Hx8fbN26FRs2bNDbv23bNvj4+EiPlCxKS/ylzsIwRC1HchKIi4vDhAkTkJSUhAEDBgAAfv75Z1y6dAlff/217AGS+WhsRm9BSSXm7cngjF4iCyR5nkBUVBQuXLiA6OhoFBcXo6ioCOPGjcP58+cRFRVljBjJBGq1Ik5cKsLfM2/gxKUiVN/XckYvkRXi2kFgn8CDHtbk4+5sj+Ly6se+94vZA9iUQ2QGjNYnAAB37txBQkICzpw5A0EQEBAQgJdffhlqtbrJAZN5aKzJx5AEAHBGL5GlkdwclJaWhq5duyIuLg7FxcW4desWNm7ciK5duyIjI8MYMVILedQibobijF4iyyL5SWDRokWIjo7G9u3boVLVvf3+/fuYNWsWFi5ciGPHjskeJLWMxy3i9iic0UtkmSQngbS0NL0EAAAqlQpvvvkmQkJCZA2OWlZTm3I4o5fIckluDnJzc8O1a9ca7M/Ly4Orq6ssQZFpGNqU4+5sp7etUTtyeCiRhZL8JDB58mS88sor+PDDDzFw4EAIgoAff/wRS5YsabCUBFkWQxdxO7okDOlXb3NGL5EVkJwEPvzwQwiCgOnTp+P+/fsAADs7O8ybNw9r166VPUClMsXSCYYu4mavsuEwUCIr0eR5Avfu3cOlS5cgiiK6desGJycnuWNrMeY2T8DUi6iZ+vOJqPmMtoDcb12/fh2CIKB9+/ZNvYRZMKckYC7F1rmIG5FlM1qhea1Wi1WrVkGtVqNTp07o2LEjnnjiCbz//vvQarXNClrpzKnYev0ibuN6t0doVw8mACIrJblP4J133kFCQgLWrl2LQYMGQRRF/PTTT1ixYgUqKyuxevVqY8SpCFKKrbNNnojkIDkJ7Nq1Czt27EB0dLRuX1BQENq3b48//vGPTALNwGLrRNTSJDcHFRcXw9/fv8F+f39/FBcXyxKUUrHYOhG1NMlJICgoCJs3b26wf/PmzQgKCpIlKKWqH6ffWOu7gLpROlyagYjkIrk56IMPPsDo0aNx+PBhhIaGQhAEpKSkIC8vDwcOHDBGjIrBYutE1NIkPwkMGTIE58+fx+9//3vcuXMHxcXFGD9+PM6dO4dnnnnGGDEqyshAL2yZ2gcatX6TD5dmICJjYFEZmNc8gXocp09EzSFrUZlTp04Z/MG9evUy+FxqHIutE1FLMKg5qHfv3ggODkbv3r0f+QoODpb04ceOHcPYsWPh7e0NQRCwf/9+veMrVqyAv78/nJ2d0bp1a0RERODkyZN651RVVWH+/Plo06YNnJ2dER0djevXr0uKg4hIqQx6EsjNzTXKh5eXlyMoKAgzZ87EhAkTGhzv3r07Nm/ejC5duqCiogJxcXGIjIzExYsX0bZtWwDAwoUL8f3332Pfvn3w8PBATEwMxowZg/T0dNja2holbiIia2E2fQKCIODbb7/Fs88+2+g59W1chw8fRnh4OEpKStC2bVv89a9/xeTJkwEAN2/ehI+PDw4cOIARI0YY9Nnm2CdARNQcRls7qKioSPdzXl4e3nvvPSxZsgTHjx9vWqQGqq6uRnx8PNRqtW4+Qnp6OmpqahAZGak7z9vbG4GBgUhJSWn0WlVVVSgtLdV7EREpkcFJICsrC08++STatWsHf39/ZGZmol+/foiLi0N8fDzCwsIatOnL4R//+AdcXFzg6OiIuLg4JCcno02bNgCAgoIC2Nvbo3Xr1nrv8fT0REFBQaPXXLNmDdRqte7l4+Mje9xERJbA4CTw5ptvomfPnjh69CiGDh2KMWPGICoqCiUlJbh9+zbmzJljlKIyYWFhyMzMREpKCkaOHIlJkyahsLDwke8RRRGC0PhwyqVLl6KkpET3ysvLkztsIiKLYHAS+OWXX7B69Wo8/fTT+PDDD3Hz5k388Y9/hI2NDWxsbDB//nycPXtW9gCdnZ3RrVs3DBgwAAkJCVCpVEhISAAAaDQaVFdX4/bt23rvKSwshKenZ6PXdHBwgJubm96LiEiJDE4CxcXF0Gg0AAAXFxc4OzvD3f2/a9i0bt0aZWVl8kf4AFEUUVVVBQDo27cv7OzskJycrDuen5+P7OxsDBw40OixEBFZOklrBz3YxPKoJhdD3L17FxcvXtRt5+bmIjMzE+7u7vDw8MDq1asRHR0NLy8vFBUV4dNPP8X169cxceJEAIBarcYrr7yCmJgYeHh4wN3dHW+88QZ69uyJiIiIZsX2OJzRS0TWQFISeOmll+Dg4AAAqKysxNy5c+Hs7AwAur/OpUhLS0NYWJhue/HixQCAGTNmYOvWrTh79ix27dqFW7duwcPDA/369cPx48fRo0cP3Xvi4uKgUqkwadIkVFRUIDw8HImJiUadI8AavERkLQyeJzBz5kyDLrhz585mBWQKUuYJmEsNYCKiR5F17SDAMr/c5fa4GsAC6moADw/QsGmIiCyC5MliSialBjARkSVgEpCANYCJyNowCUjAGsBEZG2YBCRgDWAisjZMAhLU1wAG0CARsAYwEVkiJgGJWAOYiKyJpMliVGdkoBeGB2g4Y5iILB6TQBOxBjARWQM2BxERKRiTABGRgjEJEBEpGPsEUFejAABrDROR1aj/PnvcGqFMAoCuGA5rDRORtSkrK4NarW70uMFLSVszrVaLmzdvwtXVtdmFcsxdaWkpfHx8kJeXx7Ka/x/vSUO8Jw1Z2j0RRRFlZWXw9vaGjU3jLf98EgBgY2ODDh06mDqMFsXayg3xnjTEe9KQJd2TRz0B1GPHMBGRgjEJEBEpGJOAwjg4OCA2NlZXK5p4Tx6G96Qha70n7BgmIlIwPgkQESkYkwARkYIxCRARKRiTABGRgjEJWKktW7agV69euoktoaGh+Ne//qU7LooiVqxYAW9vb7Rq1QpDhw5FTk6OCSNuWWvWrIEgCFi4cKFun9LuyYoVKyAIgt5Lo9HojivtftS7ceMGpk6dCg8PDzg5OaF3795IT0/XHbe2+8IkYKU6dOiAtWvXIi0tDWlpaRg2bBjGjRun+8f6wQcfYOPGjdi8eTN++eUXaDQaDB8+XLeOkjX75ZdfEB8fj169euntV+I96dGjB/Lz83WvrKws3TEl3o/bt29j0KBBsLOzw7/+9S+cPn0aGzZswBNPPKE7x+rui0iK0bp1a3HHjh2iVqsVNRqNuHbtWt2xyspKUa1Wi1u3bjVhhMZXVlYm+vr6isnJyeKQIUPE119/XRRFUZH3JDY2VgwKCnroMSXeD1EUxbfeekt8+umnGz1ujfeFTwIKUFtbi3379qG8vByhoaHIzc1FQUEBIiMjdec4ODhgyJAhSElJMWGkxvfqq69i9OjRiIiI0Nuv1Hty4cIFeHt7o3PnzpgyZQouX74MQLn347vvvkNISAgmTpyIdu3aITg4GNu3b9cdt8b7wiRgxbKysuDi4gIHBwfMnTsX3377LQICAlBQUAAA8PT01Dvf09NTd8wa7du3D+np6VizZk2DY0q8J0899RR2796NpKQkbN++HQUFBRg4cCCKiooUeT8A4PLly9iyZQt8fX2RlJSEuXPnYsGCBdi9ezcA6/x3wlVErZifnx8yMzNx584dfP3115gxYwaOHj2qO/7gstmiKFrtUtp5eXl4/fXXcejQITg6OjZ6npLuyahRo3Q/9+zZE6GhoejatSt27dqFAQMGAFDW/QDqlpUPCQnBn//8ZwBAcHAwcnJysGXLFkyfPl13njXdFz4JWDF7e3t069YNISEhWLNmDYKCgvDRRx/pRoA8+JdLYWFhg79wrEV6ejoKCwvRt29fqFQqqFQqHD16FB9//DFUKpXu91bSPXmQs7MzevbsiQsXLijy3wgAeHl5ISAgQG/f7373O1y7dg0ArPK+MAkoiCiKqKqqQufOnaHRaJCcnKw7Vl1djaNHj2LgwIEmjNB4wsPDkZWVhczMTN0rJCQEL774IjIzM9GlSxfF3ZMHVVVV4cyZM/Dy8lLkvxEAGDRoEM6dO6e37/z58+jUqRMAWOd9MWm3NBnN0qVLxWPHjom5ubniqVOnxGXLlok2NjbioUOHRFEUxbVr14pqtVr85ptvxKysLPH5558Xvby8xNLSUhNH3nJ+OzpIFJV3T2JiYsQjR46Ily9fFn/++WdxzJgxoqurq3jlyhVRFJV3P0RRFFNTU0WVSiWuXr1avHDhgrh3717RyclJ3LNnj+4ca7svTAJW6uWXXxY7deok2tvbi23bthXDw8N1CUAU64a6xcbGihqNRnRwcBAHDx4sZmVlmTDilvdgElDaPZk8ebLo5eUl2tnZid7e3uL48ePFnJwc3XGl3Y9633//vRgYGCg6ODiI/v7+Ynx8vN5xa7svXEqaiEjB2CdARKRgTAJERArGJEBEpGBMAkRECsYkQESkYEwCREQKxiRARKRgTAJERArGJEAkk5SUFNja2mLkyJGmDoXIYJwxTCSTWbNmwcXFBTt27MDp06fRsWNHU4dE9Fh8EiCSQXl5Of72t79h3rx5GDNmDBITE/WOf/fdd/D19UWrVq0QFhaGXbt2QRAE3LlzR3dOSkoKBg8ejFatWsHHxwcLFixAeXl5y/4ipDhMAkQy+PLLL+Hn5wc/Pz9MnToVO3fuRP1D9pUrV/Dcc8/h2WefRWZmJubMmYN33nlH7/1ZWVkYMWIExo8fj1OnTuHLL7/Ejz/+iNdee80Uvw4pCJuDiGQwaNAgTJo0Ca+//jru378PLy8vfPHFF4iIiMDbb7+Nf/7zn8jKytKd/+6772L16tW4ffs2nnjiCUyfPh2tWrXCtm3bdOf8+OOPGDJkCMrLyx9ZDY2oOfgkQNRM586dQ2pqKqZMmQIAUKlUmDx5Mj777DPd8X79+um9p3///nrb6enpSExMhIuLi+41YsQIaLVa5ObmtswvQorEGsNEzZSQkID79++jffv2un2iKMLOzg63b99+aP3ZBx/AtVot5syZgwULFjS4PjuYyZiYBIia4f79+9i9ezc2bNiAyMhIvWMTJkzA3r174e/vjwMHDugdS0tL09vu06cPcnJy0K1bN6PHTPRb7BMgaob9+/dj8uTJKCwshFqt1jv2zjvv4MCBA/jmm2/g5+eHRYsW4ZVXXkFmZiZiYmJw/fp13LlzB2q1GqdOncKAAQMwc+ZMzJ49G87Ozjhz5gySk5PxySefmOi3IyVgnwBRMyQkJCAiIqJBAgDqngQyMzNx+/ZtfPXVV/jmm2/Qq1cvbNmyRTc6yMHBAQDQq1cvHD16FBcuXMAzzzyD4OBgLF++HF5eXi36+5Dy8EmAyARWr16NrVu3Ii8vz9ShkMKxT4CoBXz66afo168fPDw88NNPP2H9+vWcA0BmgUmAqAVcuHABf/rTn1BcXIyOHTsiJiYGS5cuNXVYRGwOIiJSMnYMExEpGJMAEZGCMQkQESkYkwARkYIxCRARKRiTABGRgjEJEBEpGJMAEZGCMQkQESnY/wMr0RUYV/tXAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# x from 20 to 65\n",
    "x = 20 + 45 * np.random.random((20, 1))\n",
    "\n",
    "# y = a*x + b with noise\n",
    "y = 0.4 * x + 120 + np.random.normal(size=x.shape)\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = plt.axes()\n",
    "ax.scatter(x, y)\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Blood Pressure, SBP [mm Hg]')\n",
    "\n",
    "ax.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0756f",
   "metadata": {},
   "source": [
    "* **Question:** Can we predit the pressure based on a new patient's age?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ea07e",
   "metadata": {},
   "source": [
    "* Want to predict a scalar $\\hat y$ as a function of a scalar $x$<br>\n",
    "* Model: $\\hat y$ is a linear function of $x$:<br>\n",
    "> $\\it\\hat y = wx + b$<br>\n",
    "> $\\hat y$ is the prediction<br>\n",
    "> $w$ is the weight<br>\n",
    "> $b$ is the bias<br>\n",
    "* $w$ and $b$ together are the parameters<br>\n",
    "* Settings of the parameters are called hypotheses<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "befeba55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# predict y from the data\n",
    "x_new = np.linspace(20, 65, 100)\n",
    "y_new = model.predict(x_new[:, np.newaxis])\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(4, 3))\n",
    "ax = plt.axes()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x_new, y_new)\n",
    "\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Blood Pressure, SBP [mm Hg]')\n",
    "\n",
    "ax.axis('tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc4f1c",
   "metadata": {},
   "source": [
    "* How do you know your predited model is accurate? <br>\n",
    "    or how well your predited function models your dataset?"
   ]
  },
  {
   "attachments": {
    "Errors.jpeg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3b994006",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Loss:** Errors of the predition to the real data. Loss helps us to understand how much the predicted value differ from actual value\n",
    "<img src=\"attachment:Errors.jpeg\" width= \"300\"/> </div> <br>\n",
    "\n",
    "* **Loss function:** It is a function to predic the loss. It is mainly applies for a single training set.  \n",
    "     - If your predictions are totally off, your loss function will output a higher number. \n",
    "     - If they‚Äôre pretty good, it‚Äôll output a lower number. <br>\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdd15d",
   "metadata": {},
   "source": [
    "* In linear regression square loss is the most common loss function:\n",
    ">$L_s(y,\\hat y)=(y^{(i)}-\\hat y^{(i)})^{2}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878b731",
   "metadata": {},
   "source": [
    "* **Cost function:** It is a function that measures the performance of a Machine Learning model for given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number.\n",
    "* **Cost function:** loss function averaged over all training examples.<br>\n",
    "* Difference between a lost function and a cost function: A loss function/error function is for a single training example/input. A cost function, on the other hand, is the average loss over the entire training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15010548",
   "metadata": {},
   "source": [
    "* **Mean Square Error (MSE), also called Quadratic loss or L2 Loss:** Mean Square Error (MSE) is the most commonly used regression cost function. MSE is the sum of squared distances between our target variable, $y$, and predicted values, $\\hat y$.\n",
    ">$MSE=\\frac{\\sum_i^N (y^{(i)}-\\hat y^{(i)})^{2}}{N}$  \n",
    "* $\\it y-\\hat y$ is the residual (error), and we want to make this small in magnitude<br>"
   ]
  },
  {
   "attachments": {
    "Picture1.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "01b5f8c6",
   "metadata": {},
   "source": [
    "* How to optimize your linear regression model\n",
    "* Our goal is to find:   $\\underset{w,b}{min}MSE$\n",
    "* Gradient decent:   \n",
    "<img src=\"attachment:Picture1.png\" width= \"300\"/> </div> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc141132",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An example of how to perform Linear Regression with PyTorch\n",
    "* Load the dataset and create tensors\n",
    "We'll create a model that predicts crop yields for apples and oranges (target variables) by looking at the average temperature, rainfall, and humidity (input variables or features) in a region. Here's the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = torch.tensor ([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype=torch.float32)\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = torch.tensor ([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype=torch.float32)\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec3950",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The weights and biases $(w_{11}, w_{12},... w_{23})$ and $(b_1, b_2)$ can also be represented as matrices, initialized as random values. The first row of $w$ and the first element of $b$ are used to predict the first target variable, i.e., yield of apples, and similarly, the second for oranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b888e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "attachments": {
    "WGXLFvA.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ac89a2c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Our model is simply a function that performs a matrix multiplication of the inputs and the weights $w$ (transposed) and adds the bias $b$ (replicated for each observation).\n",
    "\n",
    "<img src=\"attachment:WGXLFvA.png\" width=\"350\"/> </div><br>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5860e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can define the model as follows:\n",
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "#@ represents matrix multiplication in PyTorch, and the .t method returns the transpose of a tensor.\n",
    "# The matrix obtained by passing the input data into the model is a set of predictions for the target variables.\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)\n",
    "\n",
    "# Compare with targets\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb75297",
   "metadata": {},
   "source": [
    "* You can see that there's a huge difference between the predictions of our model, and the actual values of the target variables. Obviously, this is because we've initialized our model with random weights and biases, and we can't expect it to just work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ed151",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Loss function:** Before we improve our model, we need a way to evaluate how well our model is performing. We can compare the model's predictions with the actual targets using the following method: <br>\n",
    "\n",
    "    - Calculate the difference between the two matrices (preds and targets).\n",
    "    - Square all elements of the difference matrix to remove negative values.\n",
    "    - Calculate the average of the elements in the resulting matrix.\n",
    "    - The result is a single number, known as the mean squared error (SE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af1f86",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# MSE Cost function\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "#torch.sum returns the sum of all the elements in a tensor. The .numel method of a tensor returns the number of elements in a tensor. Let's compute the mean squared error for the current predictions of our model.\n",
    "\n",
    "\n",
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7661c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Compute gradients:** As in previous lectures, with PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases because they have requires_grad set to True. We'll see how this is useful in just a moment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "#The gradients are stored in the .grad property of the respective tensors. Note that the derivative of the loss w.r.t. the weights matrix is itself a matrix with the same dimensions.\n",
    "\n",
    "# Gradients for weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52f74c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Adjust weights and biases to reduce the loss:**\n",
    "The loss is a quadratic function of our weights and biases, and our objective is to find the set of weights where the loss is the lowest. If we plot a graph of the loss w.r.t any individual weight or bias element, it will look like the figure shown below. An important insight from calculus is that the gradient indicates the rate of change of the loss, i.e., the loss function's slope w.r.t. the weights and biases.\n",
    "\n",
    "If a gradient element is positive:\n",
    "\n",
    "* increasing the weight element's value slightly will increase the loss\n",
    "* decreasing the weight element's value slightly will decrease the loss\n",
    "\n",
    "If a gradient element is negative:\n",
    "\n",
    "* increasing the weight element's value slightly will decrease the loss\n",
    "* decreasing the weight element's value slightly will increase the loss  \n",
    "\n",
    "The increase or decrease in the loss by changing a weight element is proportional to the gradient of the loss w.r.t. that element. This observation forms the basis of the gradient descent optimization algorithm that we'll use to improve our model (by descending along the gradient).\n",
    "\n",
    "We can subtract from each weight element a small quantity proportional to the derivative of the loss w.r.t. that element to reduce the loss slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c556a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)\n",
    "\n",
    "# Calculate the loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)\n",
    "\n",
    "# Adjust weights & reset gradients\n",
    "# We multiply the gradients with a very small number (10^-5 in this case) to ensure that we don't modify the weights by a very large amount. We want to take a small step in the downhill direction of the gradient, not a giant leap. This number is called the learning rate of the algorithm.\n",
    "# We use torch.no_grad to indicate to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases.    \n",
    "# Before we proceed, we reset the gradients to zero by invoking the .zero_() method. We need to do this because PyTorch accumulates gradients. Otherwise, the next time we invoke .backward on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results.   \n",
    "    \n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "print(w)\n",
    "print(b)\n",
    "\n",
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148804d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Train the model using gradient descent**\n",
    "As seen above, we reduce the loss and improve our model using the gradient descent optimization algorithm. Thus, we can train the model using the following steps:\n",
    "\n",
    "    1. Generate predictions\n",
    "    2. Calculate the loss\n",
    "    3. Compute gradients w.r.t the weights and biases\n",
    "    4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "    5. Reset the gradients to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf30fd7",
   "metadata": {},
   "source": [
    "* Linear regression using PyTorch built-ins\n",
    "We've implemented linear regression & gradient descent model using some basic tensor operations. However, since this is a common pattern in deep learning, PyTorch provides several built-in functions and classes to make it easy to create and train models with just a few lines of code.\n",
    "\n",
    "* Let's begin by importing the **torch.nn** package from PyTorch, which contains utility classes for building neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f5dc165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs= tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "targets= tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "w= tensor([[ 1.6875,  0.3959,  0.4505],\n",
      "        [ 0.3447,  0.1497, -0.3955]], requires_grad=True)\n",
      "b= tensor([-1.1702, -1.0188], requires_grad=True)\n",
      "i= 0\n",
      "w.grad= tensor([[10788.8262,  9854.7930,  6416.0322],\n",
      "        [-5876.0596, -7229.0967, -4369.9873]])\n",
      "b.grad= tensor([123.4566, -72.6320])\n",
      "new w tensor([[ 1.1481, -0.0968,  0.1297],\n",
      "        [ 0.6385,  0.5111, -0.1770]], requires_grad=True)\n",
      "new b tensor([-1.1764, -1.0151], requires_grad=True)\n",
      "Loss= tensor(11522.6562, grad_fn=<DivBackward0>)\n",
      "i= 1\n",
      "w.grad= tensor([[ 1911.3040,   373.4197,   552.6459],\n",
      "        [ -180.8003, -1081.7141,  -582.2112]])\n",
      "b.grad= tensor([18.2915, -5.0045])\n",
      "new w tensor([[ 1.0525, -0.1155,  0.1021],\n",
      "        [ 0.6475,  0.5652, -0.1479]], requires_grad=True)\n",
      "new b tensor([-1.1773, -1.0149], requires_grad=True)\n",
      "Loss= tensor(1526.1077, grad_fn=<DivBackward0>)\n",
      "i= 2\n",
      "w.grad= tensor([[ 958.7985, -596.4315,  -57.5446],\n",
      "        [ 401.3197, -427.9362, -184.5579]])\n",
      "b.grad= tensor([7.1234, 1.9721])\n",
      "new w tensor([[ 1.0046, -0.0857,  0.1050],\n",
      "        [ 0.6275,  0.5866, -0.1387]], requires_grad=True)\n",
      "new b tensor([-1.1776, -1.0150], requires_grad=True)\n",
      "Loss= tensor(1333.2070, grad_fn=<DivBackward0>)\n",
      "i= 3\n",
      "w.grad= tensor([[ 835.4476, -677.2727, -118.7147],\n",
      "        [ 449.3941, -348.7471, -141.3079]])\n",
      "b.grad= tensor([5.7862, 2.6120])\n",
      "new w tensor([[ 0.9628, -0.0518,  0.1109],\n",
      "        [ 0.6050,  0.6040, -0.1316]], requires_grad=True)\n",
      "new b tensor([-1.1779, -1.0151], requires_grad=True)\n",
      "Loss= tensor(1252.6031, grad_fn=<DivBackward0>)\n",
      "i= 4\n",
      "w.grad= tensor([[ 799.5189, -665.8398, -122.5747],\n",
      "        [ 442.0480, -329.9664, -135.1459]])\n",
      "b.grad= tensor([5.4819, 2.5924])\n",
      "new w tensor([[ 0.9228, -0.0185,  0.1171],\n",
      "        [ 0.5829,  0.6205, -0.1249]], requires_grad=True)\n",
      "new b tensor([-1.1782, -1.0152], requires_grad=True)\n",
      "Loss= tensor(1178.0862, grad_fn=<DivBackward0>)\n",
      "i= 5\n",
      "w.grad= tensor([[ 773.4557, -645.3942, -120.5201],\n",
      "        [ 429.2990, -317.8354, -132.9015]])\n",
      "b.grad= tensor([5.2909, 2.5067])\n",
      "new w tensor([[ 0.8841,  0.0138,  0.1231],\n",
      "        [ 0.5614,  0.6364, -0.1182]], requires_grad=True)\n",
      "new b tensor([-1.1785, -1.0154], requires_grad=True)\n",
      "Loss= tensor(1108.1932, grad_fn=<DivBackward0>)\n",
      "i= 6\n",
      "w.grad= tensor([[ 749.1295, -624.6174, -117.9202],\n",
      "        [ 416.3616, -306.7267, -131.1075]])\n",
      "b.grad= tensor([5.1167, 2.4167])\n",
      "new w tensor([[ 0.8467,  0.0450,  0.1290],\n",
      "        [ 0.5406,  0.6518, -0.1117]], requires_grad=True)\n",
      "new b tensor([-1.1787, -1.0155], requires_grad=True)\n",
      "Loss= tensor(1042.6245, grad_fn=<DivBackward0>)\n",
      "i= 7\n",
      "w.grad= tensor([[ 725.6669, -604.3991, -115.3343],\n",
      "        [ 403.7700, -296.0397, -129.3993]])\n",
      "b.grad= tensor([4.9493, 2.3290])\n",
      "new w tensor([[ 0.8104,  0.0752,  0.1347],\n",
      "        [ 0.5204,  0.6666, -0.1052]], requires_grad=True)\n",
      "new b tensor([-1.1790, -1.0156], requires_grad=True)\n",
      "Loss= tensor(981.1104, grad_fn=<DivBackward0>)\n",
      "i= 8\n",
      "w.grad= tensor([[ 702.9569, -584.8116, -112.8180],\n",
      "        [ 391.5667, -285.7042, -127.7389]])\n",
      "b.grad= tensor([4.7872, 2.2439])\n",
      "new w tensor([[ 0.7753,  0.1044,  0.1404],\n",
      "        [ 0.5009,  0.6808, -0.0988]], requires_grad=True)\n",
      "new b tensor([-1.1792, -1.0157], requires_grad=True)\n",
      "Loss= tensor(923.3989, grad_fn=<DivBackward0>)\n",
      "i= 9\n",
      "w.grad= tensor([[ 680.9653, -565.8464, -110.3759],\n",
      "        [ 379.7489, -275.6985, -126.1186]])\n",
      "b.grad= tensor([4.6304, 2.1617])\n",
      "new w tensor([[ 0.7412,  0.1327,  0.1459],\n",
      "        [ 0.4819,  0.6946, -0.0925]], requires_grad=True)\n",
      "new b tensor([-1.1794, -1.0158], requires_grad=True)\n",
      "Loss= tensor(869.2531, grad_fn=<DivBackward0>)\n",
      "i= 10\n",
      "w.grad= tensor([[ 659.6702, -547.4823, -108.0049],\n",
      "        [ 368.3010, -266.0161, -124.5394]])\n",
      "b.grad= tensor([4.4785, 2.0821])\n",
      "new w tensor([[ 0.7082,  0.1601,  0.1513],\n",
      "        [ 0.4635,  0.7079, -0.0863]], requires_grad=True)\n",
      "new b tensor([-1.1797, -1.0159], requires_grad=True)\n",
      "Loss= tensor(818.4510, grad_fn=<DivBackward0>)\n",
      "i= 11\n",
      "w.grad= tensor([[ 639.0470, -529.7029, -105.7044],\n",
      "        [ 357.2126, -256.6451, -122.9991]])\n",
      "b.grad= tensor([4.3315, 2.0050])\n",
      "new w tensor([[ 0.6763,  0.1866,  0.1566],\n",
      "        [ 0.4456,  0.7208, -0.0801]], requires_grad=True)\n",
      "new b tensor([-1.1799, -1.0160], requires_grad=True)\n",
      "Loss= tensor(770.7843, grad_fn=<DivBackward0>)\n",
      "i= 12\n",
      "w.grad= tensor([[ 619.0758, -512.4887, -103.4716],\n",
      "        [ 346.4734, -247.5744, -121.4959]])\n",
      "b.grad= tensor([4.1891, 1.9305])\n",
      "new w tensor([[ 0.6453,  0.2122,  0.1618],\n",
      "        [ 0.4283,  0.7331, -0.0741]], requires_grad=True)\n",
      "new b tensor([-1.1801, -1.0161], requires_grad=True)\n",
      "Loss= tensor(726.0580, grad_fn=<DivBackward0>)\n",
      "i= 13\n",
      "w.grad= tensor([[ 599.7365, -495.8209, -101.3038],\n",
      "        [ 336.0712, -238.7958, -120.0295]])\n",
      "b.grad= tensor([4.0513, 1.8584])\n",
      "new w tensor([[ 0.6153,  0.2370,  0.1668],\n",
      "        [ 0.4115,  0.7451, -0.0681]], requires_grad=True)\n",
      "new b tensor([-1.1803, -1.0162], requires_grad=True)\n",
      "Loss= tensor(684.0892, grad_fn=<DivBackward0>)\n",
      "i= 14\n",
      "w.grad= tensor([[ 581.0075, -479.6840,  -99.2001],\n",
      "        [ 325.9964, -230.2987, -118.5979]])\n",
      "b.grad= tensor([3.9179, 1.7886])\n",
      "new w tensor([[ 0.5863,  0.2610,  0.1718],\n",
      "        [ 0.3952,  0.7566, -0.0621]], requires_grad=True)\n",
      "new b tensor([-1.1805, -1.0163], requires_grad=True)\n",
      "Loss= tensor(644.7061, grad_fn=<DivBackward0>)\n",
      "i= 15\n",
      "w.grad= tensor([[ 562.8713, -464.0594,  -97.1573],\n",
      "        [ 316.2374, -222.0760, -117.2010]])\n",
      "b.grad= tensor([3.7887, 1.7210])\n",
      "new w tensor([[ 0.5581,  0.2842,  0.1766],\n",
      "        [ 0.3794,  0.7677, -0.0563]], requires_grad=True)\n",
      "new b tensor([-1.1807, -1.0164], requires_grad=True)\n",
      "Loss= tensor(607.7479, grad_fn=<DivBackward0>)\n",
      "i= 16\n",
      "w.grad= tensor([[ 545.3074, -448.9320,  -95.1745],\n",
      "        [ 306.7853, -214.1178, -115.8370]])\n",
      "b.grad= tensor([3.6637, 1.6557])\n",
      "new w tensor([[ 0.5309,  0.3067,  0.1814],\n",
      "        [ 0.3640,  0.7784, -0.0505]], requires_grad=True)\n",
      "new b tensor([-1.1808, -1.0165], requires_grad=True)\n",
      "Loss= tensor(573.0637, grad_fn=<DivBackward0>)\n",
      "i= 17\n",
      "w.grad= tensor([[ 528.2991, -434.2855,  -93.2491],\n",
      "        [ 297.6303, -206.4149, -114.5050]])\n",
      "b.grad= tensor([3.5426, 1.5925])\n",
      "new w tensor([[ 0.5045,  0.3284,  0.1861],\n",
      "        [ 0.3491,  0.7887, -0.0447]], requires_grad=True)\n",
      "new b tensor([-1.1810, -1.0165], requires_grad=True)\n",
      "Loss= tensor(540.5120, grad_fn=<DivBackward0>)\n",
      "i= 18\n",
      "w.grad= tensor([[ 511.8278, -420.1053,  -91.3799],\n",
      "        [ 288.7614, -198.9619, -113.2051]])\n",
      "b.grad= tensor([3.4254, 1.5313])\n",
      "new w tensor([[ 0.4789,  0.3494,  0.1906],\n",
      "        [ 0.3347,  0.7987, -0.0391]], requires_grad=True)\n",
      "new b tensor([-1.1812, -1.0166], requires_grad=True)\n",
      "Loss= tensor(509.9603, grad_fn=<DivBackward0>)\n",
      "i= 19\n",
      "w.grad= tensor([[ 495.8775, -406.3753,  -89.5643],\n",
      "        [ 280.1711, -191.7489, -111.9353]])\n",
      "b.grad= tensor([3.3119, 1.4722])\n",
      "new w tensor([[ 0.4541,  0.3697,  0.1951],\n",
      "        [ 0.3207,  0.8083, -0.0335]], requires_grad=True)\n",
      "new b tensor([-1.1814, -1.0167], requires_grad=True)\n",
      "Loss= tensor(481.2839, grad_fn=<DivBackward0>)\n",
      "i= 20\n",
      "w.grad= tensor([[ 480.4303, -393.0833,  -87.8019],\n",
      "        [ 271.8509, -184.7678, -110.6944]])\n",
      "b.grad= tensor([3.2020, 1.4149])\n",
      "new w tensor([[ 0.4301,  0.3893,  0.1995],\n",
      "        [ 0.3071,  0.8175, -0.0280]], requires_grad=True)\n",
      "new b tensor([-1.1815, -1.0168], requires_grad=True)\n",
      "Loss= tensor(454.3665, grad_fn=<DivBackward0>)\n",
      "i= 21\n",
      "w.grad= tensor([[ 465.4716, -380.2134,  -86.0899],\n",
      "        [ 263.7910, -178.0128, -109.4825]])\n",
      "b.grad= tensor([3.0957, 1.3595])\n",
      "new w tensor([[ 0.4068,  0.4084,  0.2038],\n",
      "        [ 0.2939,  0.8264, -0.0225]], requires_grad=True)\n",
      "new b tensor([-1.1817, -1.0168], requires_grad=True)\n",
      "Loss= tensor(429.0985, grad_fn=<DivBackward0>)\n",
      "i= 22\n",
      "w.grad= tensor([[ 450.9859, -367.7527,  -84.4268],\n",
      "        [ 255.9841, -171.4755, -108.2981]])\n",
      "b.grad= tensor([2.9927, 1.3059])\n",
      "new w tensor([[ 0.3842,  0.4267,  0.2080],\n",
      "        [ 0.2811,  0.8350, -0.0171]], requires_grad=True)\n",
      "new b tensor([-1.1818, -1.0169], requires_grad=True)\n",
      "Loss= tensor(405.3774, grad_fn=<DivBackward0>)\n",
      "i= 23\n",
      "w.grad= tensor([[ 436.9567, -355.6896,  -82.8121],\n",
      "        [ 248.4216, -165.1497, -107.1408]])\n",
      "b.grad= tensor([2.8930, 1.2541])\n",
      "new w tensor([[ 0.3624,  0.4445,  0.2122],\n",
      "        [ 0.2687,  0.8432, -0.0117]], requires_grad=True)\n",
      "new b tensor([-1.1820, -1.0170], requires_grad=True)\n",
      "Loss= tensor(383.1072, grad_fn=<DivBackward0>)\n",
      "i= 24\n",
      "w.grad= tensor([[ 423.3716, -344.0095,  -81.2430],\n",
      "        [ 241.0970, -159.0273, -106.0088]])\n",
      "b.grad= tensor([2.7965, 1.2040])\n",
      "new w tensor([[ 0.3412,  0.4617,  0.2162],\n",
      "        [ 0.2566,  0.8512, -0.0064]], requires_grad=True)\n",
      "new b tensor([-1.1821, -1.0170], requires_grad=True)\n",
      "Loss= tensor(362.1974, grad_fn=<DivBackward0>)\n",
      "i= 25\n",
      "w.grad= tensor([[ 410.2146, -332.7022,  -79.7192],\n",
      "        [ 234.0005, -153.1045, -104.9030]])\n",
      "b.grad= tensor([2.7031, 1.1555])\n",
      "new w tensor([[ 0.3207,  0.4784,  0.2202],\n",
      "        [ 0.2449,  0.8588, -0.0012]], requires_grad=True)\n",
      "new b tensor([-1.1822, -1.0171], requires_grad=True)\n",
      "Loss= tensor(342.5637, grad_fn=<DivBackward0>)\n",
      "i= 26\n",
      "w.grad= tensor([[ 397.4740, -321.7540,  -78.2383],\n",
      "        [ 227.1268, -147.3730, -103.8214]])\n",
      "b.grad= tensor([2.6126, 1.1085])\n",
      "new w tensor([[0.3008, 0.4944, 0.2241],\n",
      "        [0.2336, 0.8662, 0.0040]], requires_grad=True)\n",
      "new b tensor([-1.1824, -1.0171], requires_grad=True)\n",
      "Loss= tensor(324.1267, grad_fn=<DivBackward0>)\n",
      "i= 27\n",
      "w.grad= tensor([[ 385.1352, -311.1548,  -76.7996],\n",
      "        [ 220.4684, -141.8268, -102.7635]])\n",
      "b.grad= tensor([2.5251, 1.0632])\n",
      "new w tensor([[0.2816, 0.5100, 0.2280],\n",
      "        [0.2226, 0.8733, 0.0092]], requires_grad=True)\n",
      "new b tensor([-1.1825, -1.0172], requires_grad=True)\n",
      "Loss= tensor(306.8122, grad_fn=<DivBackward0>)\n",
      "i= 28\n",
      "w.grad= tensor([[ 373.1864, -300.8931,  -75.4015],\n",
      "        [ 214.0183, -136.4603, -101.7287]])\n",
      "b.grad= tensor([2.4403, 1.0193])\n",
      "new w tensor([[0.2629, 0.5251, 0.2317],\n",
      "        [0.2119, 0.8801, 0.0143]], requires_grad=True)\n",
      "new b tensor([-1.1826, -1.0172], requires_grad=True)\n",
      "Loss= tensor(290.5504, grad_fn=<DivBackward0>)\n",
      "i= 29\n",
      "w.grad= tensor([[ 361.6139, -290.9592,  -74.0433],\n",
      "        [ 207.7701, -131.2680, -100.7162]])\n",
      "b.grad= tensor([2.3583, 0.9768])\n",
      "new w tensor([[0.2448, 0.5396, 0.2354],\n",
      "        [0.2015, 0.8867, 0.0193]], requires_grad=True)\n",
      "new b tensor([-1.1827, -1.0173], requires_grad=True)\n",
      "Loss= tensor(275.2758, grad_fn=<DivBackward0>)\n",
      "i= 30\n",
      "w.grad= tensor([[ 350.4077, -281.3407,  -72.7228],\n",
      "        [ 201.7168, -126.2447,  -99.7259]])\n",
      "b.grad= tensor([2.2788, 0.9357])\n",
      "new w tensor([[0.2273, 0.5537, 0.2391],\n",
      "        [0.1914, 0.8930, 0.0243]], requires_grad=True)\n",
      "new b tensor([-1.1829, -1.0173], requires_grad=True)\n",
      "Loss= tensor(260.9272, grad_fn=<DivBackward0>)\n",
      "i= 31\n",
      "w.grad= tensor([[ 339.5552, -272.0285,  -71.4393],\n",
      "        [ 195.8536, -121.3834,  -98.7560]])\n",
      "b.grad= tensor([2.2019, 0.8960])\n",
      "new w tensor([[0.2103, 0.5673, 0.2427],\n",
      "        [0.1816, 0.8991, 0.0292]], requires_grad=True)\n",
      "new b tensor([-1.1830, -1.0174], requires_grad=True)\n",
      "Loss= tensor(247.4471, grad_fn=<DivBackward0>)\n",
      "i= 32\n",
      "w.grad= tensor([[ 329.0451, -263.0135,  -70.1919],\n",
      "        [ 190.1730, -116.6808,  -97.8071]])\n",
      "b.grad= tensor([2.1275, 0.8576])\n",
      "new w tensor([[0.1939, 0.5804, 0.2462],\n",
      "        [0.1721, 0.9049, 0.0341]], requires_grad=True)\n",
      "new b tensor([-1.1831, -1.0174], requires_grad=True)\n",
      "Loss= tensor(234.7817, grad_fn=<DivBackward0>)\n",
      "i= 33\n",
      "w.grad= tensor([[ 318.8666, -254.2860,  -68.9793],\n",
      "        [ 184.6699, -112.1311,  -96.8781]])\n",
      "b.grad= tensor([2.0555, 0.8205])\n",
      "new w tensor([[0.1779, 0.5931, 0.2496],\n",
      "        [0.1628, 0.9105, 0.0389]], requires_grad=True)\n",
      "new b tensor([-1.1832, -1.0175], requires_grad=True)\n",
      "Loss= tensor(222.8804, grad_fn=<DivBackward0>)\n",
      "i= 34\n",
      "w.grad= tensor([[ 309.0097, -245.8362,  -67.8000],\n",
      "        [ 179.3385, -107.7298,  -95.9684]])\n",
      "b.grad= tensor([1.9857, 0.7846])\n",
      "new w tensor([[0.1625, 0.6054, 0.2530],\n",
      "        [0.1539, 0.9159, 0.0437]], requires_grad=True)\n",
      "new b tensor([-1.1833, -1.0175], requires_grad=True)\n",
      "Loss= tensor(211.6958, grad_fn=<DivBackward0>)\n",
      "i= 35\n",
      "w.grad= tensor([[ 299.4626, -237.6574,  -66.6543],\n",
      "        [ 174.1738, -103.4714,  -95.0772]])\n",
      "b.grad= tensor([1.9182, 0.7498])\n",
      "new w tensor([[0.1475, 0.6173, 0.2563],\n",
      "        [0.1452, 0.9211, 0.0485]], requires_grad=True)\n",
      "new b tensor([-1.1834, -1.0175], requires_grad=True)\n",
      "Loss= tensor(201.1834, grad_fn=<DivBackward0>)\n",
      "i= 36\n",
      "w.grad= tensor([[ 290.2177, -229.7383,  -65.5397],\n",
      "        [ 169.1705,  -99.3516,  -94.2042]])\n",
      "b.grad= tensor([1.8528, 0.7163])\n",
      "new w tensor([[0.1330, 0.6288, 0.2596],\n",
      "        [0.1367, 0.9260, 0.0532]], requires_grad=True)\n",
      "new b tensor([-1.1835, -1.0176], requires_grad=True)\n",
      "Loss= tensor(191.3017, grad_fn=<DivBackward0>)\n",
      "i= 37\n",
      "w.grad= tensor([[ 281.2644, -222.0719,  -64.4556],\n",
      "        [ 164.3228,  -95.3668,  -93.3491]])\n",
      "b.grad= tensor([1.7896, 0.6838])\n",
      "new w tensor([[0.1189, 0.6399, 0.2628],\n",
      "        [0.1285, 0.9308, 0.0579]], requires_grad=True)\n",
      "new b tensor([-1.1836, -1.0176], requires_grad=True)\n",
      "Loss= tensor(182.0114, grad_fn=<DivBackward0>)\n",
      "i= 38\n",
      "w.grad= tensor([[ 272.5936, -214.6500,  -63.4012],\n",
      "        [ 159.6265,  -91.5117,  -92.5111]])\n",
      "b.grad= tensor([1.7283, 0.6524])\n",
      "new w tensor([[0.1053, 0.6506, 0.2660],\n",
      "        [0.1205, 0.9354, 0.0625]], requires_grad=True)\n",
      "new b tensor([-1.1836, -1.0176], requires_grad=True)\n",
      "Loss= tensor(173.2760, grad_fn=<DivBackward0>)\n",
      "i= 39\n",
      "w.grad= tensor([[ 264.1964, -207.4648,  -62.3755],\n",
      "        [ 155.0768,  -87.7822,  -91.6893]])\n",
      "b.grad= tensor([1.6690, 0.6220])\n",
      "new w tensor([[0.0921, 0.6610, 0.2691],\n",
      "        [0.1128, 0.9398, 0.0671]], requires_grad=True)\n",
      "new b tensor([-1.1837, -1.0177], requires_grad=True)\n",
      "Loss= tensor(165.0610, grad_fn=<DivBackward0>)\n",
      "i= 40\n",
      "w.grad= tensor([[ 256.0638, -200.5093,  -61.3779],\n",
      "        [ 150.6685,  -84.1753,  -90.8842]])\n",
      "b.grad= tensor([1.6117, 0.5927])\n",
      "new w tensor([[0.0793, 0.6710, 0.2722],\n",
      "        [0.1052, 0.9440, 0.0716]], requires_grad=True)\n",
      "new b tensor([-1.1838, -1.0177], requires_grad=True)\n",
      "Loss= tensor(157.3344, grad_fn=<DivBackward0>)\n",
      "i= 41\n",
      "w.grad= tensor([[ 248.1873, -193.7764,  -60.4076],\n",
      "        [ 146.3970,  -80.6870,  -90.0950]])\n",
      "b.grad= tensor([1.5561, 0.5643])\n",
      "new w tensor([[0.0669, 0.6807, 0.2752],\n",
      "        [0.0979, 0.9480, 0.0761]], requires_grad=True)\n",
      "new b tensor([-1.1839, -1.0177], requires_grad=True)\n",
      "Loss= tensor(150.0658, grad_fn=<DivBackward0>)\n",
      "i= 42\n",
      "w.grad= tensor([[ 240.5596, -187.2577,  -59.4630],\n",
      "        [ 142.2589,  -77.3124,  -89.3207]])\n",
      "b.grad= tensor([1.5023, 0.5369])\n",
      "new w tensor([[0.0549, 0.6901, 0.2782],\n",
      "        [0.0908, 0.9519, 0.0806]], requires_grad=True)\n",
      "new b tensor([-1.1840, -1.0178], requires_grad=True)\n",
      "Loss= tensor(143.2270, grad_fn=<DivBackward0>)\n",
      "i= 43\n",
      "w.grad= tensor([[ 233.1727, -180.9471,  -58.5437],\n",
      "        [ 138.2497,  -74.0484,  -88.5612]])\n",
      "b.grad= tensor([1.4503, 0.5104])\n",
      "new w tensor([[0.0432, 0.6991, 0.2811],\n",
      "        [0.0839, 0.9556, 0.0850]], requires_grad=True)\n",
      "new b tensor([-1.1840, -1.0178], requires_grad=True)\n",
      "Loss= tensor(136.7913, grad_fn=<DivBackward0>)\n",
      "i= 44\n",
      "w.grad= tensor([[ 226.0182, -174.8385,  -57.6493],\n",
      "        [ 134.3654,  -70.8910,  -87.8158]])\n",
      "b.grad= tensor([1.3999, 0.4847])\n",
      "new w tensor([[0.0319, 0.7079, 0.2840],\n",
      "        [0.0772, 0.9591, 0.0894]], requires_grad=True)\n",
      "new b tensor([-1.1841, -1.0178], requires_grad=True)\n",
      "Loss= tensor(130.7339, grad_fn=<DivBackward0>)\n",
      "i= 45\n",
      "w.grad= tensor([[ 219.0892, -168.9251,  -56.7788],\n",
      "        [ 130.6014,  -67.8380,  -87.0847]])\n",
      "b.grad= tensor([1.3512, 0.4600])\n",
      "new w tensor([[0.0209, 0.7163, 0.2868],\n",
      "        [0.0706, 0.9625, 0.0938]], requires_grad=True)\n",
      "new b tensor([-1.1842, -1.0178], requires_grad=True)\n",
      "Loss= tensor(125.0313, grad_fn=<DivBackward0>)\n",
      "i= 46\n",
      "w.grad= tensor([[ 212.3787, -163.2007,  -55.9313],\n",
      "        [ 126.9543,  -64.8856,  -86.3673]])\n",
      "b.grad= tensor([1.3040, 0.4360])\n",
      "new w tensor([[0.0103, 0.7245, 0.2896],\n",
      "        [0.0643, 0.9658, 0.0981]], requires_grad=True)\n",
      "new b tensor([-1.1842, -1.0179], requires_grad=True)\n",
      "Loss= tensor(119.6617, grad_fn=<DivBackward0>)\n",
      "i= 47\n",
      "w.grad= tensor([[ 205.8799, -157.6591,  -55.1061],\n",
      "        [ 123.4200,  -62.0308,  -85.6633]])\n",
      "b.grad= tensor([1.2583, 0.4129])\n",
      "new w tensor([[3.2103e-05, 7.3236e-01, 2.9239e-01],\n",
      "        [5.8111e-02, 9.6886e-01, 1.0238e-01]], requires_grad=True)\n",
      "new b tensor([-1.1843, -1.0179], requires_grad=True)\n",
      "Loss= tensor(114.6046, grad_fn=<DivBackward0>)\n",
      "i= 48\n",
      "w.grad= tensor([[ 199.5854, -152.2951,  -54.3028],\n",
      "        [ 119.9960,  -59.2694,  -84.9716]])\n",
      "b.grad= tensor([1.2141, 0.3905])\n",
      "new w tensor([[-0.0099,  0.7400,  0.2951],\n",
      "        [ 0.0521,  0.9718,  0.1066]], requires_grad=True)\n",
      "new b tensor([-1.1844, -1.0179], requires_grad=True)\n",
      "Loss= tensor(109.8406, grad_fn=<DivBackward0>)\n",
      "i= 49\n",
      "w.grad= tensor([[ 193.4890, -147.1031,  -53.5208],\n",
      "        [ 116.6782,  -56.5992,  -84.2925]])\n",
      "b.grad= tensor([1.1713, 0.3689])\n",
      "new w tensor([[-0.0196,  0.7473,  0.2978],\n",
      "        [ 0.0463,  0.9747,  0.1108]], requires_grad=True)\n",
      "new b tensor([-1.1844, -1.0179], requires_grad=True)\n",
      "Loss= tensor(105.3516, grad_fn=<DivBackward0>)\n",
      "i= 50\n",
      "w.grad= tensor([[ 187.5854, -142.0761,  -52.7585],\n",
      "        [ 113.4624,  -54.0182,  -83.6260]])\n",
      "b.grad= tensor([1.1299, 0.3480])\n",
      "new w tensor([[-0.0290,  0.7544,  0.3004],\n",
      "        [ 0.0406,  0.9774,  0.1150]], requires_grad=True)\n",
      "new b tensor([-1.1845, -1.0179], requires_grad=True)\n",
      "Loss= tensor(101.1208, grad_fn=<DivBackward0>)\n",
      "i= 51\n",
      "w.grad= tensor([[ 181.8667, -137.2114,  -52.0168],\n",
      "        [ 110.3476,  -51.5210,  -82.9703]])\n",
      "b.grad= tensor([1.0899, 0.3279])\n",
      "new w tensor([[-0.0381,  0.7613,  0.3030],\n",
      "        [ 0.0351,  0.9799,  0.1192]], requires_grad=True)\n",
      "new b tensor([-1.1845, -1.0179], requires_grad=True)\n",
      "Loss= tensor(97.1323, grad_fn=<DivBackward0>)\n",
      "i= 52\n",
      "w.grad= tensor([[ 176.3291, -132.5012,  -51.2935],\n",
      "        [ 107.3279,  -49.1081,  -82.3270]])\n",
      "b.grad= tensor([1.0511, 0.3084])\n",
      "new w tensor([[-0.0469,  0.7679,  0.3056],\n",
      "        [ 0.0297,  0.9824,  0.1233]], requires_grad=True)\n",
      "new b tensor([-1.1846, -1.0180], requires_grad=True)\n",
      "Loss= tensor(93.3711, grad_fn=<DivBackward0>)\n",
      "i= 53\n",
      "w.grad= tensor([[ 170.9653, -127.9427,  -50.5893],\n",
      "        [ 104.4024,  -46.7744,  -81.6943]])\n",
      "b.grad= tensor([1.0135, 0.2895])\n",
      "new w tensor([[-0.0555,  0.7743,  0.3081],\n",
      "        [ 0.0245,  0.9847,  0.1274]], requires_grad=True)\n",
      "new b tensor([-1.1846, -1.0180], requires_grad=True)\n",
      "Loss= tensor(89.8233, grad_fn=<DivBackward0>)\n",
      "i= 54\n",
      "w.grad= tensor([[ 165.7705, -123.5298,  -49.9028],\n",
      "        [ 101.5663,  -44.5194,  -81.0730]])\n",
      "b.grad= tensor([0.9772, 0.2714])\n",
      "new w tensor([[-0.0637,  0.7805,  0.3106],\n",
      "        [ 0.0194,  0.9870,  0.1314]], requires_grad=True)\n",
      "new b tensor([-1.1847, -1.0180], requires_grad=True)\n",
      "Loss= tensor(86.4758, grad_fn=<DivBackward0>)\n",
      "i= 55\n",
      "w.grad= tensor([[ 160.7391, -119.2585,  -49.2339],\n",
      "        [  98.8200,  -42.3368,  -80.4608]])\n",
      "b.grad= tensor([0.9421, 0.2538])\n",
      "new w tensor([[-0.0718,  0.7865,  0.3131],\n",
      "        [ 0.0145,  0.9891,  0.1354]], requires_grad=True)\n",
      "new b tensor([-1.1847, -1.0180], requires_grad=True)\n",
      "Loss= tensor(83.3163, grad_fn=<DivBackward0>)\n",
      "i= 56\n",
      "w.grad= tensor([[ 155.8659, -115.1242,  -48.5820],\n",
      "        [  96.1566,  -40.2291,  -79.8601]])\n",
      "b.grad= tensor([0.9080, 0.2368])\n",
      "new w tensor([[-0.0796,  0.7922,  0.3155],\n",
      "        [ 0.0097,  0.9911,  0.1394]], requires_grad=True)\n",
      "new b tensor([-1.1848, -1.0180], requires_grad=True)\n",
      "Loss= tensor(80.3332, grad_fn=<DivBackward0>)\n",
      "i= 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([[ 151.1458, -111.1229,  -47.9466],\n",
      "        [  93.5754,  -38.1917,  -79.2694]])\n",
      "b.grad= tensor([0.8751, 0.2204])\n",
      "new w tensor([[-0.0871,  0.7978,  0.3179],\n",
      "        [ 0.0050,  0.9930,  0.1434]], requires_grad=True)\n",
      "new b tensor([-1.1848, -1.0180], requires_grad=True)\n",
      "Loss= tensor(77.5157, grad_fn=<DivBackward0>)\n",
      "i= 58\n",
      "w.grad= tensor([[ 146.5755, -107.2485,  -47.3264],\n",
      "        [  91.0748,  -36.2212,  -78.6875]])\n",
      "b.grad= tensor([0.8433, 0.2046])\n",
      "new w tensor([[-9.4464e-02,  8.0313e-01,  3.2026e-01],\n",
      "        [ 4.4025e-04,  9.9480e-01,  1.4734e-01]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0180], requires_grad=True)\n",
      "Loss= tensor(74.8538, grad_fn=<DivBackward0>)\n",
      "i= 59\n",
      "w.grad= tensor([[ 142.1475, -103.5000,  -46.7225],\n",
      "        [  88.6499,  -34.3183,  -78.1161]])\n",
      "b.grad= tensor([0.8124, 0.1893])\n",
      "new w tensor([[-0.1016,  0.8083,  0.3226],\n",
      "        [-0.0040,  0.9965,  0.1512]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0180], requires_grad=True)\n",
      "Loss= tensor(72.3379, grad_fn=<DivBackward0>)\n",
      "i= 60\n",
      "w.grad= tensor([[137.8591, -99.8713, -46.1332],\n",
      "        [ 86.3003, -32.4784, -77.5534]])\n",
      "b.grad= tensor([0.7826, 0.1746])\n",
      "new w tensor([[-0.1085,  0.8133,  0.3249],\n",
      "        [-0.0083,  0.9981,  0.1551]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0181], requires_grad=True)\n",
      "Loss= tensor(69.9591, grad_fn=<DivBackward0>)\n",
      "i= 61\n",
      "w.grad= tensor([[133.7054, -96.3593, -45.5585],\n",
      "        [ 84.0233, -30.7001, -76.9993]])\n",
      "b.grad= tensor([0.7537, 0.1603])\n",
      "new w tensor([[-0.1151,  0.8181,  0.3272],\n",
      "        [-0.0125,  0.9997,  0.1590]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0181], requires_grad=True)\n",
      "Loss= tensor(67.7090, grad_fn=<DivBackward0>)\n",
      "i= 62\n",
      "w.grad= tensor([[129.6828, -92.9597, -44.9977],\n",
      "        [ 81.8163, -28.9813, -76.4539]])\n",
      "b.grad= tensor([0.7258, 0.1466])\n",
      "new w tensor([[-0.1216,  0.8228,  0.3294],\n",
      "        [-0.0166,  1.0011,  0.1628]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0181], requires_grad=True)\n",
      "Loss= tensor(65.5798, grad_fn=<DivBackward0>)\n",
      "i= 63\n",
      "w.grad= tensor([[125.7859, -89.6701, -44.4509],\n",
      "        [ 79.6769, -27.3208, -75.9170]])\n",
      "b.grad= tensor([0.6987, 0.1333])\n",
      "new w tensor([[-0.1279,  0.8272,  0.3317],\n",
      "        [-0.0206,  1.0025,  0.1666]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0181], requires_grad=True)\n",
      "Loss= tensor(63.5641, grad_fn=<DivBackward0>)\n",
      "i= 64\n",
      "w.grad= tensor([[122.0124, -86.4853, -43.9166],\n",
      "        [ 77.6025, -25.7169, -75.3888]])\n",
      "b.grad= tensor([0.6726, 0.1205])\n",
      "new w tensor([[-0.1340,  0.8316,  0.3338],\n",
      "        [-0.0245,  1.0038,  0.1704]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0181], requires_grad=True)\n",
      "Loss= tensor(61.6551, grad_fn=<DivBackward0>)\n",
      "i= 65\n",
      "w.grad= tensor([[118.3561, -83.4045, -43.3962],\n",
      "        [ 75.5923, -24.1667, -74.8681]])\n",
      "b.grad= tensor([0.6473, 0.1081])\n",
      "new w tensor([[-0.1399,  0.8357,  0.3360],\n",
      "        [-0.0282,  1.0050,  0.1741]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0181], requires_grad=True)\n",
      "Loss= tensor(59.8462, grad_fn=<DivBackward0>)\n",
      "i= 66\n",
      "w.grad= tensor([[114.8160, -80.4211, -42.8871],\n",
      "        [ 73.6438, -22.6689, -74.3552]])\n",
      "b.grad= tensor([0.6228, 0.0962])\n",
      "new w tensor([[-0.1457,  0.8398,  0.3382],\n",
      "        [-0.0319,  1.0061,  0.1778]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0181], requires_grad=True)\n",
      "Loss= tensor(58.1314, grad_fn=<DivBackward0>)\n",
      "i= 67\n",
      "w.grad= tensor([[111.3861, -77.5350, -42.3908],\n",
      "        [ 71.7543, -21.2227, -73.8502]])\n",
      "b.grad= tensor([0.5991, 0.0847])\n",
      "new w tensor([[-0.1513,  0.8436,  0.3403],\n",
      "        [-0.0355,  1.0072,  0.1815]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0181], requires_grad=True)\n",
      "Loss= tensor(56.5050, grad_fn=<DivBackward0>)\n",
      "i= 68\n",
      "w.grad= tensor([[108.0643, -74.7411, -41.9058],\n",
      "        [ 69.9232, -19.8250, -73.3522]])\n",
      "b.grad= tensor([0.5761, 0.0736])\n",
      "new w tensor([[-0.1567,  0.8474,  0.3424],\n",
      "        [-0.0390,  1.0082,  0.1852]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0181], requires_grad=True)\n",
      "Loss= tensor(54.9617, grad_fn=<DivBackward0>)\n",
      "i= 69\n",
      "w.grad= tensor([[104.8459, -72.0385, -41.4329],\n",
      "        [ 68.1483, -18.4744, -72.8611]])\n",
      "b.grad= tensor([0.5540, 0.0628])\n",
      "new w tensor([[-0.1619,  0.8510,  0.3444],\n",
      "        [-0.0424,  1.0091,  0.1888]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0181], requires_grad=True)\n",
      "Loss= tensor(53.4964, grad_fn=<DivBackward0>)\n",
      "i= 70\n",
      "w.grad= tensor([[101.7295, -69.4219, -40.9702],\n",
      "        [ 66.4279, -17.1697, -72.3769]])\n",
      "b.grad= tensor([0.5325, 0.0525])\n",
      "new w tensor([[-0.1670,  0.8545,  0.3465],\n",
      "        [-0.0457,  1.0100,  0.1924]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0181], requires_grad=True)\n",
      "Loss= tensor(52.1046, grad_fn=<DivBackward0>)\n",
      "i= 71\n",
      "w.grad= tensor([[ 98.7104, -66.8902, -40.5186],\n",
      "        [ 64.7577, -15.9122, -71.9009]])\n",
      "b.grad= tensor([0.5117, 0.0425])\n",
      "new w tensor([[-0.1719,  0.8578,  0.3485],\n",
      "        [-0.0490,  1.0107,  0.1960]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0181], requires_grad=True)\n",
      "Loss= tensor(50.7816, grad_fn=<DivBackward0>)\n",
      "i= 72\n",
      "w.grad= tensor([[ 95.7865, -64.4396, -40.0768],\n",
      "        [ 63.1399, -14.6964, -71.4306]])\n",
      "b.grad= tensor([0.4917, 0.0329])\n",
      "new w tensor([[-0.1767,  0.8610,  0.3505],\n",
      "        [-0.0521,  1.0115,  0.1996]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0181], requires_grad=True)\n",
      "Loss= tensor(49.5235, grad_fn=<DivBackward0>)\n",
      "i= 73\n",
      "w.grad= tensor([[ 92.9538, -62.0685, -39.6454],\n",
      "        [ 61.5721, -13.5212, -70.9661]])\n",
      "b.grad= tensor([0.4722, 0.0237])\n",
      "new w tensor([[-0.1814,  0.8641,  0.3525],\n",
      "        [-0.0552,  1.0122,  0.2032]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0181], requires_grad=True)\n",
      "Loss= tensor(48.3263, grad_fn=<DivBackward0>)\n",
      "i= 74\n",
      "w.grad= tensor([[ 90.2093, -59.7750, -39.2243],\n",
      "        [ 60.0513, -12.3873, -70.5084]])\n",
      "b.grad= tensor([0.4534, 0.0147])\n",
      "new w tensor([[-0.1859,  0.8671,  0.3545],\n",
      "        [-0.0582,  1.0128,  0.2067]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0181], requires_grad=True)\n",
      "Loss= tensor(47.1865, grad_fn=<DivBackward0>)\n",
      "i= 75\n",
      "w.grad= tensor([[ 87.5520, -57.5541, -38.8117],\n",
      "        [ 58.5757, -11.2938, -70.0576]])\n",
      "b.grad= tensor([0.4352, 0.0061])\n",
      "new w tensor([[-0.1902,  0.8700,  0.3564],\n",
      "        [-0.0611,  1.0133,  0.2102]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0181], requires_grad=True)\n",
      "Loss= tensor(46.1005, grad_fn=<DivBackward0>)\n",
      "i= 76\n",
      "w.grad= tensor([[ 84.9772, -55.4058, -38.4088],\n",
      "        [ 57.1458, -10.2370, -69.6121]])\n",
      "b.grad= tensor([ 0.4177, -0.0022])\n",
      "new w tensor([[-0.1945,  0.8728,  0.3583],\n",
      "        [-0.0640,  1.0139,  0.2137]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0181], requires_grad=True)\n",
      "Loss= tensor(45.0653, grad_fn=<DivBackward0>)\n",
      "i= 77\n",
      "w.grad= tensor([[ 82.4828, -53.3274, -38.0150],\n",
      "        [ 55.7599,  -9.2160, -69.1717]])\n",
      "b.grad= tensor([ 0.4006, -0.0102])\n",
      "new w tensor([[-0.1986,  0.8754,  0.3602],\n",
      "        [-0.0668,  1.0143,  0.2171]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0181], requires_grad=True)\n",
      "Loss= tensor(44.0777, grad_fn=<DivBackward0>)\n",
      "i= 78\n",
      "w.grad= tensor([[ 80.0669, -51.3158, -37.6295],\n",
      "        [ 54.4131,  -8.2341, -68.7391]])\n",
      "b.grad= tensor([ 0.3842, -0.0179])\n",
      "new w tensor([[-0.2026,  0.8780,  0.3621],\n",
      "        [-0.0695,  1.0147,  0.2206]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0181], requires_grad=True)\n",
      "Loss= tensor(43.1350, grad_fn=<DivBackward0>)\n",
      "i= 79\n",
      "w.grad= tensor([[ 77.7266, -49.3696, -37.2524],\n",
      "        [ 53.1107,  -7.2822, -68.3095]])\n",
      "b.grad= tensor([ 0.3683, -0.0253])\n",
      "new w tensor([[-0.2065,  0.8805,  0.3640],\n",
      "        [-0.0722,  1.0151,  0.2240]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0181], requires_grad=True)\n",
      "Loss= tensor(42.2345, grad_fn=<DivBackward0>)\n",
      "i= 80\n",
      "w.grad= tensor([[ 75.4589, -47.4871, -36.8839],\n",
      "        [ 51.8448,  -6.3668, -67.8872]])\n",
      "b.grad= tensor([ 0.3529, -0.0325])\n",
      "new w tensor([[-0.2103,  0.8828,  0.3658],\n",
      "        [-0.0748,  1.0154,  0.2274]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0181], requires_grad=True)\n",
      "Loss= tensor(41.3738, grad_fn=<DivBackward0>)\n",
      "i= 81\n",
      "w.grad= tensor([[ 73.2631, -45.6645, -36.5224],\n",
      "        [ 50.6175,  -5.4831, -67.4698]])\n",
      "b.grad= tensor([ 0.3380, -0.0394])\n",
      "new w tensor([[-0.2139,  0.8851,  0.3676],\n",
      "        [-0.0773,  1.0157,  0.2307]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0181], requires_grad=True)\n",
      "Loss= tensor(40.5505, grad_fn=<DivBackward0>)\n",
      "i= 82\n",
      "w.grad= tensor([[ 71.1353, -43.9018, -36.1692],\n",
      "        [ 49.4292,  -4.6282, -67.0563]])\n",
      "b.grad= tensor([ 0.3236, -0.0460])\n",
      "new w tensor([[-0.2175,  0.8873,  0.3695],\n",
      "        [-0.0798,  1.0159,  0.2341]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0181], requires_grad=True)\n",
      "Loss= tensor(39.7626, grad_fn=<DivBackward0>)\n",
      "i= 83\n",
      "w.grad= tensor([[ 69.0741, -42.1964, -35.8233],\n",
      "        [ 48.2742,  -3.8062, -66.6493]])\n",
      "b.grad= tensor([ 0.3097, -0.0524])\n",
      "new w tensor([[-0.2210,  0.8894,  0.3712],\n",
      "        [-0.0822,  1.0161,  0.2374]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0181], requires_grad=True)\n",
      "Loss= tensor(39.0078, grad_fn=<DivBackward0>)\n",
      "i= 84\n",
      "w.grad= tensor([[ 67.0775, -40.5461, -35.4845],\n",
      "        [ 47.1544,  -3.0128, -66.2469]])\n",
      "b.grad= tensor([ 0.2962, -0.0585])\n",
      "new w tensor([[-0.2243,  0.8914,  0.3730],\n",
      "        [-0.0845,  1.0163,  0.2407]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0181], requires_grad=True)\n",
      "Loss= tensor(38.2843, grad_fn=<DivBackward0>)\n",
      "i= 85\n",
      "w.grad= tensor([[ 65.1427, -38.9502, -35.1531],\n",
      "        [ 46.0690,  -2.2468, -65.8487]])\n",
      "b.grad= tensor([ 0.2832, -0.0644])\n",
      "new w tensor([[-0.2276,  0.8934,  0.3748],\n",
      "        [-0.0868,  1.0164,  0.2440]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0181], requires_grad=True)\n",
      "Loss= tensor(37.5902, grad_fn=<DivBackward0>)\n",
      "i= 86\n",
      "w.grad= tensor([[ 63.2688, -37.4057, -34.8281],\n",
      "        [ 45.0152,  -1.5090, -65.4557]])\n",
      "b.grad= tensor([ 0.2706, -0.0701])\n",
      "new w tensor([[-0.2307,  0.8953,  0.3765],\n",
      "        [-0.0891,  1.0164,  0.2473]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0181], requires_grad=True)\n",
      "Loss= tensor(36.9240, grad_fn=<DivBackward0>)\n",
      "i= 87\n",
      "w.grad= tensor([[ 61.4526, -35.9126, -34.5103],\n",
      "        [ 43.9937,  -0.7970, -65.0668]])\n",
      "b.grad= tensor([ 0.2584, -0.0755])\n",
      "new w tensor([[-0.2338,  0.8971,  0.3782],\n",
      "        [-0.0913,  1.0165,  0.2506]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0181], requires_grad=True)\n",
      "Loss= tensor(36.2839, grad_fn=<DivBackward0>)\n",
      "i= 88\n",
      "w.grad= tensor([[ 59.6942, -34.4668, -34.1981],\n",
      "        [ 43.0016,  -0.1118, -64.6830]])\n",
      "b.grad= tensor([ 0.2466, -0.0808])\n",
      "new w tensor([[-0.2368,  0.8988,  0.3800],\n",
      "        [-0.0934,  1.0165,  0.2538]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0181], requires_grad=True)\n",
      "Loss= tensor(35.6686, grad_fn=<DivBackward0>)\n",
      "i= 89\n",
      "w.grad= tensor([[ 57.9902, -33.0686, -33.8924],\n",
      "        [ 42.0396,   0.5495, -64.3031]])\n",
      "b.grad= tensor([ 0.2352, -0.0859])\n",
      "new w tensor([[-0.2397,  0.9004,  0.3816],\n",
      "        [-0.0955,  1.0165,  0.2570]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0181], requires_grad=True)\n",
      "Loss= tensor(35.0765, grad_fn=<DivBackward0>)\n",
      "i= 90\n",
      "w.grad= tensor([[ 56.3387, -31.7169, -33.5931],\n",
      "        [ 41.1073,   1.1877, -63.9268]])\n",
      "b.grad= tensor([ 0.2242, -0.0907])\n",
      "new w tensor([[-0.2425,  0.9020,  0.3833],\n",
      "        [-0.0976,  1.0164,  0.2602]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0181], requires_grad=True)\n",
      "Loss= tensor(34.5065, grad_fn=<DivBackward0>)\n",
      "i= 91\n",
      "w.grad= tensor([[ 54.7391, -30.4089, -33.2994],\n",
      "        [ 40.2007,   1.8005, -63.5560]])\n",
      "b.grad= tensor([ 0.2136, -0.0954])\n",
      "new w tensor([[-0.2452,  0.9035,  0.3850],\n",
      "        [-0.0996,  1.0163,  0.2634]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0181], requires_grad=True)\n",
      "Loss= tensor(33.9572, grad_fn=<DivBackward0>)\n",
      "i= 92\n",
      "w.grad= tensor([[ 53.1895, -29.1436, -33.0114],\n",
      "        [ 39.3226,   2.3929, -63.1880]])\n",
      "b.grad= tensor([ 0.2033, -0.0998])\n",
      "new w tensor([[-0.2479,  0.9050,  0.3866],\n",
      "        [-0.1016,  1.0162,  0.2665]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0181], requires_grad=True)\n",
      "Loss= tensor(33.4275, grad_fn=<DivBackward0>)\n",
      "i= 93\n",
      "w.grad= tensor([[ 51.6878, -27.9198, -32.7289],\n",
      "        [ 38.4690,   2.9617, -62.8251]])\n",
      "b.grad= tensor([ 0.1934, -0.1041])\n",
      "new w tensor([[-0.2505,  0.9064,  0.3883],\n",
      "        [-0.1035,  1.0160,  0.2697]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0181], requires_grad=True)\n",
      "Loss= tensor(32.9164, grad_fn=<DivBackward0>)\n",
      "i= 94\n",
      "w.grad= tensor([[ 50.2329, -26.7364, -32.4519],\n",
      "        [ 37.6418,   3.5110, -62.4652]])\n",
      "b.grad= tensor([ 0.1838, -0.1083])\n",
      "new w tensor([[-0.2530,  0.9077,  0.3899],\n",
      "        [-0.1054,  1.0159,  0.2728]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0181], requires_grad=True)\n",
      "Loss= tensor(32.4228, grad_fn=<DivBackward0>)\n",
      "i= 95\n",
      "w.grad= tensor([[ 48.8230, -25.5919, -32.1802],\n",
      "        [ 36.8378,   4.0383, -62.1098]])\n",
      "b.grad= tensor([ 0.1745, -0.1122])\n",
      "new w tensor([[-0.2554,  0.9090,  0.3915],\n",
      "        [-0.1072,  1.0157,  0.2759]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(31.9456, grad_fn=<DivBackward0>)\n",
      "i= 96\n",
      "w.grad= tensor([[ 47.4567, -24.4856, -31.9139],\n",
      "        [ 36.0581,   4.5466, -61.7577]])\n",
      "b.grad= tensor([ 0.1655, -0.1160])\n",
      "new w tensor([[-0.2578,  0.9102,  0.3931],\n",
      "        [-0.1090,  1.0154,  0.2790]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(31.4841, grad_fn=<DivBackward0>)\n",
      "i= 97\n",
      "w.grad= tensor([[ 46.1342, -23.4140, -31.6514],\n",
      "        [ 35.3020,   5.0370, -61.4087]])\n",
      "b.grad= tensor([ 0.1568, -0.1197])\n",
      "new w tensor([[-0.2601,  0.9114,  0.3947],\n",
      "        [-0.1108,  1.0152,  0.2821]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(31.0375, grad_fn=<DivBackward0>)\n",
      "i= 98\n",
      "w.grad= tensor([[ 44.8515, -22.3790, -31.3946],\n",
      "        [ 34.5667,   5.5071, -61.0642]])\n",
      "b.grad= tensor([ 0.1484, -0.1232])\n",
      "new w tensor([[-0.2624,  0.9125,  0.3963],\n",
      "        [-0.1125,  1.0149,  0.2851]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(30.6049, grad_fn=<DivBackward0>)\n",
      "i= 99\n",
      "w.grad= tensor([[ 43.6088, -21.3780, -31.1423],\n",
      "        [ 33.8541,   5.9608, -60.7224]])\n",
      "b.grad= tensor([ 0.1403, -0.1265])\n",
      "new w tensor([[-0.2645,  0.9136,  0.3978],\n",
      "        [-0.1142,  1.0146,  0.2882]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(30.1855, grad_fn=<DivBackward0>)\n",
      "i= 100\n",
      "w.grad= tensor([[ 42.4047, -20.4098, -30.8945],\n",
      "        [ 33.1617,   6.3966, -60.3843]])\n",
      "b.grad= tensor([ 0.1325, -0.1297])\n",
      "new w tensor([[-0.2667,  0.9146,  0.3994],\n",
      "        [-0.1159,  1.0143,  0.2912]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(29.7787, grad_fn=<DivBackward0>)\n",
      "i= 101\n",
      "w.grad= tensor([[ 41.2384, -19.4733, -30.6507],\n",
      "        [ 32.4895,   6.8156, -60.0496]])\n",
      "b.grad= tensor([ 0.1249, -0.1328])\n",
      "new w tensor([[-0.2687,  0.9156,  0.4009],\n",
      "        [-0.1175,  1.0140,  0.2942]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(29.3838, grad_fn=<DivBackward0>)\n",
      "i= 102\n",
      "w.grad= tensor([[ 40.1070, -18.5691, -30.4121],\n",
      "        [ 31.8371,   7.2191, -59.7178]])\n",
      "b.grad= tensor([ 0.1176, -0.1357])\n",
      "new w tensor([[-0.2707,  0.9165,  0.4024],\n",
      "        [-0.1191,  1.0136,  0.2972]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(29.0001, grad_fn=<DivBackward0>)\n",
      "i= 103\n",
      "w.grad= tensor([[ 39.0114, -17.6939, -30.1771],\n",
      "        [ 31.2031,   7.6061, -59.3895]])\n",
      "b.grad= tensor([ 0.1105, -0.1385])\n",
      "new w tensor([[-0.2727,  0.9174,  0.4039],\n",
      "        [-0.1206,  1.0132,  0.3001]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(28.6273, grad_fn=<DivBackward0>)\n",
      "i= 104\n",
      "w.grad= tensor([[ 37.9501, -16.8476, -29.9459],\n",
      "        [ 30.5874,   7.9778, -59.0645]])\n",
      "b.grad= tensor([ 0.1037, -0.1412])\n",
      "new w tensor([[-0.2746,  0.9182,  0.4054],\n",
      "        [-0.1222,  1.0128,  0.3031]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(28.2645, grad_fn=<DivBackward0>)\n",
      "i= 105\n",
      "w.grad= tensor([[ 36.9209, -16.0298, -29.7190],\n",
      "        [ 29.9901,   8.3357, -58.7420]])\n",
      "b.grad= tensor([ 0.0971, -0.1438])\n",
      "new w tensor([[-0.2764,  0.9190,  0.4069],\n",
      "        [-0.1237,  1.0124,  0.3060]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(27.9114, grad_fn=<DivBackward0>)\n",
      "i= 106\n",
      "w.grad= tensor([[ 35.9238, -15.2391, -29.4957],\n",
      "        [ 29.4100,   8.6796, -58.4224]])\n",
      "b.grad= tensor([ 0.0908, -0.1462])\n",
      "new w tensor([[-0.2782,  0.9198,  0.4084],\n",
      "        [-0.1251,  1.0120,  0.3089]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(27.5675, grad_fn=<DivBackward0>)\n",
      "i= 107\n",
      "w.grad= tensor([[ 34.9570, -14.4753, -29.2766],\n",
      "        [ 28.8463,   9.0093, -58.1060]])\n",
      "b.grad= tensor([ 0.0846, -0.1486])\n",
      "new w tensor([[-0.2800,  0.9205,  0.4098],\n",
      "        [-0.1266,  1.0115,  0.3119]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(27.2323, grad_fn=<DivBackward0>)\n",
      "i= 108\n",
      "w.grad= tensor([[ 34.0210, -13.7358, -29.0602],\n",
      "        [ 28.2983,   9.3254, -57.7927]])\n",
      "b.grad= tensor([ 0.0787, -0.1508])\n",
      "new w tensor([[-0.2817,  0.9212,  0.4113],\n",
      "        [-0.1280,  1.0110,  0.3147]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(26.9055, grad_fn=<DivBackward0>)\n",
      "i= 109\n",
      "w.grad= tensor([[ 33.1123, -13.0229, -28.8486],\n",
      "        [ 27.7661,   9.6288, -57.4821]])\n",
      "b.grad= tensor([ 0.0729, -0.1529])\n",
      "new w tensor([[-0.2833,  0.9219,  0.4127],\n",
      "        [-0.1294,  1.0106,  0.3176]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0180], requires_grad=True)\n",
      "Loss= tensor(26.5865, grad_fn=<DivBackward0>)\n",
      "i= 110\n",
      "w.grad= tensor([[ 32.2332, -12.3319, -28.6392],\n",
      "        [ 27.2489,   9.9197, -57.1744]])\n",
      "b.grad= tensor([ 0.0674, -0.1550])\n",
      "new w tensor([[-0.2849,  0.9225,  0.4142],\n",
      "        [-0.1307,  1.0101,  0.3205]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0179], requires_grad=True)\n",
      "Loss= tensor(26.2750, grad_fn=<DivBackward0>)\n",
      "i= 111\n",
      "w.grad= tensor([[ 31.3804, -11.6652, -28.4337],\n",
      "        [ 26.7468,  10.1987, -56.8691]])\n",
      "b.grad= tensor([ 0.0620, -0.1569])\n",
      "new w tensor([[-0.2865,  0.9231,  0.4156],\n",
      "        [-0.1321,  1.0096,  0.3233]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0179], requires_grad=True)\n",
      "Loss= tensor(25.9707, grad_fn=<DivBackward0>)\n",
      "i= 112\n",
      "w.grad= tensor([[ 30.5539, -11.0205, -28.2313],\n",
      "        [ 26.2587,  10.4662, -56.5665]])\n",
      "b.grad= tensor([ 0.0569, -0.1587])\n",
      "new w tensor([[-0.2880,  0.9236,  0.4170],\n",
      "        [-0.1334,  1.0090,  0.3261]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0179], requires_grad=True)\n",
      "Loss= tensor(25.6732, grad_fn=<DivBackward0>)\n",
      "i= 113\n",
      "w.grad= tensor([[ 29.7529, -10.3976, -28.0321],\n",
      "        [ 25.7847,  10.7227, -56.2663]])\n",
      "b.grad= tensor([ 0.0519, -0.1605])\n",
      "new w tensor([[-0.2895,  0.9241,  0.4184],\n",
      "        [-0.1347,  1.0085,  0.3290]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0179], requires_grad=True)\n",
      "Loss= tensor(25.3823, grad_fn=<DivBackward0>)\n",
      "i= 114\n",
      "w.grad= tensor([[ 28.9760,  -9.7962, -27.8362],\n",
      "        [ 25.3246,  10.9690, -55.9684]])\n",
      "b.grad= tensor([ 0.0471, -0.1621])\n",
      "new w tensor([[-0.2910,  0.9246,  0.4198],\n",
      "        [-0.1360,  1.0080,  0.3318]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0179], requires_grad=True)\n",
      "Loss= tensor(25.0975, grad_fn=<DivBackward0>)\n",
      "i= 115\n",
      "w.grad= tensor([[ 28.2239,  -9.2139, -27.6426],\n",
      "        [ 24.8755,  11.2027, -55.6741]])\n",
      "b.grad= tensor([ 0.0425, -0.1637])\n",
      "new w tensor([[-0.2924,  0.9251,  0.4212],\n",
      "        [-0.1372,  1.0074,  0.3345]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0179], requires_grad=True)\n",
      "Loss= tensor(24.8187, grad_fn=<DivBackward0>)\n",
      "i= 116\n",
      "w.grad= tensor([[ 27.4935,  -8.6530, -27.4529],\n",
      "        [ 24.4407,  11.4282, -55.3812]])\n",
      "b.grad= tensor([ 0.0380, -0.1652])\n",
      "new w tensor([[-0.2938,  0.9255,  0.4226],\n",
      "        [-0.1384,  1.0068,  0.3373]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0179], requires_grad=True)\n",
      "Loss= tensor(24.5457, grad_fn=<DivBackward0>)\n",
      "i= 117\n",
      "w.grad= tensor([[ 26.7869,  -8.1094, -27.2651],\n",
      "        [ 24.0175,  11.6431, -55.0910]])\n",
      "b.grad= tensor([ 0.0337, -0.1666])\n",
      "new w tensor([[-0.2951,  0.9259,  0.4239],\n",
      "        [-0.1396,  1.0062,  0.3401]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0179], requires_grad=True)\n",
      "Loss= tensor(24.2780, grad_fn=<DivBackward0>)\n",
      "i= 118\n",
      "w.grad= tensor([[ 26.1014,  -7.5849, -27.0803],\n",
      "        [ 23.6062,  11.8488, -54.8031]])\n",
      "b.grad= tensor([ 0.0295, -0.1679])\n",
      "new w tensor([[-0.2964,  0.9263,  0.4253],\n",
      "        [-0.1408,  1.0056,  0.3428]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0179], requires_grad=True)\n",
      "Loss= tensor(24.0156, grad_fn=<DivBackward0>)\n",
      "i= 119\n",
      "w.grad= tensor([[ 25.4364,  -7.0788, -26.8986],\n",
      "        [ 23.2055,  12.0443, -54.5181]])\n",
      "b.grad= tensor([ 0.0255, -0.1692])\n",
      "new w tensor([[-0.2977,  0.9267,  0.4266],\n",
      "        [-0.1420,  1.0050,  0.3455]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0179], requires_grad=True)\n",
      "Loss= tensor(23.7581, grad_fn=<DivBackward0>)\n",
      "i= 120\n",
      "w.grad= tensor([[ 24.7918,  -6.5896, -26.7193],\n",
      "        [ 22.8170,  12.2324, -54.2345]])\n",
      "b.grad= tensor([ 0.0216, -0.1703])\n",
      "new w tensor([[-0.2989,  0.9270,  0.4280],\n",
      "        [-0.1431,  1.0044,  0.3482]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0179], requires_grad=True)\n",
      "Loss= tensor(23.5055, grad_fn=<DivBackward0>)\n",
      "i= 121\n",
      "w.grad= tensor([[ 24.1674,  -6.1169, -26.5423],\n",
      "        [ 22.4378,  12.4102, -53.9541]])\n",
      "b.grad= tensor([ 0.0178, -0.1715])\n",
      "new w tensor([[-0.3001,  0.9273,  0.4293],\n",
      "        [-0.1442,  1.0038,  0.3509]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0179], requires_grad=True)\n",
      "Loss= tensor(23.2574, grad_fn=<DivBackward0>)\n",
      "i= 122\n",
      "w.grad= tensor([[ 23.5616,  -5.6606, -26.3679],\n",
      "        [ 22.0703,  12.5811, -53.6751]])\n",
      "b.grad= tensor([ 0.0142, -0.1725])\n",
      "new w tensor([[-0.3013,  0.9276,  0.4306],\n",
      "        [-0.1453,  1.0032,  0.3536]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(23.0138, grad_fn=<DivBackward0>)\n",
      "i= 123\n",
      "w.grad= tensor([[ 22.9736,  -5.2210, -26.1965],\n",
      "        [ 21.7131,  12.7445, -53.3980]])\n",
      "b.grad= tensor([ 0.0107, -0.1735])\n",
      "new w tensor([[-0.3024,  0.9278,  0.4319],\n",
      "        [-0.1464,  1.0025,  0.3563]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(22.7744, grad_fn=<DivBackward0>)\n",
      "i= 124\n",
      "w.grad= tensor([[ 22.4052,  -4.7943, -26.0262],\n",
      "        [ 21.3644,  12.8984, -53.1239]])\n",
      "b.grad= tensor([ 0.0074, -0.1744])\n",
      "new w tensor([[-0.3036,  0.9281,  0.4332],\n",
      "        [-0.1475,  1.0019,  0.3590]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(22.5391, grad_fn=<DivBackward0>)\n",
      "i= 125\n",
      "w.grad= tensor([[ 21.8527,  -4.3840, -25.8591],\n",
      "        [ 21.0264,  13.0466, -52.8509]])\n",
      "b.grad= tensor([ 0.0041, -0.1753])\n",
      "new w tensor([[-0.3047,  0.9283,  0.4345],\n",
      "        [-0.1485,  1.0012,  0.3616]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(22.3077, grad_fn=<DivBackward0>)\n",
      "i= 126\n",
      "w.grad= tensor([[ 21.3174,  -3.9875, -25.6941],\n",
      "        [ 20.6959,  13.1855, -52.5812]])\n",
      "b.grad= tensor([ 0.0010, -0.1761])\n",
      "new w tensor([[-0.3057,  0.9285,  0.4358],\n",
      "        [-0.1496,  1.0006,  0.3642]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(22.0801, grad_fn=<DivBackward0>)\n",
      "i= 127\n",
      "w.grad= tensor([[ 20.7981,  -3.6048, -25.5313],\n",
      "        [ 20.3760,  13.3193, -52.3123]])\n",
      "b.grad= tensor([-0.0020, -0.1768])\n",
      "new w tensor([[-0.3068,  0.9287,  0.4371],\n",
      "        [-0.1506,  0.9999,  0.3668]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(21.8561, grad_fn=<DivBackward0>)\n",
      "i= 128\n",
      "w.grad= tensor([[ 20.2949,  -3.2351, -25.3703],\n",
      "        [ 20.0631,  13.4445, -52.0465]])\n",
      "b.grad= tensor([-0.0049, -0.1775])\n",
      "new w tensor([[-0.3078,  0.9288,  0.4383],\n",
      "        [-0.1516,  0.9993,  0.3694]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(21.6356, grad_fn=<DivBackward0>)\n",
      "i= 129\n",
      "w.grad= tensor([[ 19.8064,  -2.8789, -25.2119],\n",
      "        [ 19.7594,  13.5643, -51.7819]])\n",
      "b.grad= tensor([-0.0077, -0.1781])\n",
      "new w tensor([[-0.3088,  0.9290,  0.4396],\n",
      "        [-0.1526,  0.9986,  0.3720]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(21.4185, grad_fn=<DivBackward0>)\n",
      "i= 130\n",
      "w.grad= tensor([[ 19.3332,  -2.5345, -25.0550],\n",
      "        [ 19.4638,  13.6776, -51.5194]])\n",
      "b.grad= tensor([-0.0103, -0.1787])\n",
      "new w tensor([[-0.3097,  0.9291,  0.4408],\n",
      "        [-0.1535,  0.9979,  0.3746]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(21.2047, grad_fn=<DivBackward0>)\n",
      "i= 131\n",
      "w.grad= tensor([[ 18.8736,  -2.2029, -24.9005],\n",
      "        [ 19.1756,  13.7843, -51.2590]])\n",
      "b.grad= tensor([-0.0129, -0.1793])\n",
      "new w tensor([[-0.3107,  0.9292,  0.4421],\n",
      "        [-0.1545,  0.9972,  0.3772]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(20.9941, grad_fn=<DivBackward0>)\n",
      "i= 132\n",
      "w.grad= tensor([[ 18.4290,  -1.8816, -24.7472],\n",
      "        [ 18.8964,  13.8870, -50.9994]])\n",
      "b.grad= tensor([-0.0154, -0.1797])\n",
      "new w tensor([[-0.3116,  0.9293,  0.4433],\n",
      "        [-0.1555,  0.9965,  0.3797]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0178], requires_grad=True)\n",
      "Loss= tensor(20.7866, grad_fn=<DivBackward0>)\n",
      "i= 133\n",
      "w.grad= tensor([[ 17.9963,  -1.5735, -24.5967],\n",
      "        [ 18.6223,  13.9810, -50.7432]])\n",
      "b.grad= tensor([-0.0178, -0.1802])\n",
      "new w tensor([[-0.3125,  0.9294,  0.4446],\n",
      "        [-0.1564,  0.9958,  0.3823]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0177], requires_grad=True)\n",
      "Loss= tensor(20.5819, grad_fn=<DivBackward0>)\n",
      "i= 134\n",
      "w.grad= tensor([[ 17.5773,  -1.2754, -24.4476],\n",
      "        [ 18.3569,  14.0716, -50.4878]])\n",
      "b.grad= tensor([-0.0201, -0.1806])\n",
      "new w tensor([[-0.3134,  0.9295,  0.4458],\n",
      "        [-0.1573,  0.9951,  0.3848]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0177], requires_grad=True)\n",
      "Loss= tensor(20.3802, grad_fn=<DivBackward0>)\n",
      "i= 135\n",
      "w.grad= tensor([[ 17.1710,  -0.9879, -24.3002],\n",
      "        [ 18.0979,  14.1562, -50.2345]])\n",
      "b.grad= tensor([-0.0224, -0.1809])\n",
      "new w tensor([[-0.3142,  0.9295,  0.4470],\n",
      "        [-0.1582,  0.9944,  0.3873]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0177], requires_grad=True)\n",
      "Loss= tensor(20.1813, grad_fn=<DivBackward0>)\n",
      "i= 136\n",
      "w.grad= tensor([[ 16.7770,  -0.7103, -24.1544],\n",
      "        [ 17.8459,  14.2358, -49.9827]])\n",
      "b.grad= tensor([-0.0245, -0.1813])\n",
      "new w tensor([[-0.3151,  0.9295,  0.4482],\n",
      "        [-0.1591,  0.9937,  0.3898]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0177], requires_grad=True)\n",
      "Loss= tensor(19.9851, grad_fn=<DivBackward0>)\n",
      "i= 137\n",
      "w.grad= tensor([[ 16.3944,  -0.4431, -24.0106],\n",
      "        [ 17.6002,  14.3105, -49.7328]])\n",
      "b.grad= tensor([-0.0265, -0.1815])\n",
      "new w tensor([[-0.3159,  0.9296,  0.4494],\n",
      "        [-0.1600,  0.9930,  0.3923]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0177], requires_grad=True)\n",
      "Loss= tensor(19.7915, grad_fn=<DivBackward0>)\n",
      "i= 138\n",
      "w.grad= tensor([[ 16.0232,  -0.1857, -23.8685],\n",
      "        [ 17.3604,  14.3797, -49.4848]])\n",
      "b.grad= tensor([-0.0285, -0.1818])\n",
      "new w tensor([[-0.3167,  0.9296,  0.4506],\n",
      "        [-0.1608,  0.9922,  0.3948]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0177], requires_grad=True)\n",
      "Loss= tensor(19.6005, grad_fn=<DivBackward0>)\n",
      "i= 139\n",
      "w.grad= tensor([[ 15.6637,   0.0633, -23.7275],\n",
      "        [ 17.1273,  14.4449, -49.2381]])\n",
      "b.grad= tensor([-0.0304, -0.1820])\n",
      "new w tensor([[-0.3175,  0.9296,  0.4518],\n",
      "        [-0.1617,  0.9915,  0.3972]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0177], requires_grad=True)\n",
      "Loss= tensor(19.4119, grad_fn=<DivBackward0>)\n",
      "i= 140\n",
      "w.grad= tensor([[ 15.3143,   0.3026, -23.5886],\n",
      "        [ 16.8995,  14.5052, -48.9934]])\n",
      "b.grad= tensor([-0.0322, -0.1821])\n",
      "new w tensor([[-0.3182,  0.9296,  0.4530],\n",
      "        [-0.1625,  0.9908,  0.3997]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0177], requires_grad=True)\n",
      "Loss= tensor(19.2257, grad_fn=<DivBackward0>)\n",
      "i= 141\n",
      "w.grad= tensor([[ 14.9747,   0.5324, -23.4516],\n",
      "        [ 16.6782,  14.5618, -48.7498]])\n",
      "b.grad= tensor([-0.0340, -0.1823])\n",
      "new w tensor([[-0.3190,  0.9295,  0.4541],\n",
      "        [-0.1634,  0.9901,  0.4021]], requires_grad=True)\n",
      "new b tensor([-1.1858, -1.0177], requires_grad=True)\n",
      "Loss= tensor(19.0419, grad_fn=<DivBackward0>)\n",
      "i= 142\n",
      "w.grad= tensor([[ 14.6459,   0.7547, -23.3156],\n",
      "        [ 16.4625,  14.6144, -48.5078]])\n",
      "b.grad= tensor([-0.0357, -0.1824])\n",
      "new w tensor([[-0.3197,  0.9295,  0.4553],\n",
      "        [-0.1642,  0.9893,  0.4045]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0177], requires_grad=True)\n",
      "Loss= tensor(18.8604, grad_fn=<DivBackward0>)\n",
      "i= 143\n",
      "w.grad= tensor([[ 14.3270,   0.9690, -23.1809],\n",
      "        [ 16.2514,  14.6619, -48.2680]])\n",
      "b.grad= tensor([-0.0373, -0.1824])\n",
      "new w tensor([[-0.3204,  0.9294,  0.4565],\n",
      "        [-0.1650,  0.9886,  0.4069]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0177], requires_grad=True)\n",
      "Loss= tensor(18.6812, grad_fn=<DivBackward0>)\n",
      "i= 144\n",
      "w.grad= tensor([[ 14.0178,   1.1758, -23.0475],\n",
      "        [ 16.0458,  14.7061, -48.0294]])\n",
      "b.grad= tensor([-0.0389, -0.1825])\n",
      "new w tensor([[-0.3211,  0.9294,  0.4576],\n",
      "        [-0.1658,  0.9879,  0.4093]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(18.5041, grad_fn=<DivBackward0>)\n",
      "i= 145\n",
      "w.grad= tensor([[ 13.7167,   1.3737, -22.9163],\n",
      "        [ 15.8453,  14.7464, -47.7924]])\n",
      "b.grad= tensor([-0.0404, -0.1825])\n",
      "new w tensor([[-0.3218,  0.9293,  0.4588],\n",
      "        [-0.1666,  0.9871,  0.4117]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(18.3291, grad_fn=<DivBackward0>)\n",
      "i= 146\n",
      "w.grad= tensor([[ 13.4252,   1.5651, -22.7859],\n",
      "        [ 15.6499,  14.7831, -47.5569]])\n",
      "b.grad= tensor([-0.0418, -0.1824])\n",
      "new w tensor([[-0.3225,  0.9292,  0.4599],\n",
      "        [-0.1674,  0.9864,  0.4141]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(18.1563, grad_fn=<DivBackward0>)\n",
      "i= 147\n",
      "w.grad= tensor([[ 13.1418,   1.7488, -22.6571],\n",
      "        [ 15.4592,  14.8164, -47.3227]])\n",
      "b.grad= tensor([-0.0432, -0.1824])\n",
      "new w tensor([[-0.3232,  0.9292,  0.4610],\n",
      "        [-0.1682,  0.9857,  0.4165]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(17.9854, grad_fn=<DivBackward0>)\n",
      "i= 148\n",
      "w.grad= tensor([[ 12.8674,   1.9265, -22.5292],\n",
      "        [ 15.2726,  14.8458, -47.0905]])\n",
      "b.grad= tensor([-0.0445, -0.1823])\n",
      "new w tensor([[-0.3238,  0.9291,  0.4622],\n",
      "        [-0.1689,  0.9849,  0.4188]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(17.8166, grad_fn=<DivBackward0>)\n",
      "i= 149\n",
      "w.grad= tensor([[ 12.6000,   2.0962, -22.4033],\n",
      "        [ 15.0912,  14.8726, -46.8593]])\n",
      "b.grad= tensor([-0.0457, -0.1822])\n",
      "new w tensor([[-0.3244,  0.9290,  0.4633],\n",
      "        [-0.1697,  0.9842,  0.4212]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(17.6497, grad_fn=<DivBackward0>)\n",
      "i= 150\n",
      "w.grad= tensor([[ 12.3418,   2.2609, -22.2777],\n",
      "        [ 14.9134,  14.8954, -46.6299]])\n",
      "b.grad= tensor([-0.0469, -0.1821])\n",
      "new w tensor([[-0.3250,  0.9288,  0.4644],\n",
      "        [-0.1704,  0.9834,  0.4235]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(17.4846, grad_fn=<DivBackward0>)\n",
      "i= 151\n",
      "w.grad= tensor([[ 12.0902,   2.4186, -22.1539],\n",
      "        [ 14.7404,  14.9157, -46.4016]])\n",
      "b.grad= tensor([-0.0481, -0.1819])\n",
      "new w tensor([[-0.3257,  0.9287,  0.4655],\n",
      "        [-0.1712,  0.9827,  0.4258]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(17.3214, grad_fn=<DivBackward0>)\n",
      "i= 152\n",
      "w.grad= tensor([[ 11.8456,   2.5696, -22.0317],\n",
      "        [ 14.5718,  14.9337, -46.1745]])\n",
      "b.grad= tensor([-0.0492, -0.1818])\n",
      "new w tensor([[-0.3262,  0.9286,  0.4666],\n",
      "        [-0.1719,  0.9819,  0.4281]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(17.1601, grad_fn=<DivBackward0>)\n",
      "i= 153\n",
      "w.grad= tensor([[ 11.6082,   2.7149, -21.9105],\n",
      "        [ 14.4068,  14.9482, -45.9489]])\n",
      "b.grad= tensor([-0.0503, -0.1816])\n",
      "new w tensor([[-0.3268,  0.9285,  0.4677],\n",
      "        [-0.1726,  0.9812,  0.4304]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(17.0005, grad_fn=<DivBackward0>)\n",
      "i= 154\n",
      "w.grad= tensor([[ 11.3790,   2.8561, -21.7895],\n",
      "        [ 14.2455,  14.9600, -45.7249]])\n",
      "b.grad= tensor([-0.0513, -0.1813])\n",
      "new w tensor([[-0.3274,  0.9283,  0.4688],\n",
      "        [-0.1733,  0.9804,  0.4327]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0176], requires_grad=True)\n",
      "Loss= tensor(16.8428, grad_fn=<DivBackward0>)\n",
      "i= 155\n",
      "w.grad= tensor([[ 11.1550,   2.9902, -21.6706],\n",
      "        [ 14.0885,  14.9698, -45.5019]])\n",
      "b.grad= tensor([-0.0523, -0.1811])\n",
      "new w tensor([[-0.3279,  0.9282,  0.4699],\n",
      "        [-0.1740,  0.9797,  0.4350]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(16.6867, grad_fn=<DivBackward0>)\n",
      "i= 156\n",
      "w.grad= tensor([[ 10.9378,   3.1192, -21.5527],\n",
      "        [ 13.9330,  14.9746, -45.2815]])\n",
      "b.grad= tensor([-0.0532, -0.1809])\n",
      "new w tensor([[-0.3285,  0.9280,  0.4709],\n",
      "        [-0.1747,  0.9789,  0.4373]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(16.5323, grad_fn=<DivBackward0>)\n",
      "i= 157\n",
      "w.grad= tensor([[ 10.7271,   3.2436, -21.4356],\n",
      "        [ 13.7837,  14.9799, -45.0608]])\n",
      "b.grad= tensor([-0.0541, -0.1806])\n",
      "new w tensor([[-0.3290,  0.9278,  0.4720],\n",
      "        [-0.1754,  0.9782,  0.4395]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(16.3796, grad_fn=<DivBackward0>)\n",
      "i= 158\n",
      "w.grad= tensor([[ 10.5224,   3.3629, -21.3196],\n",
      "        [ 13.6362,  14.9811, -44.8423]])\n",
      "b.grad= tensor([-0.0549, -0.1803])\n",
      "new w tensor([[-0.3296,  0.9277,  0.4731],\n",
      "        [-0.1761,  0.9774,  0.4418]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(16.2285, grad_fn=<DivBackward0>)\n",
      "i= 159\n",
      "w.grad= tensor([[ 10.3228,   3.4764, -21.2052],\n",
      "        [ 13.4930,  14.9810, -44.6246]])\n",
      "b.grad= tensor([-0.0557, -0.1800])\n",
      "new w tensor([[-0.3301,  0.9275,  0.4741],\n",
      "        [-0.1768,  0.9767,  0.4440]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(16.0790, grad_fn=<DivBackward0>)\n",
      "i= 160\n",
      "w.grad= tensor([[ 10.1304,   3.5872, -21.0908],\n",
      "        [ 13.3526,  14.9784, -44.4083]])\n",
      "b.grad= tensor([-0.0565, -0.1796])\n",
      "new w tensor([[-0.3306,  0.9273,  0.4752],\n",
      "        [-0.1774,  0.9759,  0.4462]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(15.9311, grad_fn=<DivBackward0>)\n",
      "i= 161\n",
      "w.grad= tensor([[  9.9423,   3.6920, -20.9782],\n",
      "        [ 13.2149,  14.9733, -44.1935]])\n",
      "b.grad= tensor([-0.0572, -0.1793])\n",
      "new w tensor([[-0.3311,  0.9271,  0.4762],\n",
      "        [-0.1781,  0.9752,  0.4484]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(15.7847, grad_fn=<DivBackward0>)\n",
      "i= 162\n",
      "w.grad= tensor([[  9.7597,   3.7926, -20.8665],\n",
      "        [ 13.0808,  14.9668, -43.9796]])\n",
      "b.grad= tensor([-0.0579, -0.1789])\n",
      "new w tensor([[-0.3316,  0.9269,  0.4773],\n",
      "        [-0.1788,  0.9744,  0.4506]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(15.6399, grad_fn=<DivBackward0>)\n",
      "i= 163\n",
      "w.grad= tensor([[  9.5829,   3.8898, -20.7552],\n",
      "        [ 12.9495,  14.9581, -43.7672]])\n",
      "b.grad= tensor([-0.0586, -0.1786])\n",
      "new w tensor([[-0.3320,  0.9268,  0.4783],\n",
      "        [-0.1794,  0.9737,  0.4528]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(15.4965, grad_fn=<DivBackward0>)\n",
      "i= 164\n",
      "w.grad= tensor([[  9.4099,   3.9816, -20.6458],\n",
      "        [ 12.8211,  14.9477, -43.5557]])\n",
      "b.grad= tensor([-0.0592, -0.1782])\n",
      "new w tensor([[-0.3325,  0.9266,  0.4794],\n",
      "        [-0.1800,  0.9730,  0.4550]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(15.3546, grad_fn=<DivBackward0>)\n",
      "i= 165\n",
      "w.grad= tensor([[  9.2432,   4.0711, -20.5363],\n",
      "        [ 12.6947,  14.9343, -43.3461]])\n",
      "b.grad= tensor([-0.0598, -0.1778])\n",
      "new w tensor([[-0.3330,  0.9264,  0.4804],\n",
      "        [-0.1807,  0.9722,  0.4571]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0175], requires_grad=True)\n",
      "Loss= tensor(15.2142, grad_fn=<DivBackward0>)\n",
      "i= 166\n",
      "w.grad= tensor([[  9.0799,   4.1553, -20.4285],\n",
      "        [ 12.5720,  14.9206, -43.1369]])\n",
      "b.grad= tensor([-0.0603, -0.1773])\n",
      "new w tensor([[-0.3334,  0.9261,  0.4814],\n",
      "        [-0.1813,  0.9715,  0.4593]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(15.0752, grad_fn=<DivBackward0>)\n",
      "i= 167\n",
      "w.grad= tensor([[  8.9211,   4.2357, -20.3217],\n",
      "        [ 12.4511,  14.9043, -42.9295]])\n",
      "b.grad= tensor([-0.0609, -0.1769])\n",
      "new w tensor([[-0.3339,  0.9259,  0.4824],\n",
      "        [-0.1819,  0.9707,  0.4614]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(14.9377, grad_fn=<DivBackward0>)\n",
      "i= 168\n",
      "w.grad= tensor([[  8.7683,   4.3145, -20.2147],\n",
      "        [ 12.3331,  14.8867, -42.7231]])\n",
      "b.grad= tensor([-0.0614, -0.1765])\n",
      "new w tensor([[-0.3343,  0.9257,  0.4834],\n",
      "        [-0.1825,  0.9700,  0.4636]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(14.8015, grad_fn=<DivBackward0>)\n",
      "i= 169\n",
      "w.grad= tensor([[  8.6182,   4.3879, -20.1096],\n",
      "        [ 12.2174,  14.8673, -42.5179]])\n",
      "b.grad= tensor([-0.0619, -0.1760])\n",
      "new w tensor([[-0.3347,  0.9255,  0.4844],\n",
      "        [-0.1832,  0.9692,  0.4657]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(14.6667, grad_fn=<DivBackward0>)\n",
      "i= 170\n",
      "w.grad= tensor([[  8.4732,   4.4592, -20.0047],\n",
      "        [ 12.1046,  14.8471, -42.3134]])\n",
      "b.grad= tensor([-0.0623, -0.1756])\n",
      "new w tensor([[-0.3352,  0.9253,  0.4854],\n",
      "        [-0.1838,  0.9685,  0.4678]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(14.5332, grad_fn=<DivBackward0>)\n",
      "i= 171\n",
      "w.grad= tensor([[  8.3313,   4.5261, -19.9013],\n",
      "        [ 11.9936,  14.8248, -42.1104]])\n",
      "b.grad= tensor([-0.0627, -0.1751])\n",
      "new w tensor([[-0.3356,  0.9250,  0.4864],\n",
      "        [-0.1844,  0.9677,  0.4699]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(14.4010, grad_fn=<DivBackward0>)\n",
      "i= 172\n",
      "w.grad= tensor([[  8.1937,   4.5904, -19.7984],\n",
      "        [ 11.8849,  14.8013, -41.9085]])\n",
      "b.grad= tensor([-0.0631, -0.1746])\n",
      "new w tensor([[-0.3360,  0.9248,  0.4874],\n",
      "        [-0.1850,  0.9670,  0.4720]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(14.2702, grad_fn=<DivBackward0>)\n",
      "i= 173\n",
      "w.grad= tensor([[  8.0606,   4.6527, -19.6957],\n",
      "        [ 11.7788,  14.7770, -41.7074]])\n",
      "b.grad= tensor([-0.0635, -0.1741])\n",
      "new w tensor([[-0.3364,  0.9246,  0.4884],\n",
      "        [-0.1855,  0.9663,  0.4741]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(14.1407, grad_fn=<DivBackward0>)\n",
      "i= 174\n",
      "w.grad= tensor([[  7.9300,   4.7106, -19.5947],\n",
      "        [ 11.6742,  14.7506, -41.5079]])\n",
      "b.grad= tensor([-0.0639, -0.1736])\n",
      "new w tensor([[-0.3368,  0.9243,  0.4894],\n",
      "        [-0.1861,  0.9655,  0.4762]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(14.0125, grad_fn=<DivBackward0>)\n",
      "i= 175\n",
      "w.grad= tensor([[  7.8039,   4.7667, -19.4938],\n",
      "        [ 11.5717,  14.7228, -41.3093]])\n",
      "b.grad= tensor([-0.0642, -0.1731])\n",
      "new w tensor([[-0.3372,  0.9241,  0.4904],\n",
      "        [-0.1867,  0.9648,  0.4783]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(13.8855, grad_fn=<DivBackward0>)\n",
      "i= 176\n",
      "w.grad= tensor([[  7.6803,   4.8193, -19.3942],\n",
      "        [ 11.4718,  14.6948, -41.1115]])\n",
      "b.grad= tensor([-0.0645, -0.1726])\n",
      "new w tensor([[-0.3376,  0.9239,  0.4913],\n",
      "        [-0.1873,  0.9641,  0.4803]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(13.7598, grad_fn=<DivBackward0>)\n",
      "i= 177\n",
      "w.grad= tensor([[  7.5610,   4.8703, -19.2947],\n",
      "        [ 11.3735,  14.6652, -40.9150]])\n",
      "b.grad= tensor([-0.0648, -0.1720])\n",
      "new w tensor([[-0.3379,  0.9236,  0.4923],\n",
      "        [-0.1879,  0.9633,  0.4824]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0174], requires_grad=True)\n",
      "Loss= tensor(13.6353, grad_fn=<DivBackward0>)\n",
      "i= 178\n",
      "w.grad= tensor([[  7.4442,   4.9177, -19.1965],\n",
      "        [ 11.2770,  14.6346, -40.7196]])\n",
      "b.grad= tensor([-0.0650, -0.1715])\n",
      "new w tensor([[-0.3383,  0.9234,  0.4933],\n",
      "        [-0.1884,  0.9626,  0.4844]], requires_grad=True)\n",
      "new b tensor([-1.1857, -1.0173], requires_grad=True)\n",
      "Loss= tensor(13.5120, grad_fn=<DivBackward0>)\n",
      "i= 179\n",
      "w.grad= tensor([[  7.3308,   4.9631, -19.0988],\n",
      "        [ 11.1820,  14.6023, -40.5254]])\n",
      "b.grad= tensor([-0.0653, -0.1710])\n",
      "new w tensor([[-0.3387,  0.9231,  0.4942],\n",
      "        [-0.1890,  0.9619,  0.4864]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(13.3899, grad_fn=<DivBackward0>)\n",
      "i= 180\n",
      "w.grad= tensor([[  7.2210,   5.0070, -19.0014],\n",
      "        [ 11.0893,  14.5699, -40.3320]])\n",
      "b.grad= tensor([-0.0655, -0.1704])\n",
      "new w tensor([[-0.3390,  0.9229,  0.4952],\n",
      "        [-0.1895,  0.9611,  0.4884]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(13.2690, grad_fn=<DivBackward0>)\n",
      "i= 181\n",
      "w.grad= tensor([[  7.1124,   5.0464, -18.9059],\n",
      "        [ 10.9979,  14.5359, -40.1398]])\n",
      "b.grad= tensor([-0.0657, -0.1699])\n",
      "new w tensor([[-0.3394,  0.9226,  0.4961],\n",
      "        [-0.1901,  0.9604,  0.4904]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(13.1493, grad_fn=<DivBackward0>)\n",
      "i= 182\n",
      "w.grad= tensor([[  7.0080,   5.0852, -18.8102],\n",
      "        [ 10.9077,  14.5002, -39.9491]])\n",
      "b.grad= tensor([-0.0659, -0.1693])\n",
      "new w tensor([[-0.3398,  0.9224,  0.4971],\n",
      "        [-0.1906,  0.9597,  0.4924]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(13.0307, grad_fn=<DivBackward0>)\n",
      "i= 183\n",
      "w.grad= tensor([[  6.9062,   5.1217, -18.7152],\n",
      "        [ 10.8206,  14.4655, -39.7584]])\n",
      "b.grad= tensor([-0.0661, -0.1687])\n",
      "new w tensor([[-0.3401,  0.9221,  0.4980],\n",
      "        [-0.1912,  0.9590,  0.4944]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(12.9133, grad_fn=<DivBackward0>)\n",
      "i= 184\n",
      "w.grad= tensor([[  6.8076,   5.1567, -18.6205],\n",
      "        [ 10.7338,  14.4285, -39.5694]])\n",
      "b.grad= tensor([-0.0662, -0.1681])\n",
      "new w tensor([[-0.3404,  0.9219,  0.4989],\n",
      "        [-0.1917,  0.9582,  0.4964]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(12.7970, grad_fn=<DivBackward0>)\n",
      "i= 185\n",
      "w.grad= tensor([[  6.7098,   5.1877, -18.5277],\n",
      "        [ 10.6497,  14.3920, -39.3808]])\n",
      "b.grad= tensor([-0.0664, -0.1676])\n",
      "new w tensor([[-0.3408,  0.9216,  0.4998],\n",
      "        [-0.1922,  0.9575,  0.4984]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(12.6819, grad_fn=<DivBackward0>)\n",
      "i= 186\n",
      "w.grad= tensor([[  6.6158,   5.2182, -18.4347],\n",
      "        [ 10.5659,  14.3532, -39.1939]])\n",
      "b.grad= tensor([-0.0665, -0.1670])\n",
      "new w tensor([[-0.3411,  0.9213,  0.5008],\n",
      "        [-0.1928,  0.9568,  0.5003]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(12.5678, grad_fn=<DivBackward0>)\n",
      "i= 187\n",
      "w.grad= tensor([[  6.5246,   5.2472, -18.3421],\n",
      "        [ 10.4847,  14.3152, -39.0072]])\n",
      "b.grad= tensor([-0.0666, -0.1664])\n",
      "new w tensor([[-0.3414,  0.9211,  0.5017],\n",
      "        [-0.1933,  0.9561,  0.5023]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(12.4549, grad_fn=<DivBackward0>)\n",
      "i= 188\n",
      "w.grad= tensor([[  6.4348,   5.2735, -18.2507],\n",
      "        [ 10.4042,  14.2755, -38.8220]])\n",
      "b.grad= tensor([-0.0667, -0.1658])\n",
      "new w tensor([[-0.3418,  0.9208,  0.5026],\n",
      "        [-0.1938,  0.9554,  0.5042]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(12.3430, grad_fn=<DivBackward0>)\n",
      "i= 189\n",
      "w.grad= tensor([[  6.3479,   5.2986, -18.1595],\n",
      "        [ 10.3254,  14.2357, -38.6376]])\n",
      "b.grad= tensor([-0.0668, -0.1652])\n",
      "new w tensor([[-0.3421,  0.9206,  0.5035],\n",
      "        [-0.1943,  0.9547,  0.5062]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0173], requires_grad=True)\n",
      "Loss= tensor(12.2322, grad_fn=<DivBackward0>)\n",
      "i= 190\n",
      "w.grad= tensor([[  6.2627,   5.3214, -18.0693],\n",
      "        [ 10.2471,  14.1941, -38.4545]])\n",
      "b.grad= tensor([-0.0669, -0.1646])\n",
      "new w tensor([[-0.3424,  0.9203,  0.5044],\n",
      "        [-0.1948,  0.9539,  0.5081]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(12.1224, grad_fn=<DivBackward0>)\n",
      "i= 191\n",
      "w.grad= tensor([[  6.1807,   5.3438, -17.9791],\n",
      "        [ 10.1711,  14.1532, -38.2719]])\n",
      "b.grad= tensor([-0.0669, -0.1640])\n",
      "new w tensor([[-0.3427,  0.9200,  0.5053],\n",
      "        [-0.1953,  0.9532,  0.5100]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(12.0137, grad_fn=<DivBackward0>)\n",
      "i= 192\n",
      "w.grad= tensor([[  6.0993,   5.3630, -17.8903],\n",
      "        [ 10.0954,  14.1105, -38.0908]])\n",
      "b.grad= tensor([-0.0670, -0.1634])\n",
      "new w tensor([[-0.3430,  0.9197,  0.5062],\n",
      "        [-0.1959,  0.9525,  0.5119]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(11.9061, grad_fn=<DivBackward0>)\n",
      "i= 193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([[  6.0207,   5.3815, -17.8017],\n",
      "        [ 10.0218,  14.0682, -37.9101]])\n",
      "b.grad= tensor([-0.0670, -0.1627])\n",
      "new w tensor([[-0.3433,  0.9195,  0.5071],\n",
      "        [-0.1964,  0.9518,  0.5138]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(11.7994, grad_fn=<DivBackward0>)\n",
      "i= 194\n",
      "w.grad= tensor([[  5.9446,   5.3992, -17.7132],\n",
      "        [  9.9492,  14.0253, -37.7304]])\n",
      "b.grad= tensor([-0.0670, -0.1621])\n",
      "new w tensor([[-0.3436,  0.9192,  0.5080],\n",
      "        [-0.1968,  0.9511,  0.5157]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(11.6938, grad_fn=<DivBackward0>)\n",
      "i= 195\n",
      "w.grad= tensor([[  5.8697,   5.4147, -17.6258],\n",
      "        [  9.8778,  13.9817, -37.5516]])\n",
      "b.grad= tensor([-0.0670, -0.1615])\n",
      "new w tensor([[-0.3439,  0.9189,  0.5089],\n",
      "        [-0.1973,  0.9504,  0.5176]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(11.5891, grad_fn=<DivBackward0>)\n",
      "i= 196\n",
      "w.grad= tensor([[  5.7968,   5.4291, -17.5388],\n",
      "        [  9.8069,  13.9370, -37.3742]])\n",
      "b.grad= tensor([-0.0670, -0.1609])\n",
      "new w tensor([[-0.3442,  0.9187,  0.5097],\n",
      "        [-0.1978,  0.9497,  0.5194]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(11.4855, grad_fn=<DivBackward0>)\n",
      "i= 197\n",
      "w.grad= tensor([[  5.7256,   5.4420, -17.4525],\n",
      "        [  9.7379,  13.8928, -37.1971]])\n",
      "b.grad= tensor([-0.0670, -0.1602])\n",
      "new w tensor([[-0.3445,  0.9184,  0.5106],\n",
      "        [-0.1983,  0.9490,  0.5213]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(11.3828, grad_fn=<DivBackward0>)\n",
      "i= 198\n",
      "w.grad= tensor([[  5.6560,   5.4535, -17.3667],\n",
      "        [  9.6703,  13.8486, -37.0207]])\n",
      "b.grad= tensor([-0.0670, -0.1596])\n",
      "new w tensor([[-0.3448,  0.9181,  0.5115],\n",
      "        [-0.1988,  0.9483,  0.5231]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(11.2812, grad_fn=<DivBackward0>)\n",
      "i= 199\n",
      "w.grad= tensor([[  5.5877,   5.4633, -17.2818],\n",
      "        [  9.6017,  13.8014, -36.8465]])\n",
      "b.grad= tensor([-0.0670, -0.1590])\n",
      "new w tensor([[-0.3450,  0.9178,  0.5123],\n",
      "        [-0.1993,  0.9477,  0.5250]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(11.1804, grad_fn=<DivBackward0>)\n",
      "i= 200\n",
      "w.grad= tensor([[  5.5222,   5.4733, -17.1967],\n",
      "        [  9.5366,  13.7570, -36.6716]])\n",
      "b.grad= tensor([-0.0669, -0.1583])\n",
      "new w tensor([[-0.3453,  0.9176,  0.5132],\n",
      "        [-0.1998,  0.9470,  0.5268]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(11.0807, grad_fn=<DivBackward0>)\n",
      "i= 201\n",
      "w.grad= tensor([[  5.4578,   5.4815, -17.1123],\n",
      "        [  9.4709,  13.7101, -36.4987]])\n",
      "b.grad= tensor([-0.0668, -0.1577])\n",
      "new w tensor([[-0.3456,  0.9173,  0.5141],\n",
      "        [-0.2002,  0.9463,  0.5286]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0172], requires_grad=True)\n",
      "Loss= tensor(10.9818, grad_fn=<DivBackward0>)\n",
      "i= 202\n",
      "w.grad= tensor([[  5.3943,   5.4881, -17.0289],\n",
      "        [  9.4067,  13.6637, -36.3263]])\n",
      "b.grad= tensor([-0.0668, -0.1571])\n",
      "new w tensor([[-0.3458,  0.9170,  0.5149],\n",
      "        [-0.2007,  0.9456,  0.5305]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0171], requires_grad=True)\n",
      "Loss= tensor(10.8840, grad_fn=<DivBackward0>)\n",
      "i= 203\n",
      "w.grad= tensor([[  5.3330,   5.4942, -16.9456],\n",
      "        [  9.3429,  13.6161, -36.1550]])\n",
      "b.grad= tensor([-0.0667, -0.1564])\n",
      "new w tensor([[-0.3461,  0.9168,  0.5158],\n",
      "        [-0.2012,  0.9449,  0.5323]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0171], requires_grad=True)\n",
      "Loss= tensor(10.7870, grad_fn=<DivBackward0>)\n",
      "i= 204\n",
      "w.grad= tensor([[  5.2728,   5.4990, -16.8629],\n",
      "        [  9.2812,  13.5698, -35.9839]])\n",
      "b.grad= tensor([-0.0667, -0.1558])\n",
      "new w tensor([[-0.3464,  0.9165,  0.5166],\n",
      "        [-0.2016,  0.9442,  0.5341]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0171], requires_grad=True)\n",
      "Loss= tensor(10.6909, grad_fn=<DivBackward0>)\n",
      "i= 205\n",
      "w.grad= tensor([[  5.2143,   5.5033, -16.7805],\n",
      "        [  9.2191,  13.5215, -35.8144]])\n",
      "b.grad= tensor([-0.0666, -0.1552])\n",
      "new w tensor([[-0.3466,  0.9162,  0.5174],\n",
      "        [-0.2021,  0.9436,  0.5359]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0171], requires_grad=True)\n",
      "Loss= tensor(10.5958, grad_fn=<DivBackward0>)\n",
      "i= 206\n",
      "w.grad= tensor([[  5.1561,   5.5052, -16.6995],\n",
      "        [  9.1589,  13.4741, -35.6452]])\n",
      "b.grad= tensor([-0.0665, -0.1545])\n",
      "new w tensor([[-0.3469,  0.9159,  0.5183],\n",
      "        [-0.2026,  0.9429,  0.5376]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0171], requires_grad=True)\n",
      "Loss= tensor(10.5015, grad_fn=<DivBackward0>)\n",
      "i= 207\n",
      "w.grad= tensor([[  5.1010,   5.5083, -16.6176],\n",
      "        [  9.0985,  13.4254, -35.4775]])\n",
      "b.grad= tensor([-0.0664, -0.1539])\n",
      "new w tensor([[-0.3472,  0.9157,  0.5191],\n",
      "        [-0.2030,  0.9422,  0.5394]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0171], requires_grad=True)\n",
      "Loss= tensor(10.4081, grad_fn=<DivBackward0>)\n",
      "i= 208\n",
      "w.grad= tensor([[  5.0460,   5.5092, -16.5370],\n",
      "        [  9.0401,  13.3779, -35.3098]])\n",
      "b.grad= tensor([-0.0663, -0.1532])\n",
      "new w tensor([[-0.3474,  0.9154,  0.5199],\n",
      "        [-0.2035,  0.9416,  0.5412]], requires_grad=True)\n",
      "new b tensor([-1.1856, -1.0171], requires_grad=True)\n",
      "Loss= tensor(10.3156, grad_fn=<DivBackward0>)\n",
      "i= 209\n",
      "w.grad= tensor([[  4.9924,   5.5094, -16.4568],\n",
      "        [  8.9815,  13.3289, -35.1436]])\n",
      "b.grad= tensor([-0.0662, -0.1526])\n",
      "new w tensor([[-0.3477,  0.9151,  0.5208],\n",
      "        [-0.2039,  0.9409,  0.5429]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0171], requires_grad=True)\n",
      "Loss= tensor(10.2240, grad_fn=<DivBackward0>)\n",
      "i= 210\n",
      "w.grad= tensor([[  4.9401,   5.5090, -16.3770],\n",
      "        [  8.9245,  13.2807, -34.9776]])\n",
      "b.grad= tensor([-0.0660, -0.1519])\n",
      "new w tensor([[-0.3479,  0.9148,  0.5216],\n",
      "        [-0.2044,  0.9402,  0.5447]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0171], requires_grad=True)\n",
      "Loss= tensor(10.1333, grad_fn=<DivBackward0>)\n",
      "i= 211\n",
      "w.grad= tensor([[  4.8888,   5.5075, -16.2978],\n",
      "        [  8.8673,  13.2310, -34.8133]])\n",
      "b.grad= tensor([-0.0659, -0.1513])\n",
      "new w tensor([[-0.3481,  0.9146,  0.5224],\n",
      "        [-0.2048,  0.9396,  0.5464]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0171], requires_grad=True)\n",
      "Loss= tensor(10.0434, grad_fn=<DivBackward0>)\n",
      "i= 212\n",
      "w.grad= tensor([[  4.8390,   5.5056, -16.2187],\n",
      "        [  8.8119,  13.1825, -34.6490]])\n",
      "b.grad= tensor([-0.0658, -0.1506])\n",
      "new w tensor([[-0.3484,  0.9143,  0.5232],\n",
      "        [-0.2052,  0.9389,  0.5482]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0171], requires_grad=True)\n",
      "Loss= tensor(9.9543, grad_fn=<DivBackward0>)\n",
      "i= 213\n",
      "w.grad= tensor([[  4.7900,   5.5028, -16.1404],\n",
      "        [  8.7565,  13.1330, -34.4859]])\n",
      "b.grad= tensor([-0.0657, -0.1500])\n",
      "new w tensor([[-0.3486,  0.9140,  0.5240],\n",
      "        [-0.2057,  0.9382,  0.5499]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0171], requires_grad=True)\n",
      "Loss= tensor(9.8661, grad_fn=<DivBackward0>)\n",
      "i= 214\n",
      "w.grad= tensor([[  4.7426,   5.4999, -16.0621],\n",
      "        [  8.7020,  13.0835, -34.3236]])\n",
      "b.grad= tensor([-0.0655, -0.1493])\n",
      "new w tensor([[-0.3489,  0.9137,  0.5248],\n",
      "        [-0.2061,  0.9376,  0.5516]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0171], requires_grad=True)\n",
      "Loss= tensor(9.7787, grad_fn=<DivBackward0>)\n",
      "i= 215\n",
      "w.grad= tensor([[  4.6947,   5.4944, -15.9853],\n",
      "        [  8.6481,  13.0338, -34.1621]])\n",
      "b.grad= tensor([-0.0654, -0.1487])\n",
      "new w tensor([[-0.3491,  0.9134,  0.5256],\n",
      "        [-0.2065,  0.9369,  0.5533]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(9.6921, grad_fn=<DivBackward0>)\n",
      "i= 216\n",
      "w.grad= tensor([[  4.6496,   5.4904, -15.9078],\n",
      "        [  8.5951,  12.9843, -34.0012]])\n",
      "b.grad= tensor([-0.0652, -0.1480])\n",
      "new w tensor([[-0.3493,  0.9132,  0.5264],\n",
      "        [-0.2070,  0.9363,  0.5550]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(9.6063, grad_fn=<DivBackward0>)\n",
      "i= 217\n",
      "w.grad= tensor([[  4.6038,   5.4838, -15.8317],\n",
      "        [  8.5423,  12.9339, -33.8415]])\n",
      "b.grad= tensor([-0.0651, -0.1474])\n",
      "new w tensor([[-0.3496,  0.9129,  0.5272],\n",
      "        [-0.2074,  0.9356,  0.5567]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(9.5213, grad_fn=<DivBackward0>)\n",
      "i= 218\n",
      "w.grad= tensor([[  4.5605,   5.4783, -15.7552],\n",
      "        [  8.4906,  12.8842, -33.6822]])\n",
      "b.grad= tensor([-0.0649, -0.1467])\n",
      "new w tensor([[-0.3498,  0.9126,  0.5280],\n",
      "        [-0.2078,  0.9350,  0.5584]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(9.4371, grad_fn=<DivBackward0>)\n",
      "i= 219\n",
      "w.grad= tensor([[  4.5177,   5.4719, -15.6792],\n",
      "        [  8.4393,  12.8341, -33.5238]])\n",
      "b.grad= tensor([-0.0647, -0.1461])\n",
      "new w tensor([[-0.3500,  0.9124,  0.5288],\n",
      "        [-0.2082,  0.9344,  0.5601]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(9.3538, grad_fn=<DivBackward0>)\n",
      "i= 220\n",
      "w.grad= tensor([[  4.4755,   5.4645, -15.6040],\n",
      "        [  8.3890,  12.7843, -33.3660]])\n",
      "b.grad= tensor([-0.0646, -0.1455])\n",
      "new w tensor([[-0.3502,  0.9121,  0.5295],\n",
      "        [-0.2087,  0.9337,  0.5617]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(9.2712, grad_fn=<DivBackward0>)\n",
      "i= 221\n",
      "w.grad= tensor([[  4.4342,   5.4567, -15.5291],\n",
      "        [  8.3384,  12.7334, -33.2095]])\n",
      "b.grad= tensor([-0.0644, -0.1448])\n",
      "new w tensor([[-0.3505,  0.9118,  0.5303],\n",
      "        [-0.2091,  0.9331,  0.5634]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(9.1893, grad_fn=<DivBackward0>)\n",
      "i= 222\n",
      "w.grad= tensor([[  4.3942,   5.4489, -15.4542],\n",
      "        [  8.2894,  12.6837, -33.0532]])\n",
      "b.grad= tensor([-0.0642, -0.1442])\n",
      "new w tensor([[-0.3507,  0.9115,  0.5311],\n",
      "        [-0.2095,  0.9324,  0.5650]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(9.1083, grad_fn=<DivBackward0>)\n",
      "i= 223\n",
      "w.grad= tensor([[  4.3537,   5.4390, -15.3808],\n",
      "        [  8.2401,  12.6328, -32.8982]])\n",
      "b.grad= tensor([-0.0641, -0.1435])\n",
      "new w tensor([[-0.3509,  0.9113,  0.5319],\n",
      "        [-0.2099,  0.9318,  0.5667]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(9.0280, grad_fn=<DivBackward0>)\n",
      "i= 224\n",
      "w.grad= tensor([[  4.3154,   5.4302, -15.3068],\n",
      "        [  8.1922,  12.5829, -32.7433]])\n",
      "b.grad= tensor([-0.0639, -0.1429])\n",
      "new w tensor([[-0.3511,  0.9110,  0.5326],\n",
      "        [-0.2103,  0.9312,  0.5683]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(8.9484, grad_fn=<DivBackward0>)\n",
      "i= 225\n",
      "w.grad= tensor([[  4.2770,   5.4199, -15.2338],\n",
      "        [  8.1445,  12.5325, -32.5895]])\n",
      "b.grad= tensor([-0.0637, -0.1422])\n",
      "new w tensor([[-0.3513,  0.9107,  0.5334],\n",
      "        [-0.2107,  0.9306,  0.5699]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(8.8696, grad_fn=<DivBackward0>)\n",
      "i= 226\n",
      "w.grad= tensor([[  4.2398,   5.4100, -15.1608],\n",
      "        [  8.0970,  12.4821, -32.4365]])\n",
      "b.grad= tensor([-0.0635, -0.1416])\n",
      "new w tensor([[-0.3515,  0.9105,  0.5341],\n",
      "        [-0.2111,  0.9299,  0.5716]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(8.7915, grad_fn=<DivBackward0>)\n",
      "i= 227\n",
      "w.grad= tensor([[  4.2035,   5.3995, -15.0881],\n",
      "        [  8.0502,  12.4313, -32.2842]])\n",
      "b.grad= tensor([-0.0633, -0.1409])\n",
      "new w tensor([[-0.3518,  0.9102,  0.5349],\n",
      "        [-0.2115,  0.9293,  0.5732]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(8.7141, grad_fn=<DivBackward0>)\n",
      "i= 228\n",
      "w.grad= tensor([[  4.1680,   5.3889, -15.0158],\n",
      "        [  8.0041,  12.3812, -32.1325]])\n",
      "b.grad= tensor([-0.0631, -0.1403])\n",
      "new w tensor([[-0.3520,  0.9099,  0.5357],\n",
      "        [-0.2119,  0.9287,  0.5748]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0170], requires_grad=True)\n",
      "Loss= tensor(8.6375, grad_fn=<DivBackward0>)\n",
      "i= 229\n",
      "w.grad= tensor([[  4.1318,   5.3764, -14.9447],\n",
      "        [  7.9585,  12.3309, -31.9815]])\n",
      "b.grad= tensor([-0.0629, -0.1397])\n",
      "new w tensor([[-0.3522,  0.9096,  0.5364],\n",
      "        [-0.2123,  0.9281,  0.5764]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(8.5616, grad_fn=<DivBackward0>)\n",
      "i= 230\n",
      "w.grad= tensor([[  4.0973,   5.3644, -14.8735],\n",
      "        [  7.9126,  12.2794, -31.8319]])\n",
      "b.grad= tensor([-0.0627, -0.1390])\n",
      "new w tensor([[-0.3524,  0.9094,  0.5371],\n",
      "        [-0.2127,  0.9275,  0.5780]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(8.4864, grad_fn=<DivBackward0>)\n",
      "i= 231\n",
      "w.grad= tensor([[  4.0642,   5.3531, -14.8020],\n",
      "        [  7.8686,  12.2300, -31.6820]])\n",
      "b.grad= tensor([-0.0625, -0.1384])\n",
      "new w tensor([[-0.3526,  0.9091,  0.5379],\n",
      "        [-0.2131,  0.9269,  0.5796]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(8.4119, grad_fn=<DivBackward0>)\n",
      "i= 232\n",
      "w.grad= tensor([[  4.0302,   5.3398, -14.7319],\n",
      "        [  7.8240,  12.1795, -31.5334]])\n",
      "b.grad= tensor([-0.0623, -0.1377])\n",
      "new w tensor([[-0.3528,  0.9088,  0.5386],\n",
      "        [-0.2135,  0.9262,  0.5811]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(8.3381, grad_fn=<DivBackward0>)\n",
      "i= 233\n",
      "w.grad= tensor([[  3.9982,   5.3276, -14.6613],\n",
      "        [  7.7795,  12.1285, -31.3859]])\n",
      "b.grad= tensor([-0.0621, -0.1371])\n",
      "new w tensor([[-0.3530,  0.9086,  0.5394],\n",
      "        [-0.2139,  0.9256,  0.5827]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(8.2650, grad_fn=<DivBackward0>)\n",
      "i= 234\n",
      "w.grad= tensor([[  3.9660,   5.3143, -14.5915],\n",
      "        [  7.7366,  12.0785, -31.2383]])\n",
      "b.grad= tensor([-0.0619, -0.1365])\n",
      "new w tensor([[-0.3532,  0.9083,  0.5401],\n",
      "        [-0.2143,  0.9250,  0.5843]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(8.1926, grad_fn=<DivBackward0>)\n",
      "i= 235\n",
      "w.grad= tensor([[  3.9340,   5.3001, -14.5225],\n",
      "        [  7.6933,  12.0281, -31.0919]])\n",
      "b.grad= tensor([-0.0617, -0.1358])\n",
      "new w tensor([[-0.3534,  0.9080,  0.5408],\n",
      "        [-0.2147,  0.9244,  0.5858]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(8.1208, grad_fn=<DivBackward0>)\n",
      "i= 236\n",
      "w.grad= tensor([[  3.9034,   5.2866, -14.4531],\n",
      "        [  7.6503,  11.9772, -30.9464]])\n",
      "b.grad= tensor([-0.0614, -0.1352])\n",
      "new w tensor([[-0.3536,  0.9078,  0.5415],\n",
      "        [-0.2151,  0.9238,  0.5874]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(8.0497, grad_fn=<DivBackward0>)\n",
      "i= 237\n",
      "w.grad= tensor([[  3.8727,   5.2722, -14.3846],\n",
      "        [  7.6088,  11.9278, -30.8008]])\n",
      "b.grad= tensor([-0.0612, -0.1346])\n",
      "new w tensor([[-0.3538,  0.9075,  0.5423],\n",
      "        [-0.2154,  0.9232,  0.5889]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(7.9793, grad_fn=<DivBackward0>)\n",
      "i= 238\n",
      "w.grad= tensor([[  3.8432,   5.2582, -14.3161],\n",
      "        [  7.5666,  11.8771, -30.6567]])\n",
      "b.grad= tensor([-0.0610, -0.1340])\n",
      "new w tensor([[-0.3540,  0.9073,  0.5430],\n",
      "        [-0.2158,  0.9226,  0.5904]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(7.9095, grad_fn=<DivBackward0>)\n",
      "i= 239\n",
      "w.grad= tensor([[  3.8134,   5.2431, -14.2484],\n",
      "        [  7.5254,  11.8272, -30.5128]])\n",
      "b.grad= tensor([-0.0608, -0.1333])\n",
      "new w tensor([[-0.3541,  0.9070,  0.5437],\n",
      "        [-0.2162,  0.9221,  0.5920]], requires_grad=True)\n",
      "new b tensor([-1.1855, -1.0169], requires_grad=True)\n",
      "Loss= tensor(7.8404, grad_fn=<DivBackward0>)\n",
      "i= 240\n",
      "w.grad= tensor([[  3.7848,   5.2284, -14.1806],\n",
      "        [  7.4844,  11.7771, -30.3698]])\n",
      "b.grad= tensor([-0.0606, -0.1327])\n",
      "new w tensor([[-0.3543,  0.9067,  0.5444],\n",
      "        [-0.2166,  0.9215,  0.5935]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0169], requires_grad=True)\n",
      "Loss= tensor(7.7719, grad_fn=<DivBackward0>)\n",
      "i= 241\n",
      "w.grad= tensor([[  3.7571,   5.2142, -14.1128],\n",
      "        [  7.4436,  11.7271, -30.2276]])\n",
      "b.grad= tensor([-0.0603, -0.1321])\n",
      "new w tensor([[-0.3545,  0.9065,  0.5451],\n",
      "        [-0.2169,  0.9209,  0.5950]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0169], requires_grad=True)\n",
      "Loss= tensor(7.7041, grad_fn=<DivBackward0>)\n",
      "i= 242\n",
      "w.grad= tensor([[  3.7286,   5.1982, -14.0463],\n",
      "        [  7.4036,  11.6774, -30.0857]])\n",
      "b.grad= tensor([-0.0601, -0.1314])\n",
      "new w tensor([[-0.3547,  0.9062,  0.5458],\n",
      "        [-0.2173,  0.9203,  0.5965]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0169], requires_grad=True)\n",
      "Loss= tensor(7.6369, grad_fn=<DivBackward0>)\n",
      "i= 243\n",
      "w.grad= tensor([[  3.7010,   5.1825, -13.9798],\n",
      "        [  7.3630,  11.6265, -29.9453]])\n",
      "b.grad= tensor([-0.0599, -0.1308])\n",
      "new w tensor([[-0.3549,  0.9059,  0.5465],\n",
      "        [-0.2177,  0.9197,  0.5980]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0169], requires_grad=True)\n",
      "Loss= tensor(7.5703, grad_fn=<DivBackward0>)\n",
      "i= 244\n",
      "w.grad= tensor([[  3.6742,   5.1669, -13.9134],\n",
      "        [  7.3241,  11.5778, -29.8045]])\n",
      "b.grad= tensor([-0.0597, -0.1302])\n",
      "new w tensor([[-0.3551,  0.9057,  0.5472],\n",
      "        [-0.2180,  0.9191,  0.5995]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(7.5044, grad_fn=<DivBackward0>)\n",
      "i= 245\n",
      "w.grad= tensor([[  3.6477,   5.1510, -13.8475],\n",
      "        [  7.2846,  11.5277, -29.6651]])\n",
      "b.grad= tensor([-0.0594, -0.1296])\n",
      "new w tensor([[-0.3553,  0.9054,  0.5479],\n",
      "        [-0.2184,  0.9186,  0.6010]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(7.4391, grad_fn=<DivBackward0>)\n",
      "i= 246\n",
      "w.grad= tensor([[  3.6217,   5.1351, -13.7818],\n",
      "        [  7.2459,  11.4783, -29.5260]])\n",
      "b.grad= tensor([-0.0592, -0.1290])\n",
      "new w tensor([[-0.3554,  0.9052,  0.5486],\n",
      "        [-0.2188,  0.9180,  0.6025]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(7.3743, grad_fn=<DivBackward0>)\n",
      "i= 247\n",
      "w.grad= tensor([[  3.5953,   5.1181, -13.7170],\n",
      "        [  7.2071,  11.4285, -29.3879]])\n",
      "b.grad= tensor([-0.0590, -0.1284])\n",
      "new w tensor([[-0.3556,  0.9049,  0.5493],\n",
      "        [-0.2191,  0.9174,  0.6039]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(7.3102, grad_fn=<DivBackward0>)\n",
      "i= 248\n",
      "w.grad= tensor([[  3.5708,   5.1025, -13.6516],\n",
      "        [  7.1685,  11.3787, -29.2505]])\n",
      "b.grad= tensor([-0.0587, -0.1278])\n",
      "new w tensor([[-0.3558,  0.9047,  0.5499],\n",
      "        [-0.2195,  0.9168,  0.6054]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(7.2467, grad_fn=<DivBackward0>)\n",
      "i= 249\n",
      "w.grad= tensor([[  3.5456,   5.0856, -13.5872],\n",
      "        [  7.1315,  11.3304, -29.1130]])\n",
      "b.grad= tensor([-0.0585, -0.1271])\n",
      "new w tensor([[-0.3560,  0.9044,  0.5506],\n",
      "        [-0.2198,  0.9163,  0.6068]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(7.1838, grad_fn=<DivBackward0>)\n",
      "i= 250\n",
      "w.grad= tensor([[  3.5211,   5.0688, -13.5230],\n",
      "        [  7.0932,  11.2805, -28.9771]])\n",
      "b.grad= tensor([-0.0583, -0.1265])\n",
      "new w tensor([[-0.3561,  0.9042,  0.5513],\n",
      "        [-0.2202,  0.9157,  0.6083]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(7.1214, grad_fn=<DivBackward0>)\n",
      "i= 251\n",
      "w.grad= tensor([[  3.4968,   5.0518, -13.4591],\n",
      "        [  7.0563,  11.2317, -28.8413]])\n",
      "b.grad= tensor([-0.0580, -0.1259])\n",
      "new w tensor([[-0.3563,  0.9039,  0.5520],\n",
      "        [-0.2206,  0.9151,  0.6097]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(7.0596, grad_fn=<DivBackward0>)\n",
      "i= 252\n",
      "w.grad= tensor([[  3.4723,   5.0340, -13.3960],\n",
      "        [  7.0193,  11.1826, -28.7063]])\n",
      "b.grad= tensor([-0.0578, -0.1253])\n",
      "new w tensor([[-0.3565,  0.9037,  0.5526],\n",
      "        [-0.2209,  0.9146,  0.6112]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(6.9984, grad_fn=<DivBackward0>)\n",
      "i= 253\n",
      "w.grad= tensor([[  3.4489,   5.0168, -13.3327],\n",
      "        [  6.9830,  11.1341, -28.5717]])\n",
      "b.grad= tensor([-0.0576, -0.1247])\n",
      "new w tensor([[-0.3567,  0.9034,  0.5533],\n",
      "        [-0.2213,  0.9140,  0.6126]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(6.9378, grad_fn=<DivBackward0>)\n",
      "i= 254\n",
      "w.grad= tensor([[  3.4268,   5.0008, -13.2690],\n",
      "        [  6.9467,  11.0857, -28.4378]])\n",
      "b.grad= tensor([-0.0573, -0.1241])\n",
      "new w tensor([[-0.3568,  0.9032,  0.5540],\n",
      "        [-0.2216,  0.9135,  0.6140]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(6.8778, grad_fn=<DivBackward0>)\n",
      "i= 255\n",
      "w.grad= tensor([[  3.4036,   4.9830, -13.2066],\n",
      "        [  6.9098,  11.0361, -28.3053]])\n",
      "b.grad= tensor([-0.0571, -0.1235])\n",
      "new w tensor([[-0.3570,  0.9029,  0.5546],\n",
      "        [-0.2219,  0.9129,  0.6154]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(6.8183, grad_fn=<DivBackward0>)\n",
      "i= 256\n",
      "w.grad= tensor([[  3.3813,   4.9656, -13.1442],\n",
      "        [  6.8746,  10.9884, -28.1725]])\n",
      "b.grad= tensor([-0.0569, -0.1229])\n",
      "new w tensor([[-0.3572,  0.9027,  0.5553],\n",
      "        [-0.2223,  0.9124,  0.6168]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(6.7593, grad_fn=<DivBackward0>)\n",
      "i= 257\n",
      "w.grad= tensor([[  3.3590,   4.9478, -13.0822],\n",
      "        [  6.8390,  10.9400, -28.0407]])\n",
      "b.grad= tensor([-0.0566, -0.1223])\n",
      "new w tensor([[-0.3573,  0.9024,  0.5559],\n",
      "        [-0.2226,  0.9118,  0.6182]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(6.7009, grad_fn=<DivBackward0>)\n",
      "i= 258\n",
      "w.grad= tensor([[  3.3366,   4.9295, -13.0209],\n",
      "        [  6.8036,  10.8915, -27.9095]])\n",
      "b.grad= tensor([-0.0564, -0.1217])\n",
      "new w tensor([[-0.3575,  0.9022,  0.5566],\n",
      "        [-0.2230,  0.9113,  0.6196]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(6.6431, grad_fn=<DivBackward0>)\n",
      "i= 259\n",
      "w.grad= tensor([[  3.3154,   4.9121, -12.9592],\n",
      "        [  6.7693,  10.8442, -27.7785]])\n",
      "b.grad= tensor([-0.0562, -0.1211])\n",
      "new w tensor([[-0.3577,  0.9019,  0.5572],\n",
      "        [-0.2233,  0.9107,  0.6210]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0168], requires_grad=True)\n",
      "Loss= tensor(6.5858, grad_fn=<DivBackward0>)\n",
      "i= 260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([[  3.2940,   4.8940, -12.8983],\n",
      "        [  6.7338,  10.7954, -27.6491]])\n",
      "b.grad= tensor([-0.0559, -0.1206])\n",
      "new w tensor([[-0.3578,  0.9017,  0.5579],\n",
      "        [-0.2236,  0.9102,  0.6224]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(6.5290, grad_fn=<DivBackward0>)\n",
      "i= 261\n",
      "w.grad= tensor([[  3.2735,   4.8767, -12.8372],\n",
      "        [  6.6998,  10.7481, -27.5195]])\n",
      "b.grad= tensor([-0.0557, -0.1200])\n",
      "new w tensor([[-0.3580,  0.9014,  0.5585],\n",
      "        [-0.2240,  0.9097,  0.6238]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(6.4728, grad_fn=<DivBackward0>)\n",
      "i= 262\n",
      "w.grad= tensor([[  3.2523,   4.8582, -12.7770],\n",
      "        [  6.6654,  10.7003, -27.3909]])\n",
      "b.grad= tensor([-0.0554, -0.1194])\n",
      "new w tensor([[-0.3582,  0.9012,  0.5592],\n",
      "        [-0.2243,  0.9091,  0.6252]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(6.4170, grad_fn=<DivBackward0>)\n",
      "i= 263\n",
      "w.grad= tensor([[  3.2318,   4.8401, -12.7169],\n",
      "        [  6.6315,  10.6526, -27.2628]])\n",
      "b.grad= tensor([-0.0552, -0.1188])\n",
      "new w tensor([[-0.3583,  0.9009,  0.5598],\n",
      "        [-0.2246,  0.9086,  0.6265]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(6.3618, grad_fn=<DivBackward0>)\n",
      "i= 264\n",
      "w.grad= tensor([[  3.2121,   4.8225, -12.6566],\n",
      "        [  6.5986,  10.6064, -27.1346]])\n",
      "b.grad= tensor([-0.0550, -0.1182])\n",
      "new w tensor([[-0.3585,  0.9007,  0.5604],\n",
      "        [-0.2250,  0.9081,  0.6279]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(6.3071, grad_fn=<DivBackward0>)\n",
      "i= 265\n",
      "w.grad= tensor([[  3.1918,   4.8037, -12.5974],\n",
      "        [  6.5642,  10.5580, -27.0085]])\n",
      "b.grad= tensor([-0.0547, -0.1176])\n",
      "new w tensor([[-0.3587,  0.9005,  0.5611],\n",
      "        [-0.2253,  0.9075,  0.6292]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(6.2530, grad_fn=<DivBackward0>)\n",
      "i= 266\n",
      "w.grad= tensor([[  3.1714,   4.7847, -12.5385],\n",
      "        [  6.5311,  10.5112, -26.8820]])\n",
      "b.grad= tensor([-0.0545, -0.1171])\n",
      "new w tensor([[-0.3588,  0.9002,  0.5617],\n",
      "        [-0.2256,  0.9070,  0.6306]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(6.1993, grad_fn=<DivBackward0>)\n",
      "i= 267\n",
      "w.grad= tensor([[  3.1528,   4.7674, -12.4790],\n",
      "        [  6.4984,  10.4645, -26.7561]])\n",
      "b.grad= tensor([-0.0543, -0.1165])\n",
      "new w tensor([[-0.3590,  0.9000,  0.5623],\n",
      "        [-0.2260,  0.9065,  0.6319]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(6.1461, grad_fn=<DivBackward0>)\n",
      "i= 268\n",
      "w.grad= tensor([[  3.1338,   4.7493, -12.4201],\n",
      "        [  6.4657,  10.4177, -26.6310]])\n",
      "b.grad= tensor([-0.0540, -0.1159])\n",
      "new w tensor([[-0.3591,  0.8997,  0.5629],\n",
      "        [-0.2263,  0.9060,  0.6332]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(6.0934, grad_fn=<DivBackward0>)\n",
      "i= 269\n",
      "w.grad= tensor([[  3.1141,   4.7300, -12.3622],\n",
      "        [  6.4333,  10.3713, -26.5064]])\n",
      "b.grad= tensor([-0.0538, -0.1153])\n",
      "new w tensor([[-0.3593,  0.8995,  0.5636],\n",
      "        [-0.2266,  0.9055,  0.6346]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(6.0413, grad_fn=<DivBackward0>)\n",
      "i= 270\n",
      "w.grad= tensor([[  3.0956,   4.7120, -12.3039],\n",
      "        [  6.4005,  10.3242, -26.3828]])\n",
      "b.grad= tensor([-0.0535, -0.1148])\n",
      "new w tensor([[-0.3594,  0.8993,  0.5642],\n",
      "        [-0.2269,  0.9049,  0.6359]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(5.9895, grad_fn=<DivBackward0>)\n",
      "i= 271\n",
      "w.grad= tensor([[  3.0769,   4.6933, -12.2461],\n",
      "        [  6.3683,  10.2776, -26.2595]])\n",
      "b.grad= tensor([-0.0533, -0.1142])\n",
      "new w tensor([[-0.3596,  0.8990,  0.5648],\n",
      "        [-0.2272,  0.9044,  0.6372]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(5.9383, grad_fn=<DivBackward0>)\n",
      "i= 272\n",
      "w.grad= tensor([[  3.0587,   4.6749, -12.1885],\n",
      "        [  6.3369,  10.2320, -26.1365]])\n",
      "b.grad= tensor([-0.0531, -0.1136])\n",
      "new w tensor([[-0.3597,  0.8988,  0.5654],\n",
      "        [-0.2276,  0.9039,  0.6385]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(5.8876, grad_fn=<DivBackward0>)\n",
      "i= 273\n",
      "w.grad= tensor([[  3.0404,   4.6562, -12.1314],\n",
      "        [  6.3052,  10.1858, -26.0144]])\n",
      "b.grad= tensor([-0.0528, -0.1131])\n",
      "new w tensor([[-0.3599,  0.8986,  0.5660],\n",
      "        [-0.2279,  0.9034,  0.6398]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(5.8373, grad_fn=<DivBackward0>)\n",
      "i= 274\n",
      "w.grad= tensor([[  3.0229,   4.6381, -12.0741],\n",
      "        [  6.2741,  10.1405, -25.8925]])\n",
      "b.grad= tensor([-0.0526, -0.1125])\n",
      "new w tensor([[-0.3600,  0.8983,  0.5666],\n",
      "        [-0.2282,  0.9029,  0.6411]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(5.7875, grad_fn=<DivBackward0>)\n",
      "i= 275\n",
      "w.grad= tensor([[  3.0050,   4.6192, -12.0176],\n",
      "        [  6.2433,  10.0952, -25.7712]])\n",
      "b.grad= tensor([-0.0524, -0.1120])\n",
      "new w tensor([[-0.3602,  0.8981,  0.5672],\n",
      "        [-0.2285,  0.9024,  0.6424]], requires_grad=True)\n",
      "new b tensor([-1.1854, -1.0167], requires_grad=True)\n",
      "Loss= tensor(5.7382, grad_fn=<DivBackward0>)\n",
      "i= 276\n",
      "w.grad= tensor([[  2.9878,   4.6011, -11.9608],\n",
      "        [  6.2116,  10.0490, -25.6512]])\n",
      "b.grad= tensor([-0.0521, -0.1114])\n",
      "new w tensor([[-0.3603,  0.8979,  0.5678],\n",
      "        [-0.2288,  0.9019,  0.6437]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0167], requires_grad=True)\n",
      "Loss= tensor(5.6893, grad_fn=<DivBackward0>)\n",
      "i= 277\n",
      "w.grad= tensor([[  2.9698,   4.5818, -11.9052],\n",
      "        [  6.1809,  10.0038, -25.5312]])\n",
      "b.grad= tensor([-0.0519, -0.1109])\n",
      "new w tensor([[-0.3605,  0.8976,  0.5684],\n",
      "        [-0.2291,  0.9014,  0.6449]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.6409, grad_fn=<DivBackward0>)\n",
      "i= 278\n",
      "w.grad= tensor([[  2.9532,   4.5637, -11.8490],\n",
      "        [  6.1500,   9.9581, -25.4121]])\n",
      "b.grad= tensor([-0.0516, -0.1103])\n",
      "new w tensor([[-0.3606,  0.8974,  0.5690],\n",
      "        [-0.2294,  0.9009,  0.6462]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.5929, grad_fn=<DivBackward0>)\n",
      "i= 279\n",
      "w.grad= tensor([[  2.9371,   4.5462, -11.7927],\n",
      "        [  6.1203,   9.9139, -25.2929]])\n",
      "b.grad= tensor([-0.0514, -0.1098])\n",
      "new w tensor([[-0.3608,  0.8972,  0.5696],\n",
      "        [-0.2297,  0.9004,  0.6475]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.5454, grad_fn=<DivBackward0>)\n",
      "i= 280\n",
      "w.grad= tensor([[  2.9188,   4.5259, -11.7384],\n",
      "        [  6.0900,   9.8690, -25.1747]])\n",
      "b.grad= tensor([-0.0512, -0.1092])\n",
      "new w tensor([[-0.3609,  0.8970,  0.5702],\n",
      "        [-0.2300,  0.8999,  0.6487]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.4983, grad_fn=<DivBackward0>)\n",
      "i= 281\n",
      "w.grad= tensor([[  2.9025,   4.5076, -11.6832],\n",
      "        [  6.0593,   9.8235, -25.0576]])\n",
      "b.grad= tensor([-0.0509, -0.1087])\n",
      "new w tensor([[-0.3611,  0.8967,  0.5707],\n",
      "        [-0.2303,  0.8994,  0.6500]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.4517, grad_fn=<DivBackward0>)\n",
      "i= 282\n",
      "w.grad= tensor([[  2.8870,   4.4901, -11.6277],\n",
      "        [  6.0296,   9.7791, -24.9403]])\n",
      "b.grad= tensor([-0.0507, -0.1081])\n",
      "new w tensor([[-0.3612,  0.8965,  0.5713],\n",
      "        [-0.2306,  0.8989,  0.6512]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.4054, grad_fn=<DivBackward0>)\n",
      "i= 283\n",
      "w.grad= tensor([[  2.8699,   4.4705, -11.5738],\n",
      "        [  6.0006,   9.7353, -24.8234]])\n",
      "b.grad= tensor([-0.0505, -0.1076])\n",
      "new w tensor([[-0.3614,  0.8963,  0.5719],\n",
      "        [-0.2309,  0.8984,  0.6525]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.3597, grad_fn=<DivBackward0>)\n",
      "i= 284\n",
      "w.grad= tensor([[  2.8542,   4.4525, -11.5192],\n",
      "        [  5.9703,   9.6902, -24.7080]])\n",
      "b.grad= tensor([-0.0502, -0.1071])\n",
      "new w tensor([[-0.3615,  0.8961,  0.5725],\n",
      "        [-0.2312,  0.8979,  0.6537]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.3143, grad_fn=<DivBackward0>)\n",
      "i= 285\n",
      "w.grad= tensor([[  2.8384,   4.4340, -11.4651],\n",
      "        [  5.9422,   9.6476, -24.5917]])\n",
      "b.grad= tensor([-0.0500, -0.1065])\n",
      "new w tensor([[-0.3617,  0.8958,  0.5731],\n",
      "        [-0.2315,  0.8975,  0.6549]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.2694, grad_fn=<DivBackward0>)\n",
      "i= 286\n",
      "w.grad= tensor([[  2.8224,   4.4152, -11.4115],\n",
      "        [  5.9125,   9.6029, -24.4773]])\n",
      "b.grad= tensor([-0.0498, -0.1060])\n",
      "new w tensor([[-0.3618,  0.8956,  0.5736],\n",
      "        [-0.2318,  0.8970,  0.6562]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.2249, grad_fn=<DivBackward0>)\n",
      "i= 287\n",
      "w.grad= tensor([[  2.8069,   4.3969, -11.3579],\n",
      "        [  5.8835,   9.5591, -24.3630]])\n",
      "b.grad= tensor([-0.0495, -0.1054])\n",
      "new w tensor([[-0.3619,  0.8954,  0.5742],\n",
      "        [-0.2321,  0.8965,  0.6574]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.1808, grad_fn=<DivBackward0>)\n",
      "i= 288\n",
      "w.grad= tensor([[  2.7918,   4.3787, -11.3044],\n",
      "        [  5.8548,   9.5156, -24.2491]])\n",
      "b.grad= tensor([-0.0493, -0.1049])\n",
      "new w tensor([[-0.3621,  0.8952,  0.5748],\n",
      "        [-0.2324,  0.8960,  0.6586]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.1371, grad_fn=<DivBackward0>)\n",
      "i= 289\n",
      "w.grad= tensor([[  2.7767,   4.3606, -11.2513],\n",
      "        [  5.8271,   9.4732, -24.1352]])\n",
      "b.grad= tensor([-0.0491, -0.1044])\n",
      "new w tensor([[-0.3622,  0.8950,  0.5753],\n",
      "        [-0.2327,  0.8956,  0.6598]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.0938, grad_fn=<DivBackward0>)\n",
      "i= 290\n",
      "w.grad= tensor([[  2.7609,   4.3415, -11.1989],\n",
      "        [  5.7977,   9.4289, -24.0231]])\n",
      "b.grad= tensor([-0.0488, -0.1039])\n",
      "new w tensor([[-0.3624,  0.8948,  0.5759],\n",
      "        [-0.2330,  0.8951,  0.6610]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.0509, grad_fn=<DivBackward0>)\n",
      "i= 291\n",
      "w.grad= tensor([[  2.7460,   4.3232, -11.1464],\n",
      "        [  5.7697,   9.3862, -23.9108]])\n",
      "b.grad= tensor([-0.0486, -0.1033])\n",
      "new w tensor([[-0.3625,  0.8945,  0.5764],\n",
      "        [-0.2333,  0.8946,  0.6622]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(5.0085, grad_fn=<DivBackward0>)\n",
      "i= 292\n",
      "w.grad= tensor([[  2.7307,   4.3044, -11.0945],\n",
      "        [  5.7419,   9.3436, -23.7989]])\n",
      "b.grad= tensor([-0.0484, -0.1028])\n",
      "new w tensor([[-0.3626,  0.8943,  0.5770],\n",
      "        [-0.2336,  0.8942,  0.6634]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(4.9664, grad_fn=<DivBackward0>)\n",
      "i= 293\n",
      "w.grad= tensor([[  2.7162,   4.2862, -11.0423],\n",
      "        [  5.7136,   9.3003, -23.6880]])\n",
      "b.grad= tensor([-0.0482, -0.1023])\n",
      "new w tensor([[-0.3628,  0.8941,  0.5775],\n",
      "        [-0.2339,  0.8937,  0.6646]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(4.9247, grad_fn=<DivBackward0>)\n",
      "i= 294\n",
      "w.grad= tensor([[  2.7017,   4.2681, -10.9904],\n",
      "        [  5.6858,   9.2578, -23.5773]])\n",
      "b.grad= tensor([-0.0479, -0.1018])\n",
      "new w tensor([[-0.3629,  0.8939,  0.5781],\n",
      "        [-0.2341,  0.8932,  0.6658]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(4.8834, grad_fn=<DivBackward0>)\n",
      "i= 295\n",
      "w.grad= tensor([[  2.6870,   4.2495, -10.9391],\n",
      "        [  5.6589,   9.2160, -23.4668]])\n",
      "b.grad= tensor([-0.0477, -0.1013])\n",
      "new w tensor([[-0.3630,  0.8937,  0.5786],\n",
      "        [-0.2344,  0.8928,  0.6669]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0166], requires_grad=True)\n",
      "Loss= tensor(4.8425, grad_fn=<DivBackward0>)\n",
      "i= 296\n",
      "w.grad= tensor([[  2.6728,   4.2314, -10.8878],\n",
      "        [  5.6313,   9.1738, -23.3573]])\n",
      "b.grad= tensor([-0.0475, -0.1008])\n",
      "new w tensor([[-0.3632,  0.8935,  0.5792],\n",
      "        [-0.2347,  0.8923,  0.6681]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.8019, grad_fn=<DivBackward0>)\n",
      "i= 297\n",
      "w.grad= tensor([[  2.6588,   4.2135, -10.8366],\n",
      "        [  5.6038,   9.1315, -23.2484]])\n",
      "b.grad= tensor([-0.0472, -0.1003])\n",
      "new w tensor([[-0.3633,  0.8933,  0.5797],\n",
      "        [-0.2350,  0.8918,  0.6693]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.7618, grad_fn=<DivBackward0>)\n",
      "i= 298\n",
      "w.grad= tensor([[  2.6440,   4.1945, -10.7863],\n",
      "        [  5.5769,   9.0898, -23.1397]])\n",
      "b.grad= tensor([-0.0470, -0.0997])\n",
      "new w tensor([[-0.3634,  0.8931,  0.5803],\n",
      "        [-0.2353,  0.8914,  0.6704]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.7220, grad_fn=<DivBackward0>)\n",
      "i= 299\n",
      "w.grad= tensor([[  2.6300,   4.1764, -10.7358],\n",
      "        [  5.5503,   9.0484, -23.0314]])\n",
      "b.grad= tensor([-0.0468, -0.0992])\n",
      "new w tensor([[-0.3636,  0.8928,  0.5808],\n",
      "        [-0.2355,  0.8909,  0.6716]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.6826, grad_fn=<DivBackward0>)\n",
      "i= 300\n",
      "w.grad= tensor([[  2.6171,   4.1595, -10.6849],\n",
      "        [  5.5233,   9.0068, -22.9240]])\n",
      "b.grad= tensor([-0.0466, -0.0987])\n",
      "new w tensor([[-0.3637,  0.8926,  0.5813],\n",
      "        [-0.2358,  0.8905,  0.6727]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.6435, grad_fn=<DivBackward0>)\n",
      "i= 301\n",
      "w.grad= tensor([[  2.6032,   4.1412, -10.6349],\n",
      "        [  5.4970,   8.9658, -22.8167]])\n",
      "b.grad= tensor([-0.0463, -0.0982])\n",
      "new w tensor([[-0.3638,  0.8924,  0.5819],\n",
      "        [-0.2361,  0.8900,  0.6739]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.6048, grad_fn=<DivBackward0>)\n",
      "i= 302\n",
      "w.grad= tensor([[  2.5890,   4.1225, -10.5855],\n",
      "        [  5.4698,   8.9238, -22.7106]])\n",
      "b.grad= tensor([-0.0461, -0.0977])\n",
      "new w tensor([[-0.3640,  0.8922,  0.5824],\n",
      "        [-0.2364,  0.8896,  0.6750]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.5665, grad_fn=<DivBackward0>)\n",
      "i= 303\n",
      "w.grad= tensor([[  2.5754,   4.1045, -10.5360],\n",
      "        [  5.4438,   8.8831, -22.6043]])\n",
      "b.grad= tensor([-0.0459, -0.0972])\n",
      "new w tensor([[-0.3641,  0.8920,  0.5829],\n",
      "        [-0.2366,  0.8892,  0.6761]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.5286, grad_fn=<DivBackward0>)\n",
      "i= 304\n",
      "w.grad= tensor([[  2.5625,   4.0871, -10.4864],\n",
      "        [  5.4180,   8.8427, -22.4985]])\n",
      "b.grad= tensor([-0.0457, -0.0967])\n",
      "new w tensor([[-0.3642,  0.8918,  0.5835],\n",
      "        [-0.2369,  0.8887,  0.6772]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.4910, grad_fn=<DivBackward0>)\n",
      "i= 305\n",
      "w.grad= tensor([[  2.5489,   4.0690, -10.4374],\n",
      "        [  5.3913,   8.8012, -22.3939]])\n",
      "b.grad= tensor([-0.0455, -0.0963])\n",
      "new w tensor([[-0.3643,  0.8916,  0.5840],\n",
      "        [-0.2372,  0.8883,  0.6784]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.4537, grad_fn=<DivBackward0>)\n",
      "i= 306\n",
      "w.grad= tensor([[  2.5361,   4.0516, -10.3883],\n",
      "        [  5.3663,   8.7618, -22.2887]])\n",
      "b.grad= tensor([-0.0452, -0.0958])\n",
      "new w tensor([[-0.3645,  0.8914,  0.5845],\n",
      "        [-0.2375,  0.8878,  0.6795]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.4168, grad_fn=<DivBackward0>)\n",
      "i= 307\n",
      "w.grad= tensor([[  2.5227,   4.0336, -10.3398],\n",
      "        [  5.3403,   8.7213, -22.1847]])\n",
      "b.grad= tensor([-0.0450, -0.0953])\n",
      "new w tensor([[-0.3646,  0.8912,  0.5850],\n",
      "        [-0.2377,  0.8874,  0.6806]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.3802, grad_fn=<DivBackward0>)\n",
      "i= 308\n",
      "w.grad= tensor([[  2.5091,   4.0150, -10.2919],\n",
      "        [  5.3141,   8.6803, -22.0816]])\n",
      "b.grad= tensor([-0.0448, -0.0948])\n",
      "new w tensor([[-0.3647,  0.8910,  0.5855],\n",
      "        [-0.2380,  0.8870,  0.6817]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.3440, grad_fn=<DivBackward0>)\n",
      "i= 309\n",
      "w.grad= tensor([[  2.4967,   3.9979, -10.2434],\n",
      "        [  5.2891,   8.6410, -21.9781]])\n",
      "b.grad= tensor([-0.0446, -0.0943])\n",
      "new w tensor([[-0.3648,  0.8908,  0.5860],\n",
      "        [-0.2382,  0.8865,  0.6828]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.3081, grad_fn=<DivBackward0>)\n",
      "i= 310\n",
      "w.grad= tensor([[  2.4838,   3.9802, -10.1954],\n",
      "        [  5.2641,   8.6015, -21.8753]])\n",
      "b.grad= tensor([-0.0444, -0.0938])\n",
      "new w tensor([[-0.3650,  0.8906,  0.5865],\n",
      "        [-0.2385,  0.8861,  0.6839]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.2725, grad_fn=<DivBackward0>)\n",
      "i= 311\n",
      "w.grad= tensor([[  2.4712,   3.9628, -10.1477],\n",
      "        [  5.2385,   8.5614, -21.7734]])\n",
      "b.grad= tensor([-0.0442, -0.0933])\n",
      "new w tensor([[-0.3651,  0.8904,  0.5871],\n",
      "        [-0.2388,  0.8857,  0.6850]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.2373, grad_fn=<DivBackward0>)\n",
      "i= 312\n",
      "w.grad= tensor([[  2.4584,   3.9450, -10.1003],\n",
      "        [  5.2137,   8.5223, -21.6716]])\n",
      "b.grad= tensor([-0.0439, -0.0929])\n",
      "new w tensor([[-0.3652,  0.8902,  0.5876],\n",
      "        [-0.2390,  0.8852,  0.6861]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.2024, grad_fn=<DivBackward0>)\n",
      "i= 313\n",
      "w.grad= tensor([[  2.4462,   3.9279, -10.0529],\n",
      "        [  5.1888,   8.4830, -21.5704]])\n",
      "b.grad= tensor([-0.0437, -0.0924])\n",
      "new w tensor([[-0.3653,  0.8900,  0.5881],\n",
      "        [-0.2393,  0.8848,  0.6871]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.1678, grad_fn=<DivBackward0>)\n",
      "i= 314\n",
      "w.grad= tensor([[  2.4337,   3.9104, -10.0059],\n",
      "        [  5.1637,   8.4434, -21.4699]])\n",
      "b.grad= tensor([-0.0435, -0.0919])\n",
      "new w tensor([[-0.3655,  0.8898,  0.5886],\n",
      "        [-0.2396,  0.8844,  0.6882]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.1336, grad_fn=<DivBackward0>)\n",
      "i= 315\n",
      "w.grad= tensor([[  2.4214,   3.8929,  -9.9591],\n",
      "        [  5.1394,   8.4047, -21.3694]])\n",
      "b.grad= tensor([-0.0433, -0.0914])\n",
      "new w tensor([[-0.3656,  0.8896,  0.5891],\n",
      "        [-0.2398,  0.8840,  0.6893]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.0996, grad_fn=<DivBackward0>)\n",
      "i= 316\n",
      "w.grad= tensor([[  2.4086,   3.8751,  -9.9129],\n",
      "        [  5.1150,   8.3662, -21.2695]])\n",
      "b.grad= tensor([-0.0431, -0.0910])\n",
      "new w tensor([[-0.3657,  0.8894,  0.5896],\n",
      "        [-0.2401,  0.8836,  0.6903]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0165], requires_grad=True)\n",
      "Loss= tensor(4.0660, grad_fn=<DivBackward0>)\n",
      "i= 317\n",
      "w.grad= tensor([[  2.3971,   3.8586,  -9.8660],\n",
      "        [  5.0908,   8.3278, -21.1701]])\n",
      "b.grad= tensor([-0.0429, -0.0905])\n",
      "new w tensor([[-0.3658,  0.8892,  0.5901],\n",
      "        [-0.2403,  0.8831,  0.6914]], requires_grad=True)\n",
      "new b tensor([-1.1853, -1.0164], requires_grad=True)\n",
      "Loss= tensor(4.0327, grad_fn=<DivBackward0>)\n",
      "i= 318\n",
      "w.grad= tensor([[  2.3843,   3.8406,  -9.8203],\n",
      "        [  5.0657,   8.2882, -21.0719]])\n",
      "b.grad= tensor([-0.0427, -0.0901])\n",
      "new w tensor([[-0.3659,  0.8890,  0.5905],\n",
      "        [-0.2406,  0.8827,  0.6925]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.9997, grad_fn=<DivBackward0>)\n",
      "i= 319\n",
      "w.grad= tensor([[  2.3725,   3.8236,  -9.7743],\n",
      "        [  5.0418,   8.2502, -20.9732]])\n",
      "b.grad= tensor([-0.0425, -0.0896])\n",
      "new w tensor([[-0.3661,  0.8889,  0.5910],\n",
      "        [-0.2408,  0.8823,  0.6935]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.9670, grad_fn=<DivBackward0>)\n",
      "i= 320\n",
      "w.grad= tensor([[  2.3609,   3.8068,  -9.7284],\n",
      "        [  5.0185,   8.2129, -20.8749]])\n",
      "b.grad= tensor([-0.0422, -0.0891])\n",
      "new w tensor([[-0.3662,  0.8887,  0.5915],\n",
      "        [-0.2411,  0.8819,  0.6945]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.9346, grad_fn=<DivBackward0>)\n",
      "i= 321\n",
      "w.grad= tensor([[  2.3492,   3.7899,  -9.6828],\n",
      "        [  4.9942,   8.1745, -20.7777]])\n",
      "b.grad= tensor([-0.0420, -0.0887])\n",
      "new w tensor([[-0.3663,  0.8885,  0.5920],\n",
      "        [-0.2413,  0.8815,  0.6956]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.9026, grad_fn=<DivBackward0>)\n",
      "i= 322\n",
      "w.grad= tensor([[  2.3375,   3.7730,  -9.6375],\n",
      "        [  4.9709,   8.1371, -20.6804]])\n",
      "b.grad= tensor([-0.0418, -0.0882])\n",
      "new w tensor([[-0.3664,  0.8883,  0.5925],\n",
      "        [-0.2416,  0.8811,  0.6966]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.8708, grad_fn=<DivBackward0>)\n",
      "i= 323\n",
      "w.grad= tensor([[  2.3256,   3.7557,  -9.5926],\n",
      "        [  4.9468,   8.0990, -20.5842]])\n",
      "b.grad= tensor([-0.0416, -0.0878])\n",
      "new w tensor([[-0.3665,  0.8881,  0.5930],\n",
      "        [-0.2418,  0.8807,  0.6977]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.8393, grad_fn=<DivBackward0>)\n",
      "i= 324\n",
      "w.grad= tensor([[  2.3134,   3.7381,  -9.5482],\n",
      "        [  4.9233,   8.0616, -20.4881]])\n",
      "b.grad= tensor([-0.0414, -0.0873])\n",
      "new w tensor([[-0.3666,  0.8879,  0.5934],\n",
      "        [-0.2421,  0.8803,  0.6987]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.8081, grad_fn=<DivBackward0>)\n",
      "i= 325\n",
      "w.grad= tensor([[  2.3028,   3.7223,  -9.5029],\n",
      "        [  4.8996,   8.0238, -20.3926]])\n",
      "b.grad= tensor([-0.0412, -0.0869])\n",
      "new w tensor([[-0.3668,  0.8877,  0.5939],\n",
      "        [-0.2423,  0.8799,  0.6997]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.7772, grad_fn=<DivBackward0>)\n",
      "i= 326\n",
      "w.grad= tensor([[  2.2910,   3.7051,  -9.4588],\n",
      "        [  4.8771,   7.9874, -20.2970]])\n",
      "b.grad= tensor([-0.0410, -0.0864])\n",
      "new w tensor([[-0.3669,  0.8875,  0.5944],\n",
      "        [-0.2426,  0.8795,  0.7007]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.7466, grad_fn=<DivBackward0>)\n",
      "i= 327\n",
      "w.grad= tensor([[  2.2797,   3.6885,  -9.4145],\n",
      "        [  4.8542,   7.9509, -20.2020]])\n",
      "b.grad= tensor([-0.0408, -0.0859])\n",
      "new w tensor([[-0.3670,  0.8874,  0.5949],\n",
      "        [-0.2428,  0.8791,  0.7017]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.7163, grad_fn=<DivBackward0>)\n",
      "i= 328\n",
      "w.grad= tensor([[  2.2684,   3.6719,  -9.3704],\n",
      "        [  4.8303,   7.9130, -20.1082]])\n",
      "b.grad= tensor([-0.0406, -0.0855])\n",
      "new w tensor([[-0.3671,  0.8872,  0.5953],\n",
      "        [-0.2430,  0.8787,  0.7027]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.6862, grad_fn=<DivBackward0>)\n",
      "i= 329\n",
      "w.grad= tensor([[  2.2572,   3.6554,  -9.3266],\n",
      "        [  4.8077,   7.8767, -20.0141]])\n",
      "b.grad= tensor([-0.0404, -0.0851])\n",
      "new w tensor([[-0.3672,  0.8870,  0.5958],\n",
      "        [-0.2433,  0.8783,  0.7037]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.6564, grad_fn=<DivBackward0>)\n",
      "i= 330\n",
      "w.grad= tensor([[  2.2457,   3.6385,  -9.2832],\n",
      "        [  4.7854,   7.8407, -19.9205]])\n",
      "b.grad= tensor([-0.0402, -0.0846])\n",
      "new w tensor([[-0.3673,  0.8868,  0.5963],\n",
      "        [-0.2435,  0.8779,  0.7047]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.6270, grad_fn=<DivBackward0>)\n",
      "i= 331\n",
      "w.grad= tensor([[  2.2348,   3.6221,  -9.2398],\n",
      "        [  4.7626,   7.8042, -19.8275]])\n",
      "b.grad= tensor([-0.0400, -0.0842])\n",
      "new w tensor([[-0.3674,  0.8866,  0.5967],\n",
      "        [-0.2438,  0.8775,  0.7057]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.5977, grad_fn=<DivBackward0>)\n",
      "i= 332\n",
      "w.grad= tensor([[  2.2243,   3.6064,  -9.1962],\n",
      "        [  4.7396,   7.7675, -19.7353]])\n",
      "b.grad= tensor([-0.0398, -0.0838])\n",
      "new w tensor([[-0.3675,  0.8864,  0.5972],\n",
      "        [-0.2440,  0.8771,  0.7067]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss= tensor(3.5688, grad_fn=<DivBackward0>)\n",
      "i= 333\n",
      "w.grad= tensor([[  2.2130,   3.5897,  -9.1535],\n",
      "        [  4.7172,   7.7314, -19.6432]])\n",
      "b.grad= tensor([-0.0396, -0.0833])\n",
      "new w tensor([[-0.3677,  0.8863,  0.5976],\n",
      "        [-0.2442,  0.8767,  0.7077]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.5401, grad_fn=<DivBackward0>)\n",
      "i= 334\n",
      "w.grad= tensor([[  2.2017,   3.5729,  -9.1110],\n",
      "        [  4.6952,   7.6961, -19.5512]])\n",
      "b.grad= tensor([-0.0394, -0.0829])\n",
      "new w tensor([[-0.3678,  0.8861,  0.5981],\n",
      "        [-0.2445,  0.8764,  0.7087]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.5117, grad_fn=<DivBackward0>)\n",
      "i= 335\n",
      "w.grad= tensor([[  2.1913,   3.5570,  -9.0682],\n",
      "        [  4.6730,   7.6603, -19.4600]])\n",
      "b.grad= tensor([-0.0392, -0.0825])\n",
      "new w tensor([[-0.3679,  0.8859,  0.5985],\n",
      "        [-0.2447,  0.8760,  0.7096]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.4836, grad_fn=<DivBackward0>)\n",
      "i= 336\n",
      "w.grad= tensor([[  2.1805,   3.5409,  -9.0258],\n",
      "        [  4.6509,   7.6248, -19.3691]])\n",
      "b.grad= tensor([-0.0390, -0.0820])\n",
      "new w tensor([[-0.3680,  0.8857,  0.5990],\n",
      "        [-0.2449,  0.8756,  0.7106]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.4557, grad_fn=<DivBackward0>)\n",
      "i= 337\n",
      "w.grad= tensor([[  2.1699,   3.5248,  -8.9836],\n",
      "        [  4.6291,   7.5897, -19.2786]])\n",
      "b.grad= tensor([-0.0388, -0.0816])\n",
      "new w tensor([[-0.3681,  0.8856,  0.5994],\n",
      "        [-0.2452,  0.8752,  0.7116]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.4281, grad_fn=<DivBackward0>)\n",
      "i= 338\n",
      "w.grad= tensor([[  2.1592,   3.5085,  -8.9418],\n",
      "        [  4.6071,   7.5544, -19.1886]])\n",
      "b.grad= tensor([-0.0386, -0.0812])\n",
      "new w tensor([[-0.3682,  0.8854,  0.5999],\n",
      "        [-0.2454,  0.8748,  0.7125]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.4007, grad_fn=<DivBackward0>)\n",
      "i= 339\n",
      "w.grad= tensor([[  2.1490,   3.4929,  -8.8998],\n",
      "        [  4.5850,   7.5189, -19.0992]])\n",
      "b.grad= tensor([-0.0384, -0.0808])\n",
      "new w tensor([[-0.3683,  0.8852,  0.6003],\n",
      "        [-0.2456,  0.8745,  0.7135]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0164], requires_grad=True)\n",
      "Loss= tensor(3.3736, grad_fn=<DivBackward0>)\n",
      "i= 340\n",
      "w.grad= tensor([[  2.1389,   3.4775,  -8.8579],\n",
      "        [  4.5641,   7.4847, -19.0096]])\n",
      "b.grad= tensor([-0.0382, -0.0803])\n",
      "new w tensor([[-0.3684,  0.8850,  0.6008],\n",
      "        [-0.2459,  0.8741,  0.7144]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.3467, grad_fn=<DivBackward0>)\n",
      "i= 341\n",
      "w.grad= tensor([[  2.1279,   3.4609,  -8.8170],\n",
      "        [  4.5418,   7.4493, -18.9214]])\n",
      "b.grad= tensor([-0.0380, -0.0799])\n",
      "new w tensor([[-0.3685,  0.8849,  0.6012],\n",
      "        [-0.2461,  0.8737,  0.7154]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.3202, grad_fn=<DivBackward0>)\n",
      "i= 342\n",
      "w.grad= tensor([[  2.1175,   3.4450,  -8.7758],\n",
      "        [  4.5206,   7.4149, -18.8329]])\n",
      "b.grad= tensor([-0.0378, -0.0795])\n",
      "new w tensor([[-0.3686,  0.8847,  0.6017],\n",
      "        [-0.2463,  0.8733,  0.7163]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.2938, grad_fn=<DivBackward0>)\n",
      "i= 343\n",
      "w.grad= tensor([[  2.1074,   3.4295,  -8.7347],\n",
      "        [  4.4992,   7.3804, -18.7451]])\n",
      "b.grad= tensor([-0.0376, -0.0791])\n",
      "new w tensor([[-0.3687,  0.8845,  0.6021],\n",
      "        [-0.2465,  0.8730,  0.7173]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.2677, grad_fn=<DivBackward0>)\n",
      "i= 344\n",
      "w.grad= tensor([[  2.0972,   3.4139,  -8.6938],\n",
      "        [  4.4787,   7.3469, -18.6571]])\n",
      "b.grad= tensor([-0.0375, -0.0787])\n",
      "new w tensor([[-0.3688,  0.8843,  0.6025],\n",
      "        [-0.2468,  0.8726,  0.7182]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.2418, grad_fn=<DivBackward0>)\n",
      "i= 345\n",
      "w.grad= tensor([[  2.0865,   3.3979,  -8.6535],\n",
      "        [  4.4568,   7.3119, -18.5705]])\n",
      "b.grad= tensor([-0.0373, -0.0783])\n",
      "new w tensor([[-0.3689,  0.8842,  0.6030],\n",
      "        [-0.2470,  0.8722,  0.7191]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.2162, grad_fn=<DivBackward0>)\n",
      "i= 346\n",
      "w.grad= tensor([[  2.0764,   3.3822,  -8.6131],\n",
      "        [  4.4361,   7.2782, -18.4836]])\n",
      "b.grad= tensor([-0.0371, -0.0778])\n",
      "new w tensor([[-0.3690,  0.8840,  0.6034],\n",
      "        [-0.2472,  0.8719,  0.7200]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.1908, grad_fn=<DivBackward0>)\n",
      "i= 347\n",
      "w.grad= tensor([[  2.0667,   3.3671,  -8.5726],\n",
      "        [  4.4154,   7.2446, -18.3972]])\n",
      "b.grad= tensor([-0.0369, -0.0774])\n",
      "new w tensor([[-0.3691,  0.8838,  0.6038],\n",
      "        [-0.2474,  0.8715,  0.7210]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.1656, grad_fn=<DivBackward0>)\n",
      "i= 348\n",
      "w.grad= tensor([[  2.0567,   3.3519,  -8.5325],\n",
      "        [  4.3945,   7.2110, -18.3113]])\n",
      "b.grad= tensor([-0.0367, -0.0770])\n",
      "new w tensor([[-0.3692,  0.8837,  0.6043],\n",
      "        [-0.2476,  0.8711,  0.7219]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.1407, grad_fn=<DivBackward0>)\n",
      "i= 349\n",
      "w.grad= tensor([[  2.0463,   3.3358,  -8.4931],\n",
      "        [  4.3738,   7.1775, -18.2259]])\n",
      "b.grad= tensor([-0.0365, -0.0766])\n",
      "new w tensor([[-0.3694,  0.8835,  0.6047],\n",
      "        [-0.2479,  0.8708,  0.7228]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.1160, grad_fn=<DivBackward0>)\n",
      "i= 350\n",
      "w.grad= tensor([[  2.0372,   3.3215,  -8.4529],\n",
      "        [  4.3530,   7.1438, -18.1410]])\n",
      "b.grad= tensor([-0.0363, -0.0762])\n",
      "new w tensor([[-0.3695,  0.8833,  0.6051],\n",
      "        [-0.2481,  0.8704,  0.7237]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.0916, grad_fn=<DivBackward0>)\n",
      "i= 351\n",
      "w.grad= tensor([[  2.0267,   3.3053,  -8.4139],\n",
      "        [  4.3324,   7.1104, -18.0564]])\n",
      "b.grad= tensor([-0.0362, -0.0758])\n",
      "new w tensor([[-0.3696,  0.8832,  0.6055],\n",
      "        [-0.2483,  0.8701,  0.7246]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.0673, grad_fn=<DivBackward0>)\n",
      "i= 352\n",
      "w.grad= tensor([[  2.0176,   3.2910,  -8.3741],\n",
      "        [  4.3126,   7.0781, -17.9716]])\n",
      "b.grad= tensor([-0.0360, -0.0754])\n",
      "new w tensor([[-0.3697,  0.8830,  0.6059],\n",
      "        [-0.2485,  0.8697,  0.7255]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.0433, grad_fn=<DivBackward0>)\n",
      "i= 353\n",
      "w.grad= tensor([[  2.0077,   3.2757,  -8.3351],\n",
      "        [  4.2917,   7.0445, -17.8883]])\n",
      "b.grad= tensor([-0.0358, -0.0750])\n",
      "new w tensor([[-0.3698,  0.8828,  0.6064],\n",
      "        [-0.2487,  0.8694,  0.7264]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(3.0196, grad_fn=<DivBackward0>)\n",
      "i= 354\n",
      "w.grad= tensor([[  1.9975,   3.2600,  -8.2965],\n",
      "        [  4.2723,   7.0126, -17.8042]])\n",
      "b.grad= tensor([-0.0356, -0.0746])\n",
      "new w tensor([[-0.3699,  0.8827,  0.6068],\n",
      "        [-0.2489,  0.8690,  0.7273]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.9960, grad_fn=<DivBackward0>)\n",
      "i= 355\n",
      "w.grad= tensor([[  1.9883,   3.2453,  -8.2576],\n",
      "        [  4.2514,   6.9791, -17.7216]])\n",
      "b.grad= tensor([-0.0354, -0.0742])\n",
      "new w tensor([[-0.3700,  0.8825,  0.6072],\n",
      "        [-0.2491,  0.8687,  0.7282]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.9727, grad_fn=<DivBackward0>)\n",
      "i= 356\n",
      "w.grad= tensor([[  1.9790,   3.2308,  -8.2188],\n",
      "        [  4.2320,   6.9472, -17.6384]])\n",
      "b.grad= tensor([-0.0352, -0.0738])\n",
      "new w tensor([[-0.3701,  0.8824,  0.6076],\n",
      "        [-0.2494,  0.8683,  0.7290]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.9495, grad_fn=<DivBackward0>)\n",
      "i= 357\n",
      "w.grad= tensor([[  1.9697,   3.2162,  -8.1803],\n",
      "        [  4.2119,   6.9149, -17.5562]])\n",
      "b.grad= tensor([-0.0351, -0.0735])\n",
      "new w tensor([[-0.3702,  0.8822,  0.6080],\n",
      "        [-0.2496,  0.8680,  0.7299]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.9266, grad_fn=<DivBackward0>)\n",
      "i= 358\n",
      "w.grad= tensor([[  1.9595,   3.2006,  -8.1425],\n",
      "        [  4.1917,   6.8822, -17.4745]])\n",
      "b.grad= tensor([-0.0349, -0.0731])\n",
      "new w tensor([[-0.3703,  0.8820,  0.6084],\n",
      "        [-0.2498,  0.8676,  0.7308]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.9039, grad_fn=<DivBackward0>)\n",
      "i= 359\n",
      "w.grad= tensor([[  1.9501,   3.1858,  -8.1046],\n",
      "        [  4.1725,   6.8507, -17.3927]])\n",
      "b.grad= tensor([-0.0347, -0.0727])\n",
      "new w tensor([[-0.3703,  0.8819,  0.6088],\n",
      "        [-0.2500,  0.8673,  0.7317]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.8815, grad_fn=<DivBackward0>)\n",
      "i= 360\n",
      "w.grad= tensor([[  1.9414,   3.1718,  -8.0663],\n",
      "        [  4.1526,   6.8184, -17.3117]])\n",
      "b.grad= tensor([-0.0345, -0.0723])\n",
      "new w tensor([[-0.3704,  0.8817,  0.6092],\n",
      "        [-0.2502,  0.8669,  0.7325]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.8592, grad_fn=<DivBackward0>)\n",
      "i= 361\n",
      "w.grad= tensor([[  1.9322,   3.1573,  -8.0286],\n",
      "        [  4.1332,   6.7869, -17.2309]])\n",
      "b.grad= tensor([-0.0343, -0.0719])\n",
      "new w tensor([[-0.3705,  0.8816,  0.6096],\n",
      "        [-0.2504,  0.8666,  0.7334]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.8371, grad_fn=<DivBackward0>)\n",
      "i= 362\n",
      "w.grad= tensor([[  1.9221,   3.1418,  -7.9917],\n",
      "        [  4.1137,   6.7552, -17.1505]])\n",
      "b.grad= tensor([-0.0342, -0.0715])\n",
      "new w tensor([[-0.3706,  0.8814,  0.6100],\n",
      "        [-0.2506,  0.8663,  0.7343]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.8153, grad_fn=<DivBackward0>)\n",
      "i= 363\n",
      "w.grad= tensor([[  1.9130,   3.1273,  -7.9543],\n",
      "        [  4.0946,   6.7240, -17.0703]])\n",
      "b.grad= tensor([-0.0340, -0.0712])\n",
      "new w tensor([[-0.3707,  0.8812,  0.6104],\n",
      "        [-0.2508,  0.8659,  0.7351]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.7936, grad_fn=<DivBackward0>)\n",
      "i= 364\n",
      "w.grad= tensor([[  1.9048,   3.1139,  -7.9165],\n",
      "        [  4.0749,   6.6920, -16.9909]])\n",
      "b.grad= tensor([-0.0338, -0.0708])\n",
      "new w tensor([[-0.3708,  0.8811,  0.6108],\n",
      "        [-0.2510,  0.8656,  0.7360]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.7721, grad_fn=<DivBackward0>)\n",
      "i= 365\n",
      "w.grad= tensor([[  1.8955,   3.0994,  -7.8796],\n",
      "        [  4.0560,   6.6612, -16.9115]])\n",
      "b.grad= tensor([-0.0336, -0.0704])\n",
      "new w tensor([[-0.3709,  0.8809,  0.6112],\n",
      "        [-0.2512,  0.8653,  0.7368]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.7509, grad_fn=<DivBackward0>)\n",
      "i= 366\n",
      "w.grad= tensor([[  1.8863,   3.0850,  -7.8430],\n",
      "        [  4.0369,   6.6301, -16.8326]])\n",
      "b.grad= tensor([-0.0335, -0.0700])\n",
      "new w tensor([[-0.3710,  0.8808,  0.6116],\n",
      "        [-0.2514,  0.8649,  0.7376]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0163], requires_grad=True)\n",
      "Loss= tensor(2.7298, grad_fn=<DivBackward0>)\n",
      "i= 367\n",
      "w.grad= tensor([[  1.8769,   3.0703,  -7.8066],\n",
      "        [  4.0182,   6.5995, -16.7538]])\n",
      "b.grad= tensor([-0.0333, -0.0697])\n",
      "new w tensor([[-0.3711,  0.8806,  0.6120],\n",
      "        [-0.2516,  0.8646,  0.7385]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.7090, grad_fn=<DivBackward0>)\n",
      "i= 368\n",
      "w.grad= tensor([[  1.8678,   3.0559,  -7.7703],\n",
      "        [  3.9993,   6.5687, -16.6757]])\n",
      "b.grad= tensor([-0.0331, -0.0693])\n",
      "new w tensor([[-0.3712,  0.8805,  0.6124],\n",
      "        [-0.2518,  0.8643,  0.7393]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.6883, grad_fn=<DivBackward0>)\n",
      "i= 369\n",
      "w.grad= tensor([[  1.8590,   3.0419,  -7.7339],\n",
      "        [  3.9809,   6.5385, -16.5975]])\n",
      "b.grad= tensor([-0.0330, -0.0689])\n",
      "new w tensor([[-0.3713,  0.8803,  0.6128],\n",
      "        [-0.2520,  0.8640,  0.7401]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.6678, grad_fn=<DivBackward0>)\n",
      "i= 370\n",
      "w.grad= tensor([[  1.8507,   3.0284,  -7.6975],\n",
      "        [  3.9619,   6.5077, -16.5204]])\n",
      "b.grad= tensor([-0.0328, -0.0686])\n",
      "new w tensor([[-0.3714,  0.8802,  0.6131],\n",
      "        [-0.2522,  0.8636,  0.7410]], requires_grad=True)\n",
      "new b tensor([-1.1852, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.6476, grad_fn=<DivBackward0>)\n",
      "i= 371\n",
      "w.grad= tensor([[  1.8417,   3.0143,  -7.6616],\n",
      "        [  3.9434,   6.4776, -16.4431]])\n",
      "b.grad= tensor([-0.0326, -0.0682])\n",
      "new w tensor([[-0.3715,  0.8800,  0.6135],\n",
      "        [-0.2524,  0.8633,  0.7418]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.6275, grad_fn=<DivBackward0>)\n",
      "i= 372\n",
      "w.grad= tensor([[  1.8327,   3.0000,  -7.6260],\n",
      "        [  3.9247,   6.4473, -16.3665]])\n",
      "b.grad= tensor([-0.0325, -0.0678])\n",
      "new w tensor([[-0.3716,  0.8799,  0.6139],\n",
      "        [-0.2526,  0.8630,  0.7426]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.6076, grad_fn=<DivBackward0>)\n",
      "i= 373\n",
      "w.grad= tensor([[  1.8236,   2.9856,  -7.5907],\n",
      "        [  3.9065,   6.4175, -16.2900]])\n",
      "b.grad= tensor([-0.0323, -0.0675])\n",
      "new w tensor([[-0.3717,  0.8797,  0.6143],\n",
      "        [-0.2528,  0.8627,  0.7434]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.5878, grad_fn=<DivBackward0>)\n",
      "i= 374\n",
      "w.grad= tensor([[  1.8164,   2.9734,  -7.5543],\n",
      "        [  3.8882,   6.3876, -16.2139]])\n",
      "b.grad= tensor([-0.0321, -0.0671])\n",
      "new w tensor([[-0.3718,  0.8796,  0.6147],\n",
      "        [-0.2530,  0.8623,  0.7442]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.5683, grad_fn=<DivBackward0>)\n",
      "i= 375\n",
      "w.grad= tensor([[  1.8076,   2.9596,  -7.5191],\n",
      "        [  3.8703,   6.3584, -16.1380]])\n",
      "b.grad= tensor([-0.0319, -0.0667])\n",
      "new w tensor([[-0.3718,  0.8794,  0.6150],\n",
      "        [-0.2532,  0.8620,  0.7450]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.5489, grad_fn=<DivBackward0>)\n",
      "i= 376\n",
      "w.grad= tensor([[  1.7979,   2.9447,  -7.4848],\n",
      "        [  3.8516,   6.3279, -16.0631]])\n",
      "b.grad= tensor([-0.0318, -0.0664])\n",
      "new w tensor([[-0.3719,  0.8793,  0.6154],\n",
      "        [-0.2534,  0.8617,  0.7458]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.5298, grad_fn=<DivBackward0>)\n",
      "i= 377\n",
      "w.grad= tensor([[  1.7900,   2.9317,  -7.4494],\n",
      "        [  3.8340,   6.2989, -15.9878]])\n",
      "b.grad= tensor([-0.0316, -0.0660])\n",
      "new w tensor([[-0.3720,  0.8791,  0.6158],\n",
      "        [-0.2536,  0.8614,  0.7466]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.5108, grad_fn=<DivBackward0>)\n",
      "i= 378\n",
      "w.grad= tensor([[  1.7814,   2.9180,  -7.4147],\n",
      "        [  3.8159,   6.2694, -15.9133]])\n",
      "b.grad= tensor([-0.0315, -0.0657])\n",
      "new w tensor([[-0.3721,  0.8790,  0.6162],\n",
      "        [-0.2538,  0.8611,  0.7474]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.4919, grad_fn=<DivBackward0>)\n",
      "i= 379\n",
      "w.grad= tensor([[  1.7730,   2.9046,  -7.3800],\n",
      "        [  3.7980,   6.2403, -15.8390]])\n",
      "b.grad= tensor([-0.0313, -0.0653])\n",
      "new w tensor([[-0.3722,  0.8788,  0.6165],\n",
      "        [-0.2540,  0.8608,  0.7482]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.4733, grad_fn=<DivBackward0>)\n",
      "i= 380\n",
      "w.grad= tensor([[  1.7644,   2.8910,  -7.3457],\n",
      "        [  3.7803,   6.2113, -15.7650]])\n",
      "b.grad= tensor([-0.0311, -0.0650])\n",
      "new w tensor([[-0.3723,  0.8787,  0.6169],\n",
      "        [-0.2542,  0.8605,  0.7490]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.4548, grad_fn=<DivBackward0>)\n",
      "i= 381\n",
      "w.grad= tensor([[  1.7560,   2.8773,  -7.3115],\n",
      "        [  3.7628,   6.1827, -15.6912]])\n",
      "b.grad= tensor([-0.0310, -0.0646])\n",
      "new w tensor([[-0.3724,  0.8786,  0.6173],\n",
      "        [-0.2543,  0.8601,  0.7498]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.4365, grad_fn=<DivBackward0>)\n",
      "i= 382\n",
      "w.grad= tensor([[  1.7478,   2.8641,  -7.2773],\n",
      "        [  3.7450,   6.1536, -15.6181]])\n",
      "b.grad= tensor([-0.0308, -0.0643])\n",
      "new w tensor([[-0.3725,  0.8784,  0.6176],\n",
      "        [-0.2545,  0.8598,  0.7506]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.4184, grad_fn=<DivBackward0>)\n",
      "i= 383\n",
      "w.grad= tensor([[  1.7392,   2.8506,  -7.2435],\n",
      "        [  3.7269,   6.1243, -15.5456]])\n",
      "b.grad= tensor([-0.0307, -0.0639])\n",
      "new w tensor([[-0.3726,  0.8783,  0.6180],\n",
      "        [-0.2547,  0.8595,  0.7514]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.4004, grad_fn=<DivBackward0>)\n",
      "i= 384\n",
      "w.grad= tensor([[  1.7318,   2.8382,  -7.2092],\n",
      "        [  3.7101,   6.0964, -15.4727]])\n",
      "b.grad= tensor([-0.0305, -0.0636])\n",
      "new w tensor([[-0.3726,  0.8781,  0.6184],\n",
      "        [-0.2549,  0.8592,  0.7521]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.3827, grad_fn=<DivBackward0>)\n",
      "i= 385\n",
      "w.grad= tensor([[  1.7232,   2.8247,  -7.1757],\n",
      "        [  3.6931,   6.0685, -15.4002]])\n",
      "b.grad= tensor([-0.0303, -0.0632])\n",
      "new w tensor([[-0.3727,  0.8780,  0.6187],\n",
      "        [-0.2551,  0.8589,  0.7529]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.3650, grad_fn=<DivBackward0>)\n",
      "i= 386\n",
      "w.grad= tensor([[  1.7146,   2.8111,  -7.1425],\n",
      "        [  3.6746,   6.0389, -15.3291]])\n",
      "b.grad= tensor([-0.0302, -0.0629])\n",
      "new w tensor([[-0.3728,  0.8778,  0.6191],\n",
      "        [-0.2553,  0.8586,  0.7537]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.3476, grad_fn=<DivBackward0>)\n",
      "i= 387\n",
      "w.grad= tensor([[  1.7069,   2.7984,  -7.1089],\n",
      "        [  3.6579,   6.0113, -15.2572]])\n",
      "b.grad= tensor([-0.0300, -0.0626])\n",
      "new w tensor([[-0.3729,  0.8777,  0.6194],\n",
      "        [-0.2554,  0.8583,  0.7544]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.3303, grad_fn=<DivBackward0>)\n",
      "i= 388\n",
      "w.grad= tensor([[  1.6985,   2.7850,  -7.0760],\n",
      "        [  3.6407,   5.9833, -15.1860]])\n",
      "b.grad= tensor([-0.0299, -0.0622])\n",
      "new w tensor([[-0.3730,  0.8776,  0.6198],\n",
      "        [-0.2556,  0.8580,  0.7552]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.3131, grad_fn=<DivBackward0>)\n",
      "i= 389\n",
      "w.grad= tensor([[  1.6910,   2.7728,  -7.0425],\n",
      "        [  3.6238,   5.9555, -15.1150]])\n",
      "b.grad= tensor([-0.0297, -0.0619])\n",
      "new w tensor([[-0.3731,  0.8774,  0.6201],\n",
      "        [-0.2558,  0.8577,  0.7560]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.2961, grad_fn=<DivBackward0>)\n",
      "i= 390\n",
      "w.grad= tensor([[  1.6830,   2.7598,  -7.0097],\n",
      "        [  3.6068,   5.9277, -15.0445]])\n",
      "b.grad= tensor([-0.0295, -0.0616])\n",
      "new w tensor([[-0.3732,  0.8773,  0.6205],\n",
      "        [-0.2560,  0.8574,  0.7567]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.2793, grad_fn=<DivBackward0>)\n",
      "i= 391\n",
      "w.grad= tensor([[  1.6748,   2.7468,  -6.9772],\n",
      "        [  3.5897,   5.8998, -14.9744]])\n",
      "b.grad= tensor([-0.0294, -0.0612])\n",
      "new w tensor([[-0.3732,  0.8771,  0.6208],\n",
      "        [-0.2562,  0.8571,  0.7575]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.2627, grad_fn=<DivBackward0>)\n",
      "i= 392\n",
      "w.grad= tensor([[  1.6671,   2.7343,  -6.9444],\n",
      "        [  3.5733,   5.8727, -14.9043]])\n",
      "b.grad= tensor([-0.0292, -0.0609])\n",
      "new w tensor([[-0.3733,  0.8770,  0.6212],\n",
      "        [-0.2564,  0.8568,  0.7582]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.2461, grad_fn=<DivBackward0>)\n",
      "i= 393\n",
      "w.grad= tensor([[  1.6587,   2.7210,  -6.9124],\n",
      "        [  3.5565,   5.8455, -14.8348]])\n",
      "b.grad= tensor([-0.0291, -0.0606])\n",
      "new w tensor([[-0.3734,  0.8769,  0.6215],\n",
      "        [-0.2565,  0.8565,  0.7589]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.2298, grad_fn=<DivBackward0>)\n",
      "i= 394\n",
      "w.grad= tensor([[  1.6513,   2.7087,  -6.8799],\n",
      "        [  3.5392,   5.8173, -14.7660]])\n",
      "b.grad= tensor([-0.0289, -0.0602])\n",
      "new w tensor([[-0.3735,  0.8767,  0.6219],\n",
      "        [-0.2567,  0.8563,  0.7597]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.2136, grad_fn=<DivBackward0>)\n",
      "i= 395\n",
      "w.grad= tensor([[  1.6441,   2.6969,  -6.8473],\n",
      "        [  3.5235,   5.7911, -14.6965]])\n",
      "b.grad= tensor([-0.0288, -0.0599])\n",
      "new w tensor([[-0.3736,  0.8766,  0.6222],\n",
      "        [-0.2569,  0.8560,  0.7604]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.1975, grad_fn=<DivBackward0>)\n",
      "i= 396\n",
      "w.grad= tensor([[  1.6360,   2.6840,  -6.8156],\n",
      "        [  3.5065,   5.7637, -14.6282]])\n",
      "b.grad= tensor([-0.0286, -0.0596])\n",
      "new w tensor([[-0.3736,  0.8765,  0.6225],\n",
      "        [-0.2571,  0.8557,  0.7611]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.1816, grad_fn=<DivBackward0>)\n",
      "i= 397\n",
      "w.grad= tensor([[  1.6281,   2.6713,  -6.7839],\n",
      "        [  3.4905,   5.7372, -14.5596]])\n",
      "b.grad= tensor([-0.0285, -0.0593])\n",
      "new w tensor([[-0.3737,  0.8763,  0.6229],\n",
      "        [-0.2572,  0.8554,  0.7619]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0162], requires_grad=True)\n",
      "Loss= tensor(2.1659, grad_fn=<DivBackward0>)\n",
      "i= 398\n",
      "w.grad= tensor([[  1.6204,   2.6588,  -6.7523],\n",
      "        [  3.4738,   5.7101, -14.4919]])\n",
      "b.grad= tensor([-0.0283, -0.0589])\n",
      "new w tensor([[-0.3738,  0.8762,  0.6232],\n",
      "        [-0.2574,  0.8551,  0.7626]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.1503, grad_fn=<DivBackward0>)\n",
      "i= 399\n",
      "w.grad= tensor([[  1.6130,   2.6466,  -6.7207],\n",
      "        [  3.4579,   5.6839, -14.4241]])\n",
      "b.grad= tensor([-0.0282, -0.0586])\n",
      "new w tensor([[-0.3739,  0.8761,  0.6236],\n",
      "        [-0.2576,  0.8548,  0.7633]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.1348, grad_fn=<DivBackward0>)\n",
      "i= 400\n",
      "w.grad= tensor([[  1.6057,   2.6348,  -6.6890],\n",
      "        [  3.4417,   5.6572, -14.3568]])\n",
      "b.grad= tensor([-0.0280, -0.0583])\n",
      "new w tensor([[-0.3740,  0.8759,  0.6239],\n",
      "        [-0.2577,  0.8545,  0.7640]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.1195, grad_fn=<DivBackward0>)\n",
      "i= 401\n",
      "w.grad= tensor([[  1.5976,   2.6220,  -6.6581],\n",
      "        [  3.4249,   5.6300, -14.2903]])\n",
      "b.grad= tensor([-0.0279, -0.0580])\n",
      "new w tensor([[-0.3741,  0.8758,  0.6242],\n",
      "        [-0.2579,  0.8543,  0.7648]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.1043, grad_fn=<DivBackward0>)\n",
      "i= 402\n",
      "w.grad= tensor([[  1.5903,   2.6100,  -6.6269],\n",
      "        [  3.4099,   5.6049, -14.2228]])\n",
      "b.grad= tensor([-0.0277, -0.0577])\n",
      "new w tensor([[-0.3741,  0.8757,  0.6246],\n",
      "        [-0.2581,  0.8540,  0.7655]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.0893, grad_fn=<DivBackward0>)\n",
      "i= 403\n",
      "w.grad= tensor([[  1.5830,   2.5981,  -6.5959],\n",
      "        [  3.3933,   5.5781, -14.1569]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.grad= tensor([-0.0276, -0.0573])\n",
      "new w tensor([[-0.3742,  0.8755,  0.6249],\n",
      "        [-0.2583,  0.8537,  0.7662]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.0744, grad_fn=<DivBackward0>)\n",
      "i= 404\n",
      "w.grad= tensor([[  1.5751,   2.5854,  -6.5654],\n",
      "        [  3.3777,   5.5524, -14.0907]])\n",
      "b.grad= tensor([-0.0274, -0.0570])\n",
      "new w tensor([[-0.3743,  0.8754,  0.6252],\n",
      "        [-0.2584,  0.8534,  0.7669]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.0596, grad_fn=<DivBackward0>)\n",
      "i= 405\n",
      "w.grad= tensor([[  1.5676,   2.5735,  -6.5347],\n",
      "        [  3.3617,   5.5262, -14.0250]])\n",
      "b.grad= tensor([-0.0273, -0.0567])\n",
      "new w tensor([[-0.3744,  0.8753,  0.6255],\n",
      "        [-0.2586,  0.8531,  0.7676]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.0450, grad_fn=<DivBackward0>)\n",
      "i= 406\n",
      "w.grad= tensor([[  1.5604,   2.5616,  -6.5042],\n",
      "        [  3.3459,   5.5004, -13.9596]])\n",
      "b.grad= tensor([-0.0272, -0.0564])\n",
      "new w tensor([[-0.3744,  0.8752,  0.6259],\n",
      "        [-0.2588,  0.8529,  0.7683]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.0305, grad_fn=<DivBackward0>)\n",
      "i= 407\n",
      "w.grad= tensor([[  1.5531,   2.5497,  -6.4738],\n",
      "        [  3.3305,   5.4750, -13.8943]])\n",
      "b.grad= tensor([-0.0270, -0.0561])\n",
      "new w tensor([[-0.3745,  0.8750,  0.6262],\n",
      "        [-0.2589,  0.8526,  0.7690]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.0162, grad_fn=<DivBackward0>)\n",
      "i= 408\n",
      "w.grad= tensor([[  1.5454,   2.5374,  -6.4439],\n",
      "        [  3.3149,   5.4494, -13.8295]])\n",
      "b.grad= tensor([-0.0269, -0.0558])\n",
      "new w tensor([[-0.3746,  0.8749,  0.6265],\n",
      "        [-0.2591,  0.8523,  0.7697]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(2.0020, grad_fn=<DivBackward0>)\n",
      "i= 409\n",
      "w.grad= tensor([[  1.5384,   2.5259,  -6.4136],\n",
      "        [  3.2996,   5.4244, -13.7648]])\n",
      "b.grad= tensor([-0.0267, -0.0555])\n",
      "new w tensor([[-0.3747,  0.8748,  0.6268],\n",
      "        [-0.2593,  0.8521,  0.7704]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.9879, grad_fn=<DivBackward0>)\n",
      "i= 410\n",
      "w.grad= tensor([[  1.5313,   2.5142,  -6.3837],\n",
      "        [  3.2841,   5.3990, -13.7006]])\n",
      "b.grad= tensor([-0.0266, -0.0552])\n",
      "new w tensor([[-0.3748,  0.8747,  0.6272],\n",
      "        [-0.2594,  0.8518,  0.7710]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.9739, grad_fn=<DivBackward0>)\n",
      "i= 411\n",
      "w.grad= tensor([[  1.5246,   2.5031,  -6.3534],\n",
      "        [  3.2686,   5.3736, -13.6367]])\n",
      "b.grad= tensor([-0.0264, -0.0549])\n",
      "new w tensor([[-0.3748,  0.8745,  0.6275],\n",
      "        [-0.2596,  0.8515,  0.7717]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.9601, grad_fn=<DivBackward0>)\n",
      "i= 412\n",
      "w.grad= tensor([[  1.5171,   2.4912,  -6.3240],\n",
      "        [  3.2531,   5.3483, -13.5732]])\n",
      "b.grad= tensor([-0.0263, -0.0546])\n",
      "new w tensor([[-0.3749,  0.8744,  0.6278],\n",
      "        [-0.2598,  0.8512,  0.7724]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.9464, grad_fn=<DivBackward0>)\n",
      "i= 413\n",
      "w.grad= tensor([[  1.5097,   2.4793,  -6.2946],\n",
      "        [  3.2383,   5.3237, -13.5096]])\n",
      "b.grad= tensor([-0.0262, -0.0543])\n",
      "new w tensor([[-0.3750,  0.8743,  0.6281],\n",
      "        [-0.2599,  0.8510,  0.7731]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.9328, grad_fn=<DivBackward0>)\n",
      "i= 414\n",
      "w.grad= tensor([[  1.5023,   2.4673,  -6.2655],\n",
      "        [  3.2229,   5.2985, -13.4467]])\n",
      "b.grad= tensor([-0.0260, -0.0540])\n",
      "new w tensor([[-0.3751,  0.8742,  0.6284],\n",
      "        [-0.2601,  0.8507,  0.7737]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.9194, grad_fn=<DivBackward0>)\n",
      "i= 415\n",
      "w.grad= tensor([[  1.4958,   2.4564,  -6.2359],\n",
      "        [  3.2082,   5.2744, -13.3837]])\n",
      "b.grad= tensor([-0.0259, -0.0537])\n",
      "new w tensor([[-0.3751,  0.8740,  0.6287],\n",
      "        [-0.2602,  0.8505,  0.7744]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.9061, grad_fn=<DivBackward0>)\n",
      "i= 416\n",
      "w.grad= tensor([[  1.4884,   2.4445,  -6.2070],\n",
      "        [  3.1930,   5.2496, -13.3213]])\n",
      "b.grad= tensor([-0.0257, -0.0534])\n",
      "new w tensor([[-0.3752,  0.8739,  0.6290],\n",
      "        [-0.2604,  0.8502,  0.7751]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.8929, grad_fn=<DivBackward0>)\n",
      "i= 417\n",
      "w.grad= tensor([[  1.4819,   2.4338,  -6.1777],\n",
      "        [  3.1779,   5.2248, -13.2593]])\n",
      "b.grad= tensor([-0.0256, -0.0531])\n",
      "new w tensor([[-0.3753,  0.8738,  0.6293],\n",
      "        [-0.2606,  0.8499,  0.7757]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.8798, grad_fn=<DivBackward0>)\n",
      "i= 418\n",
      "w.grad= tensor([[  1.4746,   2.4221,  -6.1491],\n",
      "        [  3.1628,   5.2001, -13.1976]])\n",
      "b.grad= tensor([-0.0255, -0.0528])\n",
      "new w tensor([[-0.3754,  0.8737,  0.6297],\n",
      "        [-0.2607,  0.8497,  0.7764]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.8669, grad_fn=<DivBackward0>)\n",
      "i= 419\n",
      "w.grad= tensor([[  1.4678,   2.4108,  -6.1204],\n",
      "        [  3.1486,   5.1766, -13.1356]])\n",
      "b.grad= tensor([-0.0253, -0.0525])\n",
      "new w tensor([[-0.3754,  0.8736,  0.6300],\n",
      "        [-0.2609,  0.8494,  0.7771]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.8541, grad_fn=<DivBackward0>)\n",
      "i= 420\n",
      "w.grad= tensor([[  1.4610,   2.3997,  -6.0918],\n",
      "        [  3.1343,   5.1530, -13.0740]])\n",
      "b.grad= tensor([-0.0252, -0.0522])\n",
      "new w tensor([[-0.3755,  0.8734,  0.6303],\n",
      "        [-0.2610,  0.8492,  0.7777]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.8414, grad_fn=<DivBackward0>)\n",
      "i= 421\n",
      "w.grad= tensor([[  1.4542,   2.3885,  -6.0633],\n",
      "        [  3.1183,   5.1274, -13.0139]])\n",
      "b.grad= tensor([-0.0251, -0.0519])\n",
      "new w tensor([[-0.3756,  0.8733,  0.6306],\n",
      "        [-0.2612,  0.8489,  0.7784]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.8288, grad_fn=<DivBackward0>)\n",
      "i= 422\n",
      "w.grad= tensor([[  1.4470,   2.3771,  -6.0353],\n",
      "        [  3.1049,   5.1049, -12.9524]])\n",
      "b.grad= tensor([-0.0249, -0.0516])\n",
      "new w tensor([[-0.3756,  0.8732,  0.6309],\n",
      "        [-0.2613,  0.8486,  0.7790]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.8163, grad_fn=<DivBackward0>)\n",
      "i= 423\n",
      "w.grad= tensor([[  1.4407,   2.3666,  -6.0067],\n",
      "        [  3.0898,   5.0804, -12.8923]])\n",
      "b.grad= tensor([-0.0248, -0.0513])\n",
      "new w tensor([[-0.3757,  0.8731,  0.6312],\n",
      "        [-0.2615,  0.8484,  0.7797]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.8040, grad_fn=<DivBackward0>)\n",
      "i= 424\n",
      "w.grad= tensor([[  1.4341,   2.3556,  -5.9786],\n",
      "        [  3.0752,   5.0564, -12.8323]])\n",
      "b.grad= tensor([-0.0247, -0.0511])\n",
      "new w tensor([[-0.3758,  0.8730,  0.6315],\n",
      "        [-0.2616,  0.8481,  0.7803]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.7917, grad_fn=<DivBackward0>)\n",
      "i= 425\n",
      "w.grad= tensor([[  1.4268,   2.3441,  -5.9511],\n",
      "        [  3.0611,   5.0332, -12.7721]])\n",
      "b.grad= tensor([-0.0245, -0.0508])\n",
      "new w tensor([[-0.3759,  0.8728,  0.6318],\n",
      "        [-0.2618,  0.8479,  0.7809]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.7796, grad_fn=<DivBackward0>)\n",
      "i= 426\n",
      "w.grad= tensor([[  1.4204,   2.3335,  -5.9231],\n",
      "        [  3.0471,   5.0100, -12.7124]])\n",
      "b.grad= tensor([-0.0244, -0.0505])\n",
      "new w tensor([[-0.3759,  0.8727,  0.6321],\n",
      "        [-0.2620,  0.8476,  0.7816]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.7676, grad_fn=<DivBackward0>)\n",
      "i= 427\n",
      "w.grad= tensor([[  1.4139,   2.3228,  -5.8953],\n",
      "        [  3.0322,   4.9859, -12.6536]])\n",
      "b.grad= tensor([-0.0243, -0.0502])\n",
      "new w tensor([[-0.3760,  0.8726,  0.6324],\n",
      "        [-0.2621,  0.8474,  0.7822]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.7557, grad_fn=<DivBackward0>)\n",
      "i= 428\n",
      "w.grad= tensor([[  1.4068,   2.3114,  -5.8682],\n",
      "        [  3.0189,   4.9635, -12.5938]])\n",
      "b.grad= tensor([-0.0241, -0.0499])\n",
      "new w tensor([[-0.3761,  0.8725,  0.6327],\n",
      "        [-0.2623,  0.8471,  0.7828]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.7439, grad_fn=<DivBackward0>)\n",
      "i= 429\n",
      "w.grad= tensor([[  1.4009,   2.3014,  -5.8403],\n",
      "        [  3.0044,   4.9401, -12.5353]])\n",
      "b.grad= tensor([-0.0240, -0.0497])\n",
      "new w tensor([[-0.3761,  0.8724,  0.6329],\n",
      "        [-0.2624,  0.8469,  0.7835]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.7322, grad_fn=<DivBackward0>)\n",
      "i= 430\n",
      "w.grad= tensor([[  1.3939,   2.2903,  -5.8133],\n",
      "        [  2.9897,   4.9161, -12.4773]])\n",
      "b.grad= tensor([-0.0239, -0.0494])\n",
      "new w tensor([[-0.3762,  0.8723,  0.6332],\n",
      "        [-0.2626,  0.8466,  0.7841]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.7207, grad_fn=<DivBackward0>)\n",
      "i= 431\n",
      "w.grad= tensor([[  1.3875,   2.2797,  -5.7861],\n",
      "        [  2.9767,   4.8943, -12.4184]])\n",
      "b.grad= tensor([-0.0238, -0.0491])\n",
      "new w tensor([[-0.3763,  0.8721,  0.6335],\n",
      "        [-0.2627,  0.8464,  0.7847]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.7092, grad_fn=<DivBackward0>)\n",
      "i= 432\n",
      "w.grad= tensor([[  1.3808,   2.2689,  -5.7592],\n",
      "        [  2.9620,   4.8706, -12.3610]])\n",
      "b.grad= tensor([-0.0236, -0.0488])\n",
      "new w tensor([[-0.3764,  0.8720,  0.6338],\n",
      "        [-0.2629,  0.8462,  0.7853]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.6978, grad_fn=<DivBackward0>)\n",
      "i= 433\n",
      "w.grad= tensor([[  1.3743,   2.2583,  -5.7324],\n",
      "        [  2.9485,   4.8483, -12.3031]])\n",
      "b.grad= tensor([-0.0235, -0.0486])\n",
      "new w tensor([[-0.3764,  0.8719,  0.6341],\n",
      "        [-0.2630,  0.8459,  0.7859]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.6866, grad_fn=<DivBackward0>)\n",
      "i= 434\n",
      "w.grad= tensor([[  1.3682,   2.2482,  -5.7053],\n",
      "        [  2.9351,   4.8261, -12.2453]])\n",
      "b.grad= tensor([-0.0234, -0.0483])\n",
      "new w tensor([[-0.3765,  0.8718,  0.6344],\n",
      "        [-0.2631,  0.8457,  0.7866]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.6754, grad_fn=<DivBackward0>)\n",
      "i= 435\n",
      "w.grad= tensor([[  1.3618,   2.2376,  -5.6788],\n",
      "        [  2.9211,   4.8031, -12.1884]])\n",
      "b.grad= tensor([-0.0232, -0.0480])\n",
      "new w tensor([[-0.3766,  0.8717,  0.6347],\n",
      "        [-0.2633,  0.8454,  0.7872]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0161], requires_grad=True)\n",
      "Loss= tensor(1.6644, grad_fn=<DivBackward0>)\n",
      "i= 436\n",
      "w.grad= tensor([[  1.3553,   2.2271,  -5.6523],\n",
      "        [  2.9074,   4.7807, -12.1315]])\n",
      "b.grad= tensor([-0.0231, -0.0477])\n",
      "new w tensor([[-0.3766,  0.8716,  0.6350],\n",
      "        [-0.2634,  0.8452,  0.7878]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.6535, grad_fn=<DivBackward0>)\n",
      "i= 437\n",
      "w.grad= tensor([[  1.3491,   2.2169,  -5.6258],\n",
      "        [  2.8942,   4.7588, -12.0747]])\n",
      "b.grad= tensor([-0.0230, -0.0475])\n",
      "new w tensor([[-0.3767,  0.8715,  0.6352],\n",
      "        [-0.2636,  0.8450,  0.7884]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.6426, grad_fn=<DivBackward0>)\n",
      "i= 438\n",
      "w.grad= tensor([[  1.3423,   2.2061,  -5.5999],\n",
      "        [  2.8802,   4.7360, -12.0186]])\n",
      "b.grad= tensor([-0.0229, -0.0472])\n",
      "new w tensor([[-0.3768,  0.8714,  0.6355],\n",
      "        [-0.2637,  0.8447,  0.7890]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.6319, grad_fn=<DivBackward0>)\n",
      "i= 439\n",
      "w.grad= tensor([[  1.3359,   2.1957,  -5.5738],\n",
      "        [  2.8671,   4.7146, -11.9623]])\n",
      "b.grad= tensor([-0.0227, -0.0469])\n",
      "new w tensor([[-0.3768,  0.8713,  0.6358],\n",
      "        [-0.2639,  0.8445,  0.7896]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.6213, grad_fn=<DivBackward0>)\n",
      "i= 440\n",
      "w.grad= tensor([[  1.3301,   2.1859,  -5.5475],\n",
      "        [  2.8539,   4.6927, -11.9063]])\n",
      "b.grad= tensor([-0.0226, -0.0467])\n",
      "new w tensor([[-0.3769,  0.8711,  0.6361],\n",
      "        [-0.2640,  0.8442,  0.7902]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.6107, grad_fn=<DivBackward0>)\n",
      "i= 441\n",
      "w.grad= tensor([[  1.3236,   2.1754,  -5.5218],\n",
      "        [  2.8402,   4.6704, -11.8510]])\n",
      "b.grad= tensor([-0.0225, -0.0464])\n",
      "new w tensor([[-0.3770,  0.8710,  0.6363],\n",
      "        [-0.2642,  0.8440,  0.7908]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.6003, grad_fn=<DivBackward0>)\n",
      "i= 442\n",
      "w.grad= tensor([[  1.3176,   2.1656,  -5.4958],\n",
      "        [  2.8264,   4.6479, -11.7961]])\n",
      "b.grad= tensor([-0.0224, -0.0461])\n",
      "new w tensor([[-0.3770,  0.8709,  0.6366],\n",
      "        [-0.2643,  0.8438,  0.7913]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.5899, grad_fn=<DivBackward0>)\n",
      "i= 443\n",
      "w.grad= tensor([[  1.3113,   2.1552,  -5.4703],\n",
      "        [  2.8137,   4.6268, -11.7407]])\n",
      "b.grad= tensor([-0.0223, -0.0459])\n",
      "new w tensor([[-0.3771,  0.8708,  0.6369],\n",
      "        [-0.2644,  0.8435,  0.7919]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.5797, grad_fn=<DivBackward0>)\n",
      "i= 444\n",
      "w.grad= tensor([[  1.3049,   2.1449,  -5.4450],\n",
      "        [  2.8009,   4.6057, -11.6856]])\n",
      "b.grad= tensor([-0.0221, -0.0456])\n",
      "new w tensor([[-0.3772,  0.8707,  0.6372],\n",
      "        [-0.2646,  0.8433,  0.7925]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.5695, grad_fn=<DivBackward0>)\n",
      "i= 445\n",
      "w.grad= tensor([[  1.2995,   2.1358,  -5.4191],\n",
      "        [  2.7875,   4.5839, -11.6313]])\n",
      "b.grad= tensor([-0.0220, -0.0454])\n",
      "new w tensor([[-0.3772,  0.8706,  0.6374],\n",
      "        [-0.2647,  0.8431,  0.7931]], requires_grad=True)\n",
      "new b tensor([-1.1851, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.5595, grad_fn=<DivBackward0>)\n",
      "i= 446\n",
      "w.grad= tensor([[  1.2928,   2.1251,  -5.3941],\n",
      "        [  2.7742,   4.5620, -11.5772]])\n",
      "b.grad= tensor([-0.0219, -0.0451])\n",
      "new w tensor([[-0.3773,  0.8705,  0.6377],\n",
      "        [-0.2649,  0.8429,  0.7937]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.5495, grad_fn=<DivBackward0>)\n",
      "i= 447\n",
      "w.grad= tensor([[  1.2872,   2.1156,  -5.3687],\n",
      "        [  2.7620,   4.5418, -11.5226]])\n",
      "b.grad= tensor([-0.0218, -0.0448])\n",
      "new w tensor([[-0.3773,  0.8704,  0.6380],\n",
      "        [-0.2650,  0.8426,  0.7943]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.5397, grad_fn=<DivBackward0>)\n",
      "i= 448\n",
      "w.grad= tensor([[  1.2806,   2.1050,  -5.3441],\n",
      "        [  2.7483,   4.5195, -11.4694]])\n",
      "b.grad= tensor([-0.0217, -0.0446])\n",
      "new w tensor([[-0.3774,  0.8703,  0.6382],\n",
      "        [-0.2651,  0.8424,  0.7948]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.5299, grad_fn=<DivBackward0>)\n",
      "i= 449\n",
      "w.grad= tensor([[  1.2754,   2.0962,  -5.3185],\n",
      "        [  2.7357,   4.4987, -11.4157]])\n",
      "b.grad= tensor([-0.0215, -0.0443])\n",
      "new w tensor([[-0.3775,  0.8702,  0.6385],\n",
      "        [-0.2653,  0.8422,  0.7954]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.5202, grad_fn=<DivBackward0>)\n",
      "i= 450\n",
      "w.grad= tensor([[  1.2691,   2.0861,  -5.2939],\n",
      "        [  2.7231,   4.4779, -11.3624]])\n",
      "b.grad= tensor([-0.0214, -0.0441])\n",
      "new w tensor([[-0.3775,  0.8701,  0.6388],\n",
      "        [-0.2654,  0.8420,  0.7960]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.5106, grad_fn=<DivBackward0>)\n",
      "i= 451\n",
      "w.grad= tensor([[  1.2632,   2.0763,  -5.2692],\n",
      "        [  2.7099,   4.4565, -11.3096]])\n",
      "b.grad= tensor([-0.0213, -0.0438])\n",
      "new w tensor([[-0.3776,  0.8700,  0.6390],\n",
      "        [-0.2655,  0.8417,  0.7965]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.5011, grad_fn=<DivBackward0>)\n",
      "i= 452\n",
      "w.grad= tensor([[  1.2570,   2.0663,  -5.2449],\n",
      "        [  2.6978,   4.4364, -11.2564]])\n",
      "b.grad= tensor([-0.0212, -0.0436])\n",
      "new w tensor([[-0.3777,  0.8699,  0.6393],\n",
      "        [-0.2657,  0.8415,  0.7971]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4917, grad_fn=<DivBackward0>)\n",
      "i= 453\n",
      "w.grad= tensor([[  1.2517,   2.0573,  -5.2200],\n",
      "        [  2.6851,   4.4156, -11.2040]])\n",
      "b.grad= tensor([-0.0211, -0.0433])\n",
      "new w tensor([[-0.3777,  0.8698,  0.6396],\n",
      "        [-0.2658,  0.8413,  0.7977]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4824, grad_fn=<DivBackward0>)\n",
      "i= 454\n",
      "w.grad= tensor([[  1.2455,   2.0473,  -5.1958],\n",
      "        [  2.6728,   4.3952, -11.1515]])\n",
      "b.grad= tensor([-0.0210, -0.0431])\n",
      "new w tensor([[-0.3778,  0.8697,  0.6398],\n",
      "        [-0.2659,  0.8411,  0.7982]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4731, grad_fn=<DivBackward0>)\n",
      "i= 455\n",
      "w.grad= tensor([[  1.2399,   2.0380,  -5.1714],\n",
      "        [  2.6600,   4.3744, -11.0997]])\n",
      "b.grad= tensor([-0.0208, -0.0428])\n",
      "new w tensor([[-0.3779,  0.8696,  0.6401],\n",
      "        [-0.2661,  0.8409,  0.7988]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4640, grad_fn=<DivBackward0>)\n",
      "i= 456\n",
      "w.grad= tensor([[  1.2341,   2.0286,  -5.1473],\n",
      "        [  2.6471,   4.3536, -11.0481]])\n",
      "b.grad= tensor([-0.0207, -0.0426])\n",
      "new w tensor([[-0.3779,  0.8695,  0.6403],\n",
      "        [-0.2662,  0.8406,  0.7993]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4549, grad_fn=<DivBackward0>)\n",
      "i= 457\n",
      "w.grad= tensor([[  1.2283,   2.0191,  -5.1233],\n",
      "        [  2.6349,   4.3333, -10.9965]])\n",
      "b.grad= tensor([-0.0206, -0.0424])\n",
      "new w tensor([[-0.3780,  0.8694,  0.6406],\n",
      "        [-0.2663,  0.8404,  0.7999]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4459, grad_fn=<DivBackward0>)\n",
      "i= 458\n",
      "w.grad= tensor([[  1.2219,   2.0090,  -5.0998],\n",
      "        [  2.6234,   4.3139, -10.9447]])\n",
      "b.grad= tensor([-0.0205, -0.0421])\n",
      "new w tensor([[-0.3780,  0.8693,  0.6408],\n",
      "        [-0.2665,  0.8402,  0.8004]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4370, grad_fn=<DivBackward0>)\n",
      "i= 459\n",
      "w.grad= tensor([[  1.2166,   1.9999,  -5.0758],\n",
      "        [  2.6104,   4.2929, -10.8941]])\n",
      "b.grad= tensor([-0.0204, -0.0419])\n",
      "new w tensor([[-0.3781,  0.8692,  0.6411],\n",
      "        [-0.2666,  0.8400,  0.8010]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4282, grad_fn=<DivBackward0>)\n",
      "i= 460\n",
      "w.grad= tensor([[  1.2109,   1.9907,  -5.0520],\n",
      "        [  2.5987,   4.2735, -10.8429]])\n",
      "b.grad= tensor([-0.0203, -0.0416])\n",
      "new w tensor([[-0.3782,  0.8691,  0.6414],\n",
      "        [-0.2667,  0.8398,  0.8015]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4194, grad_fn=<DivBackward0>)\n",
      "i= 461\n",
      "w.grad= tensor([[  1.2053,   1.9814,  -5.0285],\n",
      "        [  2.5864,   4.2533, -10.7924]])\n",
      "b.grad= tensor([-0.0202, -0.0414])\n",
      "new w tensor([[-0.3782,  0.8690,  0.6416],\n",
      "        [-0.2669,  0.8396,  0.8020]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4108, grad_fn=<DivBackward0>)\n",
      "i= 462\n",
      "w.grad= tensor([[  1.1996,   1.9721,  -5.0051],\n",
      "        [  2.5741,   4.2332, -10.7422]])\n",
      "b.grad= tensor([-0.0200, -0.0411])\n",
      "new w tensor([[-0.3783,  0.8689,  0.6419],\n",
      "        [-0.2670,  0.8394,  0.8026]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.4022, grad_fn=<DivBackward0>)\n",
      "i= 463\n",
      "w.grad= tensor([[  1.1944,   1.9633,  -4.9814],\n",
      "        [  2.5621,   4.2134, -10.6921]])\n",
      "b.grad= tensor([-0.0199, -0.0409])\n",
      "new w tensor([[-0.3783,  0.8688,  0.6421],\n",
      "        [-0.2671,  0.8391,  0.8031]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3937, grad_fn=<DivBackward0>)\n",
      "i= 464\n",
      "w.grad= tensor([[  1.1887,   1.9541,  -4.9582],\n",
      "        [  2.5505,   4.1942, -10.6419]])\n",
      "b.grad= tensor([-0.0198, -0.0407])\n",
      "new w tensor([[-0.3784,  0.8687,  0.6424],\n",
      "        [-0.2672,  0.8389,  0.8036]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3853, grad_fn=<DivBackward0>)\n",
      "i= 465\n",
      "w.grad= tensor([[  1.1829,   1.9447,  -4.9352],\n",
      "        [  2.5387,   4.1748, -10.5921]])\n",
      "b.grad= tensor([-0.0197, -0.0404])\n",
      "new w tensor([[-0.3785,  0.8686,  0.6426],\n",
      "        [-0.2674,  0.8387,  0.8042]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3769, grad_fn=<DivBackward0>)\n",
      "i= 466\n",
      "w.grad= tensor([[  1.1778,   1.9362,  -4.9119],\n",
      "        [  2.5264,   4.1547, -10.5431]])\n",
      "b.grad= tensor([-0.0196, -0.0402])\n",
      "new w tensor([[-0.3785,  0.8685,  0.6428],\n",
      "        [-0.2675,  0.8385,  0.8047]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3687, grad_fn=<DivBackward0>)\n",
      "i= 467\n",
      "w.grad= tensor([[  1.1715,   1.9263,  -4.8895],\n",
      "        [  2.5147,   4.1356, -10.4937]])\n",
      "b.grad= tensor([-0.0195, -0.0400])\n",
      "new w tensor([[-0.3786,  0.8684,  0.6431],\n",
      "        [-0.2676,  0.8383,  0.8052]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3605, grad_fn=<DivBackward0>)\n",
      "i= 468\n",
      "w.grad= tensor([[  1.1667,   1.9180,  -4.8662],\n",
      "        [  2.5030,   4.1164, -10.4447]])\n",
      "b.grad= tensor([-0.0194, -0.0397])\n",
      "new w tensor([[-0.3786,  0.8683,  0.6433],\n",
      "        [-0.2677,  0.8381,  0.8058]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3524, grad_fn=<DivBackward0>)\n",
      "i= 469\n",
      "w.grad= tensor([[  1.1610,   1.9088,  -4.8437],\n",
      "        [  2.4916,   4.0974, -10.3958]])\n",
      "b.grad= tensor([-0.0193, -0.0395])\n",
      "new w tensor([[-0.3787,  0.8682,  0.6436],\n",
      "        [-0.2679,  0.8379,  0.8063]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3444, grad_fn=<DivBackward0>)\n",
      "i= 470\n",
      "w.grad= tensor([[  1.1554,   1.8997,  -4.8212],\n",
      "        [  2.4800,   4.0784, -10.3472]])\n",
      "b.grad= tensor([-0.0192, -0.0393])\n",
      "new w tensor([[-0.3787,  0.8681,  0.6438],\n",
      "        [-0.2680,  0.8377,  0.8068]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3364, grad_fn=<DivBackward0>)\n",
      "i= 471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([[  1.1499,   1.8907,  -4.7988],\n",
      "        [  2.4681,   4.0589, -10.2992]])\n",
      "b.grad= tensor([-0.0191, -0.0390])\n",
      "new w tensor([[-0.3788,  0.8680,  0.6441],\n",
      "        [-0.2681,  0.8375,  0.8073]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3285, grad_fn=<DivBackward0>)\n",
      "i= 472\n",
      "w.grad= tensor([[  1.1452,   1.8827,  -4.7759],\n",
      "        [  2.4567,   4.0401, -10.2511]])\n",
      "b.grad= tensor([-0.0190, -0.0388])\n",
      "new w tensor([[-0.3789,  0.8679,  0.6443],\n",
      "        [-0.2682,  0.8373,  0.8078]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3207, grad_fn=<DivBackward0>)\n",
      "i= 473\n",
      "w.grad= tensor([[  1.1394,   1.8732,  -4.7540],\n",
      "        [  2.4450,   4.0210, -10.2033]])\n",
      "b.grad= tensor([-0.0189, -0.0386])\n",
      "new w tensor([[-0.3789,  0.8678,  0.6445],\n",
      "        [-0.2684,  0.8371,  0.8083]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3130, grad_fn=<DivBackward0>)\n",
      "i= 474\n",
      "w.grad= tensor([[  1.1343,   1.8650,  -4.7316],\n",
      "        [  2.4337,   4.0024, -10.1556]])\n",
      "b.grad= tensor([-0.0187, -0.0384])\n",
      "new w tensor([[-0.3790,  0.8677,  0.6448],\n",
      "        [-0.2685,  0.8369,  0.8088]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.3053, grad_fn=<DivBackward0>)\n",
      "i= 475\n",
      "w.grad= tensor([[  1.1288,   1.8559,  -4.7096],\n",
      "        [  2.4226,   3.9840, -10.1081]])\n",
      "b.grad= tensor([-0.0186, -0.0381])\n",
      "new w tensor([[-0.3790,  0.8676,  0.6450],\n",
      "        [-0.2686,  0.8367,  0.8093]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.2977, grad_fn=<DivBackward0>)\n",
      "i= 476\n",
      "w.grad= tensor([[  1.1236,   1.8473,  -4.6876],\n",
      "        [  2.4110,   3.9651, -10.0611]])\n",
      "b.grad= tensor([-0.0185, -0.0379])\n",
      "new w tensor([[-0.3791,  0.8675,  0.6452],\n",
      "        [-0.2687,  0.8365,  0.8098]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.2902, grad_fn=<DivBackward0>)\n",
      "i= 477\n",
      "w.grad= tensor([[  1.1182,   1.8386,  -4.6658],\n",
      "        [  2.4000,   3.9469, -10.0140]])\n",
      "b.grad= tensor([-0.0184, -0.0377])\n",
      "new w tensor([[-0.3791,  0.8674,  0.6455],\n",
      "        [-0.2688,  0.8363,  0.8103]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.2827, grad_fn=<DivBackward0>)\n",
      "i= 478\n",
      "w.grad= tensor([[ 1.1129,  1.8299, -4.6441],\n",
      "        [ 2.3884,  3.9280, -9.9675]])\n",
      "b.grad= tensor([-0.0183, -0.0375])\n",
      "new w tensor([[-0.3792,  0.8673,  0.6457],\n",
      "        [-0.2690,  0.8361,  0.8108]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.2754, grad_fn=<DivBackward0>)\n",
      "i= 479\n",
      "w.grad= tensor([[ 1.1080,  1.8217, -4.6222],\n",
      "        [ 2.3774,  3.9099, -9.9209]])\n",
      "b.grad= tensor([-0.0182, -0.0372])\n",
      "new w tensor([[-0.3793,  0.8673,  0.6459],\n",
      "        [-0.2691,  0.8359,  0.8113]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.2680, grad_fn=<DivBackward0>)\n",
      "i= 480\n",
      "w.grad= tensor([[ 1.1030,  1.8135, -4.6005],\n",
      "        [ 2.3660,  3.8913, -9.8748]])\n",
      "b.grad= tensor([-0.0181, -0.0370])\n",
      "new w tensor([[-0.3793,  0.8672,  0.6462],\n",
      "        [-0.2692,  0.8357,  0.8118]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.2608, grad_fn=<DivBackward0>)\n",
      "i= 481\n",
      "w.grad= tensor([[ 1.0973,  1.8045, -4.5794],\n",
      "        [ 2.3556,  3.8738, -9.8282]])\n",
      "b.grad= tensor([-0.0180, -0.0368])\n",
      "new w tensor([[-0.3794,  0.8671,  0.6464],\n",
      "        [-0.2693,  0.8355,  0.8123]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.2536, grad_fn=<DivBackward0>)\n",
      "i= 482\n",
      "w.grad= tensor([[ 1.0918,  1.7956, -4.5583],\n",
      "        [ 2.3440,  3.8551, -9.7828]])\n",
      "b.grad= tensor([-0.0179, -0.0366])\n",
      "new w tensor([[-0.3794,  0.8670,  0.6466],\n",
      "        [-0.2694,  0.8353,  0.8128]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0160], requires_grad=True)\n",
      "Loss= tensor(1.2465, grad_fn=<DivBackward0>)\n",
      "i= 483\n",
      "w.grad= tensor([[ 1.0876,  1.7882, -4.5364],\n",
      "        [ 2.3329,  3.8370, -9.7372]])\n",
      "b.grad= tensor([-0.0178, -0.0364])\n",
      "new w tensor([[-0.3795,  0.8669,  0.6468],\n",
      "        [-0.2696,  0.8351,  0.8133]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.2395, grad_fn=<DivBackward0>)\n",
      "i= 484\n",
      "w.grad= tensor([[ 1.0821,  1.7794, -4.5155],\n",
      "        [ 2.3221,  3.8191, -9.6918]])\n",
      "b.grad= tensor([-0.0177, -0.0362])\n",
      "new w tensor([[-0.3795,  0.8668,  0.6471],\n",
      "        [-0.2697,  0.8349,  0.8138]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.2325, grad_fn=<DivBackward0>)\n",
      "i= 485\n",
      "w.grad= tensor([[ 1.0769,  1.7710, -4.4945],\n",
      "        [ 2.3118,  3.8019, -9.6461]])\n",
      "b.grad= tensor([-0.0176, -0.0359])\n",
      "new w tensor([[-0.3796,  0.8667,  0.6473],\n",
      "        [-0.2698,  0.8347,  0.8143]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.2256, grad_fn=<DivBackward0>)\n",
      "i= 486\n",
      "w.grad= tensor([[ 1.0717,  1.7625, -4.4736],\n",
      "        [ 2.3011,  3.7843, -9.6010]])\n",
      "b.grad= tensor([-0.0175, -0.0357])\n",
      "new w tensor([[-0.3796,  0.8666,  0.6475],\n",
      "        [-0.2699,  0.8346,  0.8147]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.2187, grad_fn=<DivBackward0>)\n",
      "i= 487\n",
      "w.grad= tensor([[ 1.0668,  1.7543, -4.4527],\n",
      "        [ 2.2898,  3.7659, -9.5566]])\n",
      "b.grad= tensor([-0.0174, -0.0355])\n",
      "new w tensor([[-0.3797,  0.8665,  0.6477],\n",
      "        [-0.2700,  0.8344,  0.8152]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.2119, grad_fn=<DivBackward0>)\n",
      "i= 488\n",
      "w.grad= tensor([[ 1.0622,  1.7465, -4.4317],\n",
      "        [ 2.2795,  3.7488, -9.5117]])\n",
      "b.grad= tensor([-0.0173, -0.0353])\n",
      "new w tensor([[-0.3797,  0.8665,  0.6480],\n",
      "        [-0.2701,  0.8342,  0.8157]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.2052, grad_fn=<DivBackward0>)\n",
      "i= 489\n",
      "w.grad= tensor([[ 1.0573,  1.7385, -4.4110],\n",
      "        [ 2.2688,  3.7313, -9.4673]])\n",
      "b.grad= tensor([-0.0172, -0.0351])\n",
      "new w tensor([[-0.3798,  0.8664,  0.6482],\n",
      "        [-0.2702,  0.8340,  0.8162]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1985, grad_fn=<DivBackward0>)\n",
      "i= 490\n",
      "w.grad= tensor([[ 1.0522,  1.7302, -4.3904],\n",
      "        [ 2.2582,  3.7138, -9.4232]])\n",
      "b.grad= tensor([-0.0171, -0.0349])\n",
      "new w tensor([[-0.3798,  0.8663,  0.6484],\n",
      "        [-0.2704,  0.8338,  0.8166]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1919, grad_fn=<DivBackward0>)\n",
      "i= 491\n",
      "w.grad= tensor([[ 1.0478,  1.7227, -4.3697],\n",
      "        [ 2.2480,  3.6969, -9.3790]])\n",
      "b.grad= tensor([-0.0170, -0.0347])\n",
      "new w tensor([[-0.3799,  0.8662,  0.6486],\n",
      "        [-0.2705,  0.8336,  0.8171]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1854, grad_fn=<DivBackward0>)\n",
      "i= 492\n",
      "w.grad= tensor([[ 1.0425,  1.7143, -4.3495],\n",
      "        [ 2.2372,  3.6794, -9.3355]])\n",
      "b.grad= tensor([-0.0169, -0.0345])\n",
      "new w tensor([[-0.3800,  0.8661,  0.6488],\n",
      "        [-0.2706,  0.8334,  0.8176]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1789, grad_fn=<DivBackward0>)\n",
      "i= 493\n",
      "w.grad= tensor([[ 1.0374,  1.7060, -4.3294],\n",
      "        [ 2.2271,  3.6626, -9.2916]])\n",
      "b.grad= tensor([-0.0168, -0.0343])\n",
      "new w tensor([[-0.3800,  0.8660,  0.6491],\n",
      "        [-0.2707,  0.8333,  0.8180]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1725, grad_fn=<DivBackward0>)\n",
      "i= 494\n",
      "w.grad= tensor([[ 1.0326,  1.6981, -4.3091],\n",
      "        [ 2.2164,  3.6452, -9.2484]])\n",
      "b.grad= tensor([-0.0167, -0.0340])\n",
      "new w tensor([[-0.3801,  0.8659,  0.6493],\n",
      "        [-0.2708,  0.8331,  0.8185]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1661, grad_fn=<DivBackward0>)\n",
      "i= 495\n",
      "w.grad= tensor([[ 1.0284,  1.6908, -4.2886],\n",
      "        [ 2.2054,  3.6274, -9.2057]])\n",
      "b.grad= tensor([-0.0166, -0.0339])\n",
      "new w tensor([[-0.3801,  0.8659,  0.6495],\n",
      "        [-0.2709,  0.8329,  0.8190]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1598, grad_fn=<DivBackward0>)\n",
      "i= 496\n",
      "w.grad= tensor([[ 1.0233,  1.6825, -4.2688],\n",
      "        [ 2.1950,  3.6103, -9.1628]])\n",
      "b.grad= tensor([-0.0166, -0.0337])\n",
      "new w tensor([[-0.3802,  0.8658,  0.6497],\n",
      "        [-0.2710,  0.8327,  0.8194]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1536, grad_fn=<DivBackward0>)\n",
      "i= 497\n",
      "w.grad= tensor([[ 1.0188,  1.6751, -4.2487],\n",
      "        [ 2.1849,  3.5935, -9.1200]])\n",
      "b.grad= tensor([-0.0165, -0.0334])\n",
      "new w tensor([[-0.3802,  0.8657,  0.6499],\n",
      "        [-0.2711,  0.8325,  0.8199]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1474, grad_fn=<DivBackward0>)\n",
      "i= 498\n",
      "w.grad= tensor([[ 1.0135,  1.6667, -4.2292],\n",
      "        [ 2.1754,  3.5776, -9.0769]])\n",
      "b.grad= tensor([-0.0164, -0.0332])\n",
      "new w tensor([[-0.3803,  0.8656,  0.6501],\n",
      "        [-0.2712,  0.8324,  0.8203]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1413, grad_fn=<DivBackward0>)\n",
      "i= 499\n",
      "w.grad= tensor([[ 1.0091,  1.6593, -4.2092],\n",
      "        [ 2.1649,  3.5605, -9.0348]])\n",
      "b.grad= tensor([-0.0163, -0.0330])\n",
      "new w tensor([[-0.3803,  0.8655,  0.6503],\n",
      "        [-0.2714,  0.8322,  0.8208]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1352, grad_fn=<DivBackward0>)\n",
      "i= 500\n",
      "w.grad= tensor([[ 1.0039,  1.6510, -4.1899],\n",
      "        [ 2.1550,  3.5443, -8.9925]])\n",
      "b.grad= tensor([-0.0162, -0.0328])\n",
      "new w tensor([[-0.3804,  0.8654,  0.6505],\n",
      "        [-0.2715,  0.8320,  0.8212]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1292, grad_fn=<DivBackward0>)\n",
      "i= 501\n",
      "w.grad= tensor([[ 0.9995,  1.6436, -4.1702],\n",
      "        [ 2.1447,  3.5274, -8.9507]])\n",
      "b.grad= tensor([-0.0161, -0.0326])\n",
      "new w tensor([[-0.3804,  0.8654,  0.6508],\n",
      "        [-0.2716,  0.8318,  0.8217]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1233, grad_fn=<DivBackward0>)\n",
      "i= 502\n",
      "w.grad= tensor([[ 0.9946,  1.6358, -4.1508],\n",
      "        [ 2.1348,  3.5111, -8.9088]])\n",
      "b.grad= tensor([-0.0160, -0.0324])\n",
      "new w tensor([[-0.3805,  0.8653,  0.6510],\n",
      "        [-0.2717,  0.8316,  0.8221]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1174, grad_fn=<DivBackward0>)\n",
      "i= 503\n",
      "w.grad= tensor([[ 0.9899,  1.6280, -4.1315],\n",
      "        [ 2.1248,  3.4948, -8.8672]])\n",
      "b.grad= tensor([-0.0159, -0.0322])\n",
      "new w tensor([[-0.3805,  0.8652,  0.6512],\n",
      "        [-0.2718,  0.8315,  0.8226]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1115, grad_fn=<DivBackward0>)\n",
      "i= 504\n",
      "w.grad= tensor([[ 0.9850,  1.6201, -4.1124],\n",
      "        [ 2.1148,  3.4783, -8.8259]])\n",
      "b.grad= tensor([-0.0158, -0.0320])\n",
      "new w tensor([[-0.3806,  0.8651,  0.6514],\n",
      "        [-0.2719,  0.8313,  0.8230]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1057, grad_fn=<DivBackward0>)\n",
      "i= 505\n",
      "w.grad= tensor([[ 0.9811,  1.6133, -4.0928],\n",
      "        [ 2.1048,  3.4619, -8.7848]])\n",
      "b.grad= tensor([-0.0157, -0.0319])\n",
      "new w tensor([[-0.3806,  0.8650,  0.6516],\n",
      "        [-0.2720,  0.8311,  0.8235]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.1000, grad_fn=<DivBackward0>)\n",
      "i= 506\n",
      "w.grad= tensor([[ 0.9763,  1.6055, -4.0738],\n",
      "        [ 2.0950,  3.4457, -8.7438]])\n",
      "b.grad= tensor([-0.0156, -0.0317])\n",
      "new w tensor([[-0.3807,  0.8650,  0.6518],\n",
      "        [-0.2721,  0.8310,  0.8239]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0943, grad_fn=<DivBackward0>)\n",
      "i= 507\n",
      "w.grad= tensor([[ 0.9718,  1.5981, -4.0547],\n",
      "        [ 2.0857,  3.4302, -8.7026]])\n",
      "b.grad= tensor([-0.0155, -0.0315])\n",
      "new w tensor([[-0.3807,  0.8649,  0.6520],\n",
      "        [-0.2722,  0.8308,  0.8243]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0887, grad_fn=<DivBackward0>)\n",
      "i= 508\n",
      "w.grad= tensor([[ 0.9669,  1.5902, -4.0361],\n",
      "        [ 2.0762,  3.4143, -8.6620]])\n",
      "b.grad= tensor([-0.0155, -0.0313])\n",
      "new w tensor([[-0.3808,  0.8648,  0.6522],\n",
      "        [-0.2723,  0.8306,  0.8248]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0831, grad_fn=<DivBackward0>)\n",
      "i= 509\n",
      "w.grad= tensor([[ 0.9628,  1.5834, -4.0169],\n",
      "        [ 2.0658,  3.3978, -8.6219]])\n",
      "b.grad= tensor([-0.0154, -0.0311])\n",
      "new w tensor([[-0.3808,  0.8647,  0.6524],\n",
      "        [-0.2724,  0.8304,  0.8252]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0776, grad_fn=<DivBackward0>)\n",
      "i= 510\n",
      "w.grad= tensor([[ 0.9585,  1.5761, -3.9981],\n",
      "        [ 2.0570,  3.3828, -8.5812]])\n",
      "b.grad= tensor([-0.0153, -0.0309])\n",
      "new w tensor([[-0.3809,  0.8646,  0.6526],\n",
      "        [-0.2725,  0.8303,  0.8256]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0721, grad_fn=<DivBackward0>)\n",
      "i= 511\n",
      "w.grad= tensor([[ 0.9536,  1.5683, -3.9797],\n",
      "        [ 2.0463,  3.3659, -8.5418]])\n",
      "b.grad= tensor([-0.0152, -0.0307])\n",
      "new w tensor([[-0.3809,  0.8646,  0.6528],\n",
      "        [-0.2726,  0.8301,  0.8260]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0667, grad_fn=<DivBackward0>)\n",
      "i= 512\n",
      "w.grad= tensor([[ 0.9489,  1.5607, -3.9613],\n",
      "        [ 2.0368,  3.3502, -8.5019]])\n",
      "b.grad= tensor([-0.0151, -0.0305])\n",
      "new w tensor([[-0.3809,  0.8645,  0.6530],\n",
      "        [-0.2727,  0.8299,  0.8265]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0613, grad_fn=<DivBackward0>)\n",
      "i= 513\n",
      "w.grad= tensor([[ 0.9447,  1.5536, -3.9426],\n",
      "        [ 2.0280,  3.3353, -8.4618]])\n",
      "b.grad= tensor([-0.0150, -0.0303])\n",
      "new w tensor([[-0.3810,  0.8644,  0.6532],\n",
      "        [-0.2728,  0.8298,  0.8269]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0560, grad_fn=<DivBackward0>)\n",
      "i= 514\n",
      "w.grad= tensor([[ 0.9405,  1.5466, -3.9242],\n",
      "        [ 2.0179,  3.3190, -8.4227]])\n",
      "b.grad= tensor([-0.0149, -0.0301])\n",
      "new w tensor([[-0.3810,  0.8643,  0.6534],\n",
      "        [-0.2729,  0.8296,  0.8273]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0507, grad_fn=<DivBackward0>)\n",
      "i= 515\n",
      "w.grad= tensor([[ 0.9363,  1.5396, -3.9057],\n",
      "        [ 2.0086,  3.3036, -8.3833]])\n",
      "b.grad= tensor([-0.0148, -0.0299])\n",
      "new w tensor([[-0.3811,  0.8642,  0.6536],\n",
      "        [-0.2730,  0.8294,  0.8277]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0455, grad_fn=<DivBackward0>)\n",
      "i= 516\n",
      "w.grad= tensor([[ 0.9312,  1.5316, -3.8879],\n",
      "        [ 1.9994,  3.2884, -8.3441]])\n",
      "b.grad= tensor([-0.0148, -0.0298])\n",
      "new w tensor([[-0.3811,  0.8642,  0.6538],\n",
      "        [-0.2731,  0.8293,  0.8282]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0403, grad_fn=<DivBackward0>)\n",
      "i= 517\n",
      "w.grad= tensor([[ 0.9275,  1.5252, -3.8694],\n",
      "        [ 1.9909,  3.2739, -8.3046]])\n",
      "b.grad= tensor([-0.0147, -0.0296])\n",
      "new w tensor([[-0.3812,  0.8641,  0.6540],\n",
      "        [-0.2732,  0.8291,  0.8286]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0352, grad_fn=<DivBackward0>)\n",
      "i= 518\n",
      "w.grad= tensor([[ 0.9231,  1.5180, -3.8513],\n",
      "        [ 1.9810,  3.2580, -8.2662]])\n",
      "b.grad= tensor([-0.0146, -0.0294])\n",
      "new w tensor([[-0.3812,  0.8640,  0.6542],\n",
      "        [-0.2733,  0.8289,  0.8290]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0301, grad_fn=<DivBackward0>)\n",
      "i= 519\n",
      "w.grad= tensor([[ 0.9186,  1.5107, -3.8335],\n",
      "        [ 1.9712,  3.2423, -8.2280]])\n",
      "b.grad= tensor([-0.0145, -0.0292])\n",
      "new w tensor([[-0.3813,  0.8639,  0.6543],\n",
      "        [-0.2734,  0.8288,  0.8294]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0251, grad_fn=<DivBackward0>)\n",
      "i= 520\n",
      "w.grad= tensor([[ 0.9148,  1.5042, -3.8152],\n",
      "        [ 1.9625,  3.2278, -8.1893]])\n",
      "b.grad= tensor([-0.0144, -0.0290])\n",
      "new w tensor([[-0.3813,  0.8639,  0.6545],\n",
      "        [-0.2735,  0.8286,  0.8298]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0201, grad_fn=<DivBackward0>)\n",
      "i= 521\n",
      "w.grad= tensor([[ 0.9099,  1.4965, -3.7979],\n",
      "        [ 1.9527,  3.2119, -8.1515]])\n",
      "b.grad= tensor([-0.0143, -0.0288])\n",
      "new w tensor([[-0.3814,  0.8638,  0.6547],\n",
      "        [-0.2736,  0.8285,  0.8302]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0152, grad_fn=<DivBackward0>)\n",
      "i= 522\n",
      "w.grad= tensor([[ 0.9057,  1.4896, -3.7801],\n",
      "        [ 1.9448,  3.1983, -8.1126]])\n",
      "b.grad= tensor([-0.0142, -0.0287])\n",
      "new w tensor([[-0.3814,  0.8637,  0.6549],\n",
      "        [-0.2737,  0.8283,  0.8306]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0103, grad_fn=<DivBackward0>)\n",
      "i= 523\n",
      "w.grad= tensor([[ 0.9012,  1.4823, -3.7627],\n",
      "        [ 1.9349,  3.1824, -8.0753]])\n",
      "b.grad= tensor([-0.0142, -0.0285])\n",
      "new w tensor([[-0.3815,  0.8636,  0.6551],\n",
      "        [-0.2738,  0.8281,  0.8310]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0054, grad_fn=<DivBackward0>)\n",
      "i= 524\n",
      "w.grad= tensor([[ 0.8976,  1.4760, -3.7447],\n",
      "        [ 1.9255,  3.1672, -8.0378]])\n",
      "b.grad= tensor([-0.0141, -0.0283])\n",
      "new w tensor([[-0.3815,  0.8636,  0.6553],\n",
      "        [-0.2739,  0.8280,  0.8314]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(1.0006, grad_fn=<DivBackward0>)\n",
      "i= 525\n",
      "w.grad= tensor([[ 0.8933,  1.4692, -3.7272],\n",
      "        [ 1.9169,  3.1528, -8.0001]])\n",
      "b.grad= tensor([-0.0140, -0.0281])\n",
      "new w tensor([[-0.3815,  0.8635,  0.6555],\n",
      "        [-0.2740,  0.8278,  0.8318]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9959, grad_fn=<DivBackward0>)\n",
      "i= 526\n",
      "w.grad= tensor([[ 0.8892,  1.4624, -3.7098],\n",
      "        [ 1.9077,  3.1379, -7.9629]])\n",
      "b.grad= tensor([-0.0139, -0.0280])\n",
      "new w tensor([[-0.3816,  0.8634,  0.6557],\n",
      "        [-0.2741,  0.8277,  0.8322]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9912, grad_fn=<DivBackward0>)\n",
      "i= 527\n",
      "w.grad= tensor([[ 0.8847,  1.4552, -3.6927],\n",
      "        [ 1.8992,  3.1234, -7.9255]])\n",
      "b.grad= tensor([-0.0138, -0.0278])\n",
      "new w tensor([[-0.3816,  0.8634,  0.6558],\n",
      "        [-0.2742,  0.8275,  0.8326]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9865, grad_fn=<DivBackward0>)\n",
      "i= 528\n",
      "w.grad= tensor([[ 0.8811,  1.4490, -3.6751],\n",
      "        [ 1.8905,  3.1092, -7.8884]])\n",
      "b.grad= tensor([-0.0137, -0.0276])\n",
      "new w tensor([[-0.3817,  0.8633,  0.6560],\n",
      "        [-0.2743,  0.8274,  0.8330]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9819, grad_fn=<DivBackward0>)\n",
      "i= 529\n",
      "w.grad= tensor([[ 0.8768,  1.4420, -3.6581],\n",
      "        [ 1.8815,  3.0945, -7.8517]])\n",
      "b.grad= tensor([-0.0137, -0.0274])\n",
      "new w tensor([[-0.3817,  0.8632,  0.6562],\n",
      "        [-0.2744,  0.8272,  0.8334]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9773, grad_fn=<DivBackward0>)\n",
      "i= 530\n",
      "w.grad= tensor([[ 0.8724,  1.4350, -3.6412],\n",
      "        [ 1.8724,  3.0796, -7.8153]])\n",
      "b.grad= tensor([-0.0136, -0.0273])\n",
      "new w tensor([[-0.3818,  0.8631,  0.6564],\n",
      "        [-0.2745,  0.8271,  0.8338]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9728, grad_fn=<DivBackward0>)\n",
      "i= 531\n",
      "w.grad= tensor([[ 0.8679,  1.4277, -3.6246],\n",
      "        [ 1.8643,  3.0660, -7.7783]])\n",
      "b.grad= tensor([-0.0135, -0.0271])\n",
      "new w tensor([[-0.3818,  0.8631,  0.6566],\n",
      "        [-0.2746,  0.8269,  0.8342]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9683, grad_fn=<DivBackward0>)\n",
      "i= 532\n",
      "w.grad= tensor([[ 0.8644,  1.4216, -3.6073],\n",
      "        [ 1.8551,  3.0513, -7.7424]])\n",
      "b.grad= tensor([-0.0134, -0.0269])\n",
      "new w tensor([[-0.3818,  0.8630,  0.6568],\n",
      "        [-0.2747,  0.8267,  0.8346]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9638, grad_fn=<DivBackward0>)\n",
      "i= 533\n",
      "w.grad= tensor([[ 0.8597,  1.4142, -3.5909],\n",
      "        [ 1.8468,  3.0372, -7.7060]])\n",
      "b.grad= tensor([-0.0133, -0.0267])\n",
      "new w tensor([[-0.3819,  0.8629,  0.6569],\n",
      "        [-0.2747,  0.8266,  0.8350]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9594, grad_fn=<DivBackward0>)\n",
      "i= 534\n",
      "w.grad= tensor([[ 0.8568,  1.4088, -3.5734],\n",
      "        [ 1.8379,  3.0228, -7.6702]])\n",
      "b.grad= tensor([-0.0133, -0.0266])\n",
      "new w tensor([[-0.3819,  0.8629,  0.6571],\n",
      "        [-0.2748,  0.8264,  0.8353]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9550, grad_fn=<DivBackward0>)\n",
      "i= 535\n",
      "w.grad= tensor([[ 0.8522,  1.4015, -3.5572],\n",
      "        [ 1.8290,  3.0083, -7.6346]])\n",
      "b.grad= tensor([-0.0132, -0.0264])\n",
      "new w tensor([[-0.3820,  0.8628,  0.6573],\n",
      "        [-0.2749,  0.8263,  0.8357]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9507, grad_fn=<DivBackward0>)\n",
      "i= 536\n",
      "w.grad= tensor([[ 0.8486,  1.3954, -3.5403],\n",
      "        [ 1.8202,  2.9940, -7.5992]])\n",
      "b.grad= tensor([-0.0131, -0.0262])\n",
      "new w tensor([[-0.3820,  0.8627,  0.6575],\n",
      "        [-0.2750,  0.8261,  0.8361]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9464, grad_fn=<DivBackward0>)\n",
      "i= 537\n",
      "w.grad= tensor([[ 0.8441,  1.3884, -3.5241],\n",
      "        [ 1.8124,  2.9809, -7.5632]])\n",
      "b.grad= tensor([-0.0130, -0.0261])\n",
      "new w tensor([[-0.3821,  0.8626,  0.6576],\n",
      "        [-0.2751,  0.8260,  0.8365]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9421, grad_fn=<DivBackward0>)\n",
      "i= 538\n",
      "w.grad= tensor([[ 0.8406,  1.3823, -3.5074],\n",
      "        [ 1.8033,  2.9660, -7.5284]])\n",
      "b.grad= tensor([-0.0129, -0.0259])\n",
      "new w tensor([[-0.3821,  0.8626,  0.6578],\n",
      "        [-0.2752,  0.8258,  0.8369]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9379, grad_fn=<DivBackward0>)\n",
      "i= 539\n",
      "w.grad= tensor([[ 0.8365,  1.3758, -3.4911],\n",
      "        [ 1.7961,  2.9536, -7.4924]])\n",
      "b.grad= tensor([-0.0129, -0.0257])\n",
      "new w tensor([[-0.3821,  0.8625,  0.6580],\n",
      "        [-0.2753,  0.8257,  0.8372]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9338, grad_fn=<DivBackward0>)\n",
      "i= 540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([[ 0.8328,  1.3695, -3.4747],\n",
      "        [ 1.7869,  2.9390, -7.4580]])\n",
      "b.grad= tensor([-0.0128, -0.0256])\n",
      "new w tensor([[-0.3822,  0.8624,  0.6582],\n",
      "        [-0.2754,  0.8255,  0.8376]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9296, grad_fn=<DivBackward0>)\n",
      "i= 541\n",
      "w.grad= tensor([[ 0.8287,  1.3627, -3.4586],\n",
      "        [ 1.7787,  2.9254, -7.4231]])\n",
      "b.grad= tensor([-0.0127, -0.0254])\n",
      "new w tensor([[-0.3822,  0.8624,  0.6583],\n",
      "        [-0.2755,  0.8254,  0.8380]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9255, grad_fn=<DivBackward0>)\n",
      "i= 542\n",
      "w.grad= tensor([[ 0.8253,  1.3570, -3.4422],\n",
      "        [ 1.7705,  2.9120, -7.3884]])\n",
      "b.grad= tensor([-0.0126, -0.0252])\n",
      "new w tensor([[-0.3823,  0.8623,  0.6585],\n",
      "        [-0.2756,  0.8253,  0.8383]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9215, grad_fn=<DivBackward0>)\n",
      "i= 543\n",
      "w.grad= tensor([[ 0.8208,  1.3500, -3.4265],\n",
      "        [ 1.7624,  2.8985, -7.3539]])\n",
      "b.grad= tensor([-0.0126, -0.0251])\n",
      "new w tensor([[-0.3823,  0.8622,  0.6587],\n",
      "        [-0.2756,  0.8251,  0.8387]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9175, grad_fn=<DivBackward0>)\n",
      "i= 544\n",
      "w.grad= tensor([[ 0.8174,  1.3441, -3.4103],\n",
      "        [ 1.7542,  2.8851, -7.3195]])\n",
      "b.grad= tensor([-0.0125, -0.0249])\n",
      "new w tensor([[-0.3824,  0.8622,  0.6589],\n",
      "        [-0.2757,  0.8250,  0.8391]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9135, grad_fn=<DivBackward0>)\n",
      "i= 545\n",
      "w.grad= tensor([[ 0.8135,  1.3379, -3.3943],\n",
      "        [ 1.7457,  2.8711, -7.2855]])\n",
      "b.grad= tensor([-0.0124, -0.0247])\n",
      "new w tensor([[-0.3824,  0.8621,  0.6590],\n",
      "        [-0.2758,  0.8248,  0.8394]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9095, grad_fn=<DivBackward0>)\n",
      "i= 546\n",
      "w.grad= tensor([[ 0.8095,  1.3314, -3.3787],\n",
      "        [ 1.7380,  2.8583, -7.2512]])\n",
      "b.grad= tensor([-0.0123, -0.0246])\n",
      "new w tensor([[-0.3824,  0.8620,  0.6592],\n",
      "        [-0.2759,  0.8247,  0.8398]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9056, grad_fn=<DivBackward0>)\n",
      "i= 547\n",
      "w.grad= tensor([[ 0.8058,  1.3252, -3.3629],\n",
      "        [ 1.7296,  2.8446, -7.2176]])\n",
      "b.grad= tensor([-0.0123, -0.0244])\n",
      "new w tensor([[-0.3825,  0.8620,  0.6594],\n",
      "        [-0.2760,  0.8245,  0.8402]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.9017, grad_fn=<DivBackward0>)\n",
      "i= 548\n",
      "w.grad= tensor([[ 0.8015,  1.3183, -3.3475],\n",
      "        [ 1.7215,  2.8312, -7.1839]])\n",
      "b.grad= tensor([-0.0122, -0.0243])\n",
      "new w tensor([[-0.3825,  0.8619,  0.6595],\n",
      "        [-0.2761,  0.8244,  0.8405]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.8979, grad_fn=<DivBackward0>)\n",
      "i= 549\n",
      "w.grad= tensor([[ 0.7986,  1.3131, -3.3313],\n",
      "        [ 1.7133,  2.8179, -7.1505]])\n",
      "b.grad= tensor([-0.0121, -0.0241])\n",
      "new w tensor([[-0.3826,  0.8618,  0.6597],\n",
      "        [-0.2762,  0.8243,  0.8409]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.8941, grad_fn=<DivBackward0>)\n",
      "i= 550\n",
      "w.grad= tensor([[ 0.7944,  1.3065, -3.3161],\n",
      "        [ 1.7054,  2.8047, -7.1171]])\n",
      "b.grad= tensor([-0.0120, -0.0239])\n",
      "new w tensor([[-0.3826,  0.8618,  0.6599],\n",
      "        [-0.2763,  0.8241,  0.8412]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0159], requires_grad=True)\n",
      "Loss= tensor(0.8903, grad_fn=<DivBackward0>)\n",
      "i= 551\n",
      "w.grad= tensor([[ 0.7913,  1.3013, -3.3001],\n",
      "        [ 1.6976,  2.7919, -7.0837]])\n",
      "b.grad= tensor([-0.0120, -0.0238])\n",
      "new w tensor([[-0.3826,  0.8617,  0.6600],\n",
      "        [-0.2763,  0.8240,  0.8416]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8866, grad_fn=<DivBackward0>)\n",
      "i= 552\n",
      "w.grad= tensor([[ 0.7866,  1.2940, -3.2855],\n",
      "        [ 1.6892,  2.7783, -7.0510]])\n",
      "b.grad= tensor([-0.0119, -0.0236])\n",
      "new w tensor([[-0.3827,  0.8616,  0.6602],\n",
      "        [-0.2764,  0.8238,  0.8419]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8829, grad_fn=<DivBackward0>)\n",
      "i= 553\n",
      "w.grad= tensor([[ 0.7837,  1.2888, -3.2696],\n",
      "        [ 1.6818,  2.7658, -7.0178]])\n",
      "b.grad= tensor([-0.0118, -0.0235])\n",
      "new w tensor([[-0.3827,  0.8616,  0.6604],\n",
      "        [-0.2765,  0.8237,  0.8423]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8793, grad_fn=<DivBackward0>)\n",
      "i= 554\n",
      "w.grad= tensor([[ 0.7801,  1.2830, -3.2543],\n",
      "        [ 1.6735,  2.7524, -6.9853]])\n",
      "b.grad= tensor([-0.0117, -0.0233])\n",
      "new w tensor([[-0.3828,  0.8615,  0.6605],\n",
      "        [-0.2766,  0.8236,  0.8426]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8756, grad_fn=<DivBackward0>)\n",
      "i= 555\n",
      "w.grad= tensor([[ 0.7761,  1.2766, -3.2393],\n",
      "        [ 1.6664,  2.7404, -6.9522]])\n",
      "b.grad= tensor([-0.0117, -0.0231])\n",
      "new w tensor([[-0.3828,  0.8614,  0.6607],\n",
      "        [-0.2767,  0.8234,  0.8430]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8720, grad_fn=<DivBackward0>)\n",
      "i= 556\n",
      "w.grad= tensor([[ 0.7726,  1.2706, -3.2242],\n",
      "        [ 1.6582,  2.7273, -6.9201]])\n",
      "b.grad= tensor([-0.0116, -0.0230])\n",
      "new w tensor([[-0.3828,  0.8614,  0.6608],\n",
      "        [-0.2768,  0.8233,  0.8433]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8685, grad_fn=<DivBackward0>)\n",
      "i= 557\n",
      "w.grad= tensor([[ 0.7689,  1.2647, -3.2092],\n",
      "        [ 1.6501,  2.7141, -6.8880]])\n",
      "b.grad= tensor([-0.0115, -0.0228])\n",
      "new w tensor([[-0.3829,  0.8613,  0.6610],\n",
      "        [-0.2768,  0.8232,  0.8437]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8650, grad_fn=<DivBackward0>)\n",
      "i= 558\n",
      "w.grad= tensor([[ 0.7652,  1.2586, -3.1943],\n",
      "        [ 1.6431,  2.7021, -6.8554]])\n",
      "b.grad= tensor([-0.0115, -0.0227])\n",
      "new w tensor([[-0.3829,  0.8613,  0.6612],\n",
      "        [-0.2769,  0.8230,  0.8440]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8615, grad_fn=<DivBackward0>)\n",
      "i= 559\n",
      "w.grad= tensor([[ 0.7616,  1.2526, -3.1794],\n",
      "        [ 1.6350,  2.6891, -6.8237]])\n",
      "b.grad= tensor([-0.0114, -0.0225])\n",
      "new w tensor([[-0.3829,  0.8612,  0.6613],\n",
      "        [-0.2770,  0.8229,  0.8444]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8580, grad_fn=<DivBackward0>)\n",
      "i= 560\n",
      "w.grad= tensor([[ 0.7583,  1.2470, -3.1644],\n",
      "        [ 1.6271,  2.6761, -6.7920]])\n",
      "b.grad= tensor([-0.0113, -0.0224])\n",
      "new w tensor([[-0.3830,  0.8611,  0.6615],\n",
      "        [-0.2771,  0.8227,  0.8447]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8546, grad_fn=<DivBackward0>)\n",
      "i= 561\n",
      "w.grad= tensor([[ 0.7549,  1.2413, -3.1496],\n",
      "        [ 1.6201,  2.6644, -6.7599]])\n",
      "b.grad= tensor([-0.0112, -0.0222])\n",
      "new w tensor([[-0.3830,  0.8611,  0.6616],\n",
      "        [-0.2772,  0.8226,  0.8450]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8512, grad_fn=<DivBackward0>)\n",
      "i= 562\n",
      "w.grad= tensor([[ 0.7518,  1.2361, -3.1346],\n",
      "        [ 1.6123,  2.6518, -6.7285]])\n",
      "b.grad= tensor([-0.0112, -0.0221])\n",
      "new w tensor([[-0.3831,  0.8610,  0.6618],\n",
      "        [-0.2772,  0.8225,  0.8454]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8478, grad_fn=<DivBackward0>)\n",
      "i= 563\n",
      "w.grad= tensor([[ 0.7475,  1.2294, -3.1205],\n",
      "        [ 1.6052,  2.6398, -6.6968]])\n",
      "b.grad= tensor([-0.0111, -0.0219])\n",
      "new w tensor([[-0.3831,  0.8609,  0.6620],\n",
      "        [-0.2773,  0.8224,  0.8457]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8445, grad_fn=<DivBackward0>)\n",
      "i= 564\n",
      "w.grad= tensor([[ 0.7449,  1.2247, -3.1053],\n",
      "        [ 1.5972,  2.6269, -6.6659]])\n",
      "b.grad= tensor([-0.0110, -0.0218])\n",
      "new w tensor([[-0.3831,  0.8609,  0.6621],\n",
      "        [-0.2774,  0.8222,  0.8460]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8412, grad_fn=<DivBackward0>)\n",
      "i= 565\n",
      "w.grad= tensor([[ 0.7410,  1.2186, -3.0910],\n",
      "        [ 1.5901,  2.6150, -6.6345]])\n",
      "b.grad= tensor([-0.0110, -0.0216])\n",
      "new w tensor([[-0.3832,  0.8608,  0.6623],\n",
      "        [-0.2775,  0.8221,  0.8464]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8379, grad_fn=<DivBackward0>)\n",
      "i= 566\n",
      "w.grad= tensor([[ 0.7373,  1.2125, -3.0768],\n",
      "        [ 1.5817,  2.6018, -6.6042]])\n",
      "b.grad= tensor([-0.0109, -0.0215])\n",
      "new w tensor([[-0.3832,  0.8608,  0.6624],\n",
      "        [-0.2776,  0.8220,  0.8467]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8347, grad_fn=<DivBackward0>)\n",
      "i= 567\n",
      "w.grad= tensor([[ 0.7340,  1.2071, -3.0623],\n",
      "        [ 1.5748,  2.5900, -6.5731]])\n",
      "b.grad= tensor([-0.0108, -0.0214])\n",
      "new w tensor([[-0.3832,  0.8607,  0.6626],\n",
      "        [-0.2776,  0.8218,  0.8470]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8315, grad_fn=<DivBackward0>)\n",
      "i= 568\n",
      "w.grad= tensor([[ 0.7303,  1.2012, -3.0482],\n",
      "        [ 1.5679,  2.5786, -6.5421]])\n",
      "b.grad= tensor([-0.0108, -0.0212])\n",
      "new w tensor([[-0.3833,  0.8606,  0.6627],\n",
      "        [-0.2777,  0.8217,  0.8474]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8283, grad_fn=<DivBackward0>)\n",
      "i= 569\n",
      "w.grad= tensor([[ 0.7267,  1.1953, -3.0341],\n",
      "        [ 1.5600,  2.5660, -6.5119]])\n",
      "b.grad= tensor([-0.0107, -0.0211])\n",
      "new w tensor([[-0.3833,  0.8606,  0.6629],\n",
      "        [-0.2778,  0.8216,  0.8477]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8251, grad_fn=<DivBackward0>)\n",
      "i= 570\n",
      "w.grad= tensor([[ 0.7231,  1.1895, -3.0201],\n",
      "        [ 1.5537,  2.5550, -6.4809]])\n",
      "b.grad= tensor([-0.0106, -0.0209])\n",
      "new w tensor([[-0.3834,  0.8605,  0.6630],\n",
      "        [-0.2779,  0.8214,  0.8480]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8220, grad_fn=<DivBackward0>)\n",
      "i= 571\n",
      "w.grad= tensor([[ 0.7202,  1.1844, -3.0057],\n",
      "        [ 1.5459,  2.5425, -6.4510]])\n",
      "b.grad= tensor([-0.0106, -0.0208])\n",
      "new w tensor([[-0.3834,  0.8605,  0.6632],\n",
      "        [-0.2780,  0.8213,  0.8483]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8189, grad_fn=<DivBackward0>)\n",
      "i= 572\n",
      "w.grad= tensor([[ 0.7166,  1.1787, -2.9918],\n",
      "        [ 1.5388,  2.5308, -6.4208]])\n",
      "b.grad= tensor([-0.0105, -0.0206])\n",
      "new w tensor([[-0.3834,  0.8604,  0.6633],\n",
      "        [-0.2780,  0.8212,  0.8487]], requires_grad=True)\n",
      "new b tensor([-1.1850, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8158, grad_fn=<DivBackward0>)\n",
      "i= 573\n",
      "w.grad= tensor([[ 0.7139,  1.1738, -2.9775],\n",
      "        [ 1.5312,  2.5186, -6.3910]])\n",
      "b.grad= tensor([-0.0104, -0.0205])\n",
      "new w tensor([[-0.3835,  0.8603,  0.6635],\n",
      "        [-0.2781,  0.8211,  0.8490]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8128, grad_fn=<DivBackward0>)\n",
      "i= 574\n",
      "w.grad= tensor([[ 0.7101,  1.1679, -2.9638],\n",
      "        [ 1.5242,  2.5069, -6.3612]])\n",
      "b.grad= tensor([-0.0104, -0.0203])\n",
      "new w tensor([[-0.3835,  0.8603,  0.6636],\n",
      "        [-0.2782,  0.8209,  0.8493]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8098, grad_fn=<DivBackward0>)\n",
      "i= 575\n",
      "w.grad= tensor([[ 0.7067,  1.1622, -2.9501],\n",
      "        [ 1.5173,  2.4955, -6.3314]])\n",
      "b.grad= tensor([-0.0103, -0.0202])\n",
      "new w tensor([[-0.3835,  0.8602,  0.6638],\n",
      "        [-0.2783,  0.8208,  0.8496]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8068, grad_fn=<DivBackward0>)\n",
      "i= 576\n",
      "w.grad= tensor([[ 0.7036,  1.1570, -2.9362],\n",
      "        [ 1.5095,  2.4830, -6.3023]])\n",
      "b.grad= tensor([-0.0102, -0.0201])\n",
      "new w tensor([[-0.3836,  0.8602,  0.6639],\n",
      "        [-0.2783,  0.8207,  0.8499]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8039, grad_fn=<DivBackward0>)\n",
      "i= 577\n",
      "w.grad= tensor([[ 0.7000,  1.1513, -2.9227],\n",
      "        [ 1.5031,  2.4721, -6.2725]])\n",
      "b.grad= tensor([-0.0102, -0.0199])\n",
      "new w tensor([[-0.3836,  0.8601,  0.6641],\n",
      "        [-0.2784,  0.8206,  0.8502]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.8009, grad_fn=<DivBackward0>)\n",
      "i= 578\n",
      "w.grad= tensor([[ 0.6971,  1.1463, -2.9088],\n",
      "        [ 1.4956,  2.4600, -6.2435]])\n",
      "b.grad= tensor([-0.0101, -0.0198])\n",
      "new w tensor([[-0.3836,  0.8601,  0.6642],\n",
      "        [-0.2785,  0.8204,  0.8506]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7981, grad_fn=<DivBackward0>)\n",
      "i= 579\n",
      "w.grad= tensor([[ 0.6942,  1.1414, -2.8950],\n",
      "        [ 1.4891,  2.4490, -6.2141]])\n",
      "b.grad= tensor([-0.0100, -0.0196])\n",
      "new w tensor([[-0.3837,  0.8600,  0.6644],\n",
      "        [-0.2786,  0.8203,  0.8509]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7952, grad_fn=<DivBackward0>)\n",
      "i= 580\n",
      "w.grad= tensor([[ 0.6901,  1.1351, -2.8821],\n",
      "        [ 1.4826,  2.4382, -6.1847]])\n",
      "b.grad= tensor([-0.0100, -0.0195])\n",
      "new w tensor([[-0.3837,  0.8599,  0.6645],\n",
      "        [-0.2786,  0.8202,  0.8512]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7923, grad_fn=<DivBackward0>)\n",
      "i= 581\n",
      "w.grad= tensor([[ 0.6874,  1.1303, -2.8683],\n",
      "        [ 1.4753,  2.4262, -6.1562]])\n",
      "b.grad= tensor([-0.0099, -0.0194])\n",
      "new w tensor([[-0.3837,  0.8599,  0.6646],\n",
      "        [-0.2787,  0.8201,  0.8515]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7895, grad_fn=<DivBackward0>)\n",
      "i= 582\n",
      "w.grad= tensor([[ 0.6844,  1.1254, -2.8547],\n",
      "        [ 1.4683,  2.4149, -6.1274]])\n",
      "b.grad= tensor([-0.0098, -0.0192])\n",
      "new w tensor([[-0.3838,  0.8598,  0.6648],\n",
      "        [-0.2788,  0.8200,  0.8518]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7867, grad_fn=<DivBackward0>)\n",
      "i= 583\n",
      "w.grad= tensor([[ 0.6812,  1.1201, -2.8414],\n",
      "        [ 1.4614,  2.4036, -6.0989]])\n",
      "b.grad= tensor([-0.0098, -0.0191])\n",
      "new w tensor([[-0.3838,  0.8598,  0.6649],\n",
      "        [-0.2789,  0.8198,  0.8521]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7840, grad_fn=<DivBackward0>)\n",
      "i= 584\n",
      "w.grad= tensor([[ 0.6775,  1.1143, -2.8285],\n",
      "        [ 1.4549,  2.3928, -6.0702]])\n",
      "b.grad= tensor([-0.0097, -0.0190])\n",
      "new w tensor([[-0.3838,  0.8597,  0.6651],\n",
      "        [-0.2789,  0.8197,  0.8524]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7812, grad_fn=<DivBackward0>)\n",
      "i= 585\n",
      "w.grad= tensor([[ 0.6745,  1.1093, -2.8152],\n",
      "        [ 1.4476,  2.3812, -6.0422]])\n",
      "b.grad= tensor([-0.0097, -0.0188])\n",
      "new w tensor([[-0.3839,  0.8597,  0.6652],\n",
      "        [-0.2790,  0.8196,  0.8527]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7785, grad_fn=<DivBackward0>)\n",
      "i= 586\n",
      "w.grad= tensor([[ 0.6713,  1.1040, -2.8021],\n",
      "        [ 1.4414,  2.3705, -6.0137]])\n",
      "b.grad= tensor([-0.0096, -0.0187])\n",
      "new w tensor([[-0.3839,  0.8596,  0.6653],\n",
      "        [-0.2791,  0.8195,  0.8530]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7758, grad_fn=<DivBackward0>)\n",
      "i= 587\n",
      "w.grad= tensor([[ 0.6687,  1.0995, -2.7886],\n",
      "        [ 1.4348,  2.3597, -5.9854]])\n",
      "b.grad= tensor([-0.0095, -0.0186])\n",
      "new w tensor([[-0.3839,  0.8596,  0.6655],\n",
      "        [-0.2791,  0.8194,  0.8533]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7732, grad_fn=<DivBackward0>)\n",
      "i= 588\n",
      "w.grad= tensor([[ 0.6653,  1.0940, -2.7758],\n",
      "        [ 1.4275,  2.3479, -5.9580]])\n",
      "b.grad= tensor([-0.0095, -0.0184])\n",
      "new w tensor([[-0.3840,  0.8595,  0.6656],\n",
      "        [-0.2792,  0.8192,  0.8536]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7705, grad_fn=<DivBackward0>)\n",
      "i= 589\n",
      "w.grad= tensor([[ 0.6619,  1.0885, -2.7631],\n",
      "        [ 1.4211,  2.3373, -5.9300]])\n",
      "b.grad= tensor([-0.0094, -0.0183])\n",
      "new w tensor([[-0.3840,  0.8594,  0.6658],\n",
      "        [-0.2793,  0.8191,  0.8539]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7679, grad_fn=<DivBackward0>)\n",
      "i= 590\n",
      "w.grad= tensor([[ 0.6594,  1.0842, -2.7498],\n",
      "        [ 1.4143,  2.3264, -5.9023]])\n",
      "b.grad= tensor([-0.0093, -0.0182])\n",
      "new w tensor([[-0.3840,  0.8594,  0.6659],\n",
      "        [-0.2794,  0.8190,  0.8542]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7653, grad_fn=<DivBackward0>)\n",
      "i= 591\n",
      "w.grad= tensor([[ 0.6556,  1.0782, -2.7374],\n",
      "        [ 1.4076,  2.3153, -5.8749]])\n",
      "b.grad= tensor([-0.0093, -0.0180])\n",
      "new w tensor([[-0.3841,  0.8593,  0.6660],\n",
      "        [-0.2794,  0.8189,  0.8545]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7628, grad_fn=<DivBackward0>)\n",
      "i= 592\n",
      "w.grad= tensor([[ 0.6531,  1.0738, -2.7243],\n",
      "        [ 1.4009,  2.3043, -5.8476]])\n",
      "b.grad= tensor([-0.0092, -0.0179])\n",
      "new w tensor([[-0.3841,  0.8593,  0.6662],\n",
      "        [-0.2795,  0.8188,  0.8548]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7602, grad_fn=<DivBackward0>)\n",
      "i= 593\n",
      "w.grad= tensor([[ 0.6499,  1.0687, -2.7117],\n",
      "        [ 1.3944,  2.2935, -5.8203]])\n",
      "b.grad= tensor([-0.0092, -0.0178])\n",
      "new w tensor([[-0.3841,  0.8592,  0.6663],\n",
      "        [-0.2796,  0.8187,  0.8551]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7577, grad_fn=<DivBackward0>)\n",
      "i= 594\n",
      "w.grad= tensor([[ 0.6472,  1.0641, -2.6988],\n",
      "        [ 1.3883,  2.2832, -5.7929]])\n",
      "b.grad= tensor([-0.0091, -0.0176])\n",
      "new w tensor([[-0.3842,  0.8592,  0.6664],\n",
      "        [-0.2796,  0.8186,  0.8554]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7552, grad_fn=<DivBackward0>)\n",
      "i= 595\n",
      "w.grad= tensor([[ 0.6438,  1.0586, -2.6864],\n",
      "        [ 1.3818,  2.2726, -5.7659]])\n",
      "b.grad= tensor([-0.0090, -0.0175])\n",
      "new w tensor([[-0.3842,  0.8591,  0.6666],\n",
      "        [-0.2797,  0.8184,  0.8556]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7527, grad_fn=<DivBackward0>)\n",
      "i= 596\n",
      "w.grad= tensor([[ 0.6410,  1.0541, -2.6737],\n",
      "        [ 1.3755,  2.2622, -5.7388]])\n",
      "b.grad= tensor([-0.0090, -0.0174])\n",
      "new w tensor([[-0.3842,  0.8591,  0.6667],\n",
      "        [-0.2798,  0.8183,  0.8559]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7503, grad_fn=<DivBackward0>)\n",
      "i= 597\n",
      "w.grad= tensor([[ 0.6380,  1.0491, -2.6613],\n",
      "        [ 1.3686,  2.2511, -5.7123]])\n",
      "b.grad= tensor([-0.0089, -0.0173])\n",
      "new w tensor([[-0.3843,  0.8590,  0.6668],\n",
      "        [-0.2798,  0.8182,  0.8562]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7479, grad_fn=<DivBackward0>)\n",
      "i= 598\n",
      "w.grad= tensor([[ 0.6347,  1.0438, -2.6491],\n",
      "        [ 1.3627,  2.2412, -5.6853]])\n",
      "b.grad= tensor([-0.0089, -0.0171])\n",
      "new w tensor([[-0.3843,  0.8590,  0.6670],\n",
      "        [-0.2799,  0.8181,  0.8565]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7455, grad_fn=<DivBackward0>)\n",
      "i= 599\n",
      "w.grad= tensor([[ 0.6317,  1.0389, -2.6367],\n",
      "        [ 1.3556,  2.2298, -5.6593]])\n",
      "b.grad= tensor([-0.0088, -0.0170])\n",
      "new w tensor([[-0.3843,  0.8589,  0.6671],\n",
      "        [-0.2800,  0.8180,  0.8568]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7431, grad_fn=<DivBackward0>)\n",
      "i= 600\n",
      "w.grad= tensor([[ 0.6285,  1.0338, -2.6246],\n",
      "        [ 1.3500,  2.2203, -5.6324]])\n",
      "b.grad= tensor([-0.0088, -0.0169])\n",
      "new w tensor([[-0.3844,  0.8589,  0.6672],\n",
      "        [-0.2800,  0.8179,  0.8571]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7407, grad_fn=<DivBackward0>)\n",
      "i= 601\n",
      "w.grad= tensor([[ 0.6262,  1.0296, -2.6119],\n",
      "        [ 1.3433,  2.2095, -5.6064]])\n",
      "b.grad= tensor([-0.0087, -0.0168])\n",
      "new w tensor([[-0.3844,  0.8588,  0.6674],\n",
      "        [-0.2801,  0.8178,  0.8573]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7384, grad_fn=<DivBackward0>)\n",
      "i= 602\n",
      "w.grad= tensor([[ 0.6228,  1.0244, -2.6000],\n",
      "        [ 1.3368,  2.1990, -5.5804]])\n",
      "b.grad= tensor([-0.0086, -0.0166])\n",
      "new w tensor([[-0.3844,  0.8588,  0.6675],\n",
      "        [-0.2802,  0.8177,  0.8576]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7361, grad_fn=<DivBackward0>)\n",
      "i= 603\n",
      "w.grad= tensor([[ 0.6200,  1.0197, -2.5878],\n",
      "        [ 1.3308,  2.1888, -5.5542]])\n",
      "b.grad= tensor([-0.0086, -0.0165])\n",
      "new w tensor([[-0.3845,  0.8587,  0.6676],\n",
      "        [-0.2802,  0.8175,  0.8579]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7338, grad_fn=<DivBackward0>)\n",
      "i= 604\n",
      "w.grad= tensor([[ 0.6176,  1.0155, -2.5754],\n",
      "        [ 1.3249,  2.1791, -5.5281]])\n",
      "b.grad= tensor([-0.0085, -0.0164])\n",
      "new w tensor([[-0.3845,  0.8587,  0.6678],\n",
      "        [-0.2803,  0.8174,  0.8582]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7315, grad_fn=<DivBackward0>)\n",
      "i= 605\n",
      "w.grad= tensor([[ 0.6145,  1.0104, -2.5636],\n",
      "        [ 1.3187,  2.1688, -5.5023]])\n",
      "b.grad= tensor([-0.0085, -0.0163])\n",
      "new w tensor([[-0.3845,  0.8586,  0.6679],\n",
      "        [-0.2804,  0.8173,  0.8585]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7293, grad_fn=<DivBackward0>)\n",
      "i= 606\n",
      "w.grad= tensor([[ 0.6109,  1.0049, -2.5521],\n",
      "        [ 1.3122,  2.1583, -5.4768]])\n",
      "b.grad= tensor([-0.0084, -0.0161])\n",
      "new w tensor([[-0.3845,  0.8586,  0.6680],\n",
      "        [-0.2804,  0.8172,  0.8587]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7270, grad_fn=<DivBackward0>)\n",
      "i= 607\n",
      "w.grad= tensor([[ 0.6088,  1.0011, -2.5397],\n",
      "        [ 1.3060,  2.1481, -5.4513]])\n",
      "b.grad= tensor([-0.0083, -0.0160])\n",
      "new w tensor([[-0.3846,  0.8585,  0.6681],\n",
      "        [-0.2805,  0.8171,  0.8590]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7248, grad_fn=<DivBackward0>)\n",
      "i= 608\n",
      "w.grad= tensor([[ 0.6057,  0.9961, -2.5280],\n",
      "        [ 1.2999,  2.1381, -5.4258]])\n",
      "b.grad= tensor([-0.0083, -0.0159])\n",
      "new w tensor([[-0.3846,  0.8585,  0.6683],\n",
      "        [-0.2806,  0.8170,  0.8593]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7226, grad_fn=<DivBackward0>)\n",
      "i= 609\n",
      "w.grad= tensor([[ 0.6028,  0.9914, -2.5163],\n",
      "        [ 1.2941,  2.1284, -5.4004]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.grad= tensor([-0.0082, -0.0158])\n",
      "new w tensor([[-0.3846,  0.8584,  0.6684],\n",
      "        [-0.2806,  0.8169,  0.8595]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7205, grad_fn=<DivBackward0>)\n",
      "i= 610\n",
      "w.grad= tensor([[ 0.6000,  0.9866, -2.5046],\n",
      "        [ 1.2882,  2.1188, -5.3750]])\n",
      "b.grad= tensor([-0.0082, -0.0157])\n",
      "new w tensor([[-0.3847,  0.8584,  0.6685],\n",
      "        [-0.2807,  0.8168,  0.8598]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7183, grad_fn=<DivBackward0>)\n",
      "i= 611\n",
      "w.grad= tensor([[ 0.5972,  0.9821, -2.4929],\n",
      "        [ 1.2819,  2.1086, -5.3501]])\n",
      "b.grad= tensor([-0.0081, -0.0155])\n",
      "new w tensor([[-0.3847,  0.8583,  0.6686],\n",
      "        [-0.2808,  0.8167,  0.8601]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7162, grad_fn=<DivBackward0>)\n",
      "i= 612\n",
      "w.grad= tensor([[ 0.5949,  0.9780, -2.4809],\n",
      "        [ 1.2760,  2.0987, -5.3252]])\n",
      "b.grad= tensor([-0.0081, -0.0154])\n",
      "new w tensor([[-0.3847,  0.8583,  0.6688],\n",
      "        [-0.2808,  0.8166,  0.8603]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7141, grad_fn=<DivBackward0>)\n",
      "i= 613\n",
      "w.grad= tensor([[ 0.5919,  0.9733, -2.4694],\n",
      "        [ 1.2700,  2.0889, -5.3003]])\n",
      "b.grad= tensor([-0.0080, -0.0153])\n",
      "new w tensor([[-0.3848,  0.8582,  0.6689],\n",
      "        [-0.2809,  0.8165,  0.8606]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7120, grad_fn=<DivBackward0>)\n",
      "i= 614\n",
      "w.grad= tensor([[ 0.5894,  0.9690, -2.4577],\n",
      "        [ 1.2640,  2.0791, -5.2756]])\n",
      "b.grad= tensor([-0.0080, -0.0152])\n",
      "new w tensor([[-0.3848,  0.8582,  0.6690],\n",
      "        [-0.2810,  0.8164,  0.8609]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7099, grad_fn=<DivBackward0>)\n",
      "i= 615\n",
      "w.grad= tensor([[ 0.5861,  0.9640, -2.4466],\n",
      "        [ 1.2582,  2.0694, -5.2509]])\n",
      "b.grad= tensor([-0.0079, -0.0151])\n",
      "new w tensor([[-0.3848,  0.8581,  0.6691],\n",
      "        [-0.2810,  0.8163,  0.8611]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7079, grad_fn=<DivBackward0>)\n",
      "i= 616\n",
      "w.grad= tensor([[ 0.5836,  0.9597, -2.4350],\n",
      "        [ 1.2525,  2.0600, -5.2263]])\n",
      "b.grad= tensor([-0.0078, -0.0150])\n",
      "new w tensor([[-0.3848,  0.8581,  0.6693],\n",
      "        [-0.2811,  0.8162,  0.8614]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7058, grad_fn=<DivBackward0>)\n",
      "i= 617\n",
      "w.grad= tensor([[ 0.5810,  0.9553, -2.4235],\n",
      "        [ 1.2465,  2.0502, -5.2020]])\n",
      "b.grad= tensor([-0.0078, -0.0148])\n",
      "new w tensor([[-0.3849,  0.8580,  0.6694],\n",
      "        [-0.2811,  0.8161,  0.8617]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7038, grad_fn=<DivBackward0>)\n",
      "i= 618\n",
      "w.grad= tensor([[ 0.5782,  0.9508, -2.4123],\n",
      "        [ 1.2408,  2.0407, -5.1777]])\n",
      "b.grad= tensor([-0.0077, -0.0147])\n",
      "new w tensor([[-0.3849,  0.8580,  0.6695],\n",
      "        [-0.2812,  0.8160,  0.8619]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.7018, grad_fn=<DivBackward0>)\n",
      "i= 619\n",
      "w.grad= tensor([[ 0.5751,  0.9459, -2.4013],\n",
      "        [ 1.2341,  2.0302, -5.1540]])\n",
      "b.grad= tensor([-0.0077, -0.0146])\n",
      "new w tensor([[-0.3849,  0.8579,  0.6696],\n",
      "        [-0.2813,  0.8159,  0.8622]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6999, grad_fn=<DivBackward0>)\n",
      "i= 620\n",
      "w.grad= tensor([[ 0.5731,  0.9422, -2.3897],\n",
      "        [ 1.2296,  2.0222, -5.1292]])\n",
      "b.grad= tensor([-0.0076, -0.0145])\n",
      "new w tensor([[-0.3850,  0.8579,  0.6697],\n",
      "        [-0.2813,  0.8158,  0.8624]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6979, grad_fn=<DivBackward0>)\n",
      "i= 621\n",
      "w.grad= tensor([[ 0.5703,  0.9377, -2.3786],\n",
      "        [ 1.2234,  2.0123, -5.1055]])\n",
      "b.grad= tensor([-0.0076, -0.0144])\n",
      "new w tensor([[-0.3850,  0.8578,  0.6699],\n",
      "        [-0.2814,  0.8157,  0.8627]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6960, grad_fn=<DivBackward0>)\n",
      "i= 622\n",
      "w.grad= tensor([[ 0.5670,  0.9327, -2.3679],\n",
      "        [ 1.2180,  2.0033, -5.0815]])\n",
      "b.grad= tensor([-0.0075, -0.0143])\n",
      "new w tensor([[-0.3850,  0.8578,  0.6700],\n",
      "        [-0.2815,  0.8156,  0.8629]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6941, grad_fn=<DivBackward0>)\n",
      "i= 623\n",
      "w.grad= tensor([[ 0.5649,  0.9289, -2.3566],\n",
      "        [ 1.2118,  1.9932, -5.0581]])\n",
      "b.grad= tensor([-0.0075, -0.0142])\n",
      "new w tensor([[-0.3850,  0.8577,  0.6701],\n",
      "        [-0.2815,  0.8155,  0.8632]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6922, grad_fn=<DivBackward0>)\n",
      "i= 624\n",
      "w.grad= tensor([[ 0.5621,  0.9244, -2.3456],\n",
      "        [ 1.2065,  1.9846, -5.0343]])\n",
      "b.grad= tensor([-0.0074, -0.0140])\n",
      "new w tensor([[-0.3851,  0.8577,  0.6702],\n",
      "        [-0.2816,  0.8154,  0.8634]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6903, grad_fn=<DivBackward0>)\n",
      "i= 625\n",
      "w.grad= tensor([[ 0.5598,  0.9204, -2.3345],\n",
      "        [ 1.2009,  1.9751, -5.0108]])\n",
      "b.grad= tensor([-0.0074, -0.0139])\n",
      "new w tensor([[-0.3851,  0.8576,  0.6703],\n",
      "        [-0.2816,  0.8153,  0.8637]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6884, grad_fn=<DivBackward0>)\n",
      "i= 626\n",
      "w.grad= tensor([[ 0.5569,  0.9159, -2.3237],\n",
      "        [ 1.1945,  1.9651, -4.9879]])\n",
      "b.grad= tensor([-0.0073, -0.0138])\n",
      "new w tensor([[-0.3851,  0.8576,  0.6704],\n",
      "        [-0.2817,  0.8152,  0.8639]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6866, grad_fn=<DivBackward0>)\n",
      "i= 627\n",
      "w.grad= tensor([[ 0.5543,  0.9116, -2.3129],\n",
      "        [ 1.1896,  1.9566, -4.9642]])\n",
      "b.grad= tensor([-0.0073, -0.0137])\n",
      "new w tensor([[-0.3852,  0.8576,  0.6706],\n",
      "        [-0.2818,  0.8151,  0.8642]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6847, grad_fn=<DivBackward0>)\n",
      "i= 628\n",
      "w.grad= tensor([[ 0.5516,  0.9073, -2.3022],\n",
      "        [ 1.1843,  1.9478, -4.9409]])\n",
      "b.grad= tensor([-0.0072, -0.0136])\n",
      "new w tensor([[-0.3852,  0.8575,  0.6707],\n",
      "        [-0.2818,  0.8150,  0.8644]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6829, grad_fn=<DivBackward0>)\n",
      "i= 629\n",
      "w.grad= tensor([[ 0.5489,  0.9027, -2.2916],\n",
      "        [ 1.1785,  1.9384, -4.9180]])\n",
      "b.grad= tensor([-0.0072, -0.0135])\n",
      "new w tensor([[-0.3852,  0.8575,  0.6708],\n",
      "        [-0.2819,  0.8149,  0.8647]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6811, grad_fn=<DivBackward0>)\n",
      "i= 630\n",
      "w.grad= tensor([[ 0.5468,  0.8992, -2.2805],\n",
      "        [ 1.1724,  1.9287, -4.8954]])\n",
      "b.grad= tensor([-0.0071, -0.0134])\n",
      "new w tensor([[-0.3852,  0.8574,  0.6709],\n",
      "        [-0.2819,  0.8148,  0.8649]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6793, grad_fn=<DivBackward0>)\n",
      "i= 631\n",
      "w.grad= tensor([[ 0.5441,  0.8948, -2.2700],\n",
      "        [ 1.1679,  1.9208, -4.8719]])\n",
      "b.grad= tensor([-0.0071, -0.0133])\n",
      "new w tensor([[-0.3853,  0.8574,  0.6710],\n",
      "        [-0.2820,  0.8147,  0.8652]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6776, grad_fn=<DivBackward0>)\n",
      "i= 632\n",
      "w.grad= tensor([[ 0.5412,  0.8902, -2.2596],\n",
      "        [ 1.1618,  1.9110, -4.8496]])\n",
      "b.grad= tensor([-0.0070, -0.0132])\n",
      "new w tensor([[-0.3853,  0.8573,  0.6711],\n",
      "        [-0.2820,  0.8146,  0.8654]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6758, grad_fn=<DivBackward0>)\n",
      "i= 633\n",
      "w.grad= tensor([[ 0.5390,  0.8864, -2.2488],\n",
      "        [ 1.1568,  1.9026, -4.8268]])\n",
      "b.grad= tensor([-0.0070, -0.0131])\n",
      "new w tensor([[-0.3853,  0.8573,  0.6712],\n",
      "        [-0.2821,  0.8145,  0.8657]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6741, grad_fn=<DivBackward0>)\n",
      "i= 634\n",
      "w.grad= tensor([[ 0.5362,  0.8819, -2.2385],\n",
      "        [ 1.1510,  1.8932, -4.8045]])\n",
      "b.grad= tensor([-0.0069, -0.0130])\n",
      "new w tensor([[-0.3853,  0.8572,  0.6714],\n",
      "        [-0.2822,  0.8144,  0.8659]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6724, grad_fn=<DivBackward0>)\n",
      "i= 635\n",
      "w.grad= tensor([[ 0.5338,  0.8780, -2.2280],\n",
      "        [ 1.1458,  1.8847, -4.7819]])\n",
      "b.grad= tensor([-0.0069, -0.0128])\n",
      "new w tensor([[-0.3854,  0.8572,  0.6715],\n",
      "        [-0.2822,  0.8143,  0.8661]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6707, grad_fn=<DivBackward0>)\n",
      "i= 636\n",
      "w.grad= tensor([[ 0.5312,  0.8737, -2.2177],\n",
      "        [ 1.1406,  1.8759, -4.7595]])\n",
      "b.grad= tensor([-0.0068, -0.0127])\n",
      "new w tensor([[-0.3854,  0.8572,  0.6716],\n",
      "        [-0.2823,  0.8142,  0.8664]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6690, grad_fn=<DivBackward0>)\n",
      "i= 637\n",
      "w.grad= tensor([[ 0.5287,  0.8696, -2.2074],\n",
      "        [ 1.1351,  1.8671, -4.7373]])\n",
      "b.grad= tensor([-0.0068, -0.0126])\n",
      "new w tensor([[-0.3854,  0.8571,  0.6717],\n",
      "        [-0.2823,  0.8141,  0.8666]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6673, grad_fn=<DivBackward0>)\n",
      "i= 638\n",
      "w.grad= tensor([[ 0.5268,  0.8661, -2.1967],\n",
      "        [ 1.1298,  1.8581, -4.7153]])\n",
      "b.grad= tensor([-0.0067, -0.0125])\n",
      "new w tensor([[-0.3855,  0.8571,  0.6718],\n",
      "        [-0.2824,  0.8140,  0.8668]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6657, grad_fn=<DivBackward0>)\n",
      "i= 639\n",
      "w.grad= tensor([[ 0.5240,  0.8618, -2.1867],\n",
      "        [ 1.1244,  1.8493, -4.6934]])\n",
      "b.grad= tensor([-0.0067, -0.0124])\n",
      "new w tensor([[-0.3855,  0.8570,  0.6719],\n",
      "        [-0.2824,  0.8139,  0.8671]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6640, grad_fn=<DivBackward0>)\n",
      "i= 640\n",
      "w.grad= tensor([[ 0.5213,  0.8575, -2.1766],\n",
      "        [ 1.1199,  1.8416, -4.6710]])\n",
      "b.grad= tensor([-0.0066, -0.0123])\n",
      "new w tensor([[-0.3855,  0.8570,  0.6720],\n",
      "        [-0.2825,  0.8138,  0.8673]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6624, grad_fn=<DivBackward0>)\n",
      "i= 641\n",
      "w.grad= tensor([[ 0.5193,  0.8540, -2.1661],\n",
      "        [ 1.1138,  1.8320, -4.6498]])\n",
      "b.grad= tensor([-0.0066, -0.0122])\n",
      "new w tensor([[-0.3855,  0.8569,  0.6721],\n",
      "        [-0.2826,  0.8137,  0.8675]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6608, grad_fn=<DivBackward0>)\n",
      "i= 642\n",
      "w.grad= tensor([[ 0.5167,  0.8497, -2.1562],\n",
      "        [ 1.1090,  1.8240, -4.6278]])\n",
      "b.grad= tensor([-0.0065, -0.0121])\n",
      "new w tensor([[-0.3856,  0.8569,  0.6722],\n",
      "        [-0.2826,  0.8137,  0.8678]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6592, grad_fn=<DivBackward0>)\n",
      "i= 643\n",
      "w.grad= tensor([[ 0.5143,  0.8458, -2.1461],\n",
      "        [ 1.1037,  1.8154, -4.6063]])\n",
      "b.grad= tensor([-0.0065, -0.0120])\n",
      "new w tensor([[-0.3856,  0.8569,  0.6723],\n",
      "        [-0.2827,  0.8136,  0.8680]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6576, grad_fn=<DivBackward0>)\n",
      "i= 644\n",
      "w.grad= tensor([[ 0.5118,  0.8417, -2.1362],\n",
      "        [ 1.0985,  1.8070, -4.5848]])\n",
      "b.grad= tensor([-0.0064, -0.0119])\n",
      "new w tensor([[-0.3856,  0.8568,  0.6724],\n",
      "        [-0.2827,  0.8135,  0.8682]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6561, grad_fn=<DivBackward0>)\n",
      "i= 645\n",
      "w.grad= tensor([[ 0.5093,  0.8377, -2.1262],\n",
      "        [ 1.0933,  1.7984, -4.5635]])\n",
      "b.grad= tensor([-0.0064, -0.0118])\n",
      "new w tensor([[-0.3856,  0.8568,  0.6726],\n",
      "        [-0.2828,  0.8134,  0.8685]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6545, grad_fn=<DivBackward0>)\n",
      "i= 646\n",
      "w.grad= tensor([[ 0.5072,  0.8340, -2.1162],\n",
      "        [ 1.0880,  1.7897, -4.5423]])\n",
      "b.grad= tensor([-0.0063, -0.0117])\n",
      "new w tensor([[-0.3857,  0.8567,  0.6727],\n",
      "        [-0.2828,  0.8133,  0.8687]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6530, grad_fn=<DivBackward0>)\n",
      "i= 647\n",
      "w.grad= tensor([[ 0.5048,  0.8302, -2.1063],\n",
      "        [ 1.0835,  1.7820, -4.5208]])\n",
      "b.grad= tensor([-0.0063, -0.0116])\n",
      "new w tensor([[-0.3857,  0.8567,  0.6728],\n",
      "        [-0.2829,  0.8132,  0.8689]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6515, grad_fn=<DivBackward0>)\n",
      "i= 648\n",
      "w.grad= tensor([[ 0.5026,  0.8265, -2.0963],\n",
      "        [ 1.0779,  1.7732, -4.5000]])\n",
      "b.grad= tensor([-0.0062, -0.0115])\n",
      "new w tensor([[-0.3857,  0.8566,  0.6729],\n",
      "        [-0.2829,  0.8131,  0.8691]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6500, grad_fn=<DivBackward0>)\n",
      "i= 649\n",
      "w.grad= tensor([[ 0.5001,  0.8224, -2.0867],\n",
      "        [ 1.0733,  1.7653, -4.4787]])\n",
      "b.grad= tensor([-0.0062, -0.0114])\n",
      "new w tensor([[-0.3857,  0.8566,  0.6730],\n",
      "        [-0.2830,  0.8130,  0.8694]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6485, grad_fn=<DivBackward0>)\n",
      "i= 650\n",
      "w.grad= tensor([[ 0.4975,  0.8184, -2.0771],\n",
      "        [ 1.0681,  1.7568, -4.4580]])\n",
      "b.grad= tensor([-0.0062, -0.0113])\n",
      "new w tensor([[-0.3858,  0.8566,  0.6731],\n",
      "        [-0.2830,  0.8129,  0.8696]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6470, grad_fn=<DivBackward0>)\n",
      "i= 651\n",
      "w.grad= tensor([[ 0.4951,  0.8144, -2.0675],\n",
      "        [ 1.0630,  1.7484, -4.4372]])\n",
      "b.grad= tensor([-0.0061, -0.0112])\n",
      "new w tensor([[-0.3858,  0.8565,  0.6732],\n",
      "        [-0.2831,  0.8129,  0.8698]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6455, grad_fn=<DivBackward0>)\n",
      "i= 652\n",
      "w.grad= tensor([[ 0.4928,  0.8105, -2.0578],\n",
      "        [ 1.0582,  1.7404, -4.4164]])\n",
      "b.grad= tensor([-0.0061, -0.0111])\n",
      "new w tensor([[-0.3858,  0.8565,  0.6733],\n",
      "        [-0.2832,  0.8128,  0.8700]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6441, grad_fn=<DivBackward0>)\n",
      "i= 653\n",
      "w.grad= tensor([[ 0.4909,  0.8073, -2.0479],\n",
      "        [ 1.0533,  1.7323, -4.3958]])\n",
      "b.grad= tensor([-0.0060, -0.0110])\n",
      "new w tensor([[-0.3858,  0.8564,  0.6734],\n",
      "        [-0.2832,  0.8127,  0.8703]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6427, grad_fn=<DivBackward0>)\n",
      "i= 654\n",
      "w.grad= tensor([[ 0.4886,  0.8035, -2.0384],\n",
      "        [ 1.0482,  1.7240, -4.3754]])\n",
      "b.grad= tensor([-0.0060, -0.0109])\n",
      "new w tensor([[-0.3859,  0.8564,  0.6735],\n",
      "        [-0.2833,  0.8126,  0.8705]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6412, grad_fn=<DivBackward0>)\n",
      "i= 655\n",
      "w.grad= tensor([[ 0.4864,  0.7999, -2.0288],\n",
      "        [ 1.0434,  1.7161, -4.3549]])\n",
      "b.grad= tensor([-0.0059, -0.0108])\n",
      "new w tensor([[-0.3859,  0.8564,  0.6736],\n",
      "        [-0.2833,  0.8125,  0.8707]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6398, grad_fn=<DivBackward0>)\n",
      "i= 656\n",
      "w.grad= tensor([[ 0.4838,  0.7957, -2.0196],\n",
      "        [ 1.0383,  1.7077, -4.3348]])\n",
      "b.grad= tensor([-0.0059, -0.0107])\n",
      "new w tensor([[-0.3859,  0.8563,  0.6737],\n",
      "        [-0.2834,  0.8124,  0.8709]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6384, grad_fn=<DivBackward0>)\n",
      "i= 657\n",
      "w.grad= tensor([[ 0.4812,  0.7919, -2.0103],\n",
      "        [ 1.0335,  1.7000, -4.3144]])\n",
      "b.grad= tensor([-0.0058, -0.0106])\n",
      "new w tensor([[-0.3859,  0.8563,  0.6738],\n",
      "        [-0.2834,  0.8123,  0.8711]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6370, grad_fn=<DivBackward0>)\n",
      "i= 658\n",
      "w.grad= tensor([[ 0.4796,  0.7888, -2.0005],\n",
      "        [ 1.0291,  1.6925, -4.2940]])\n",
      "b.grad= tensor([-0.0058, -0.0105])\n",
      "new w tensor([[-0.3860,  0.8562,  0.6739],\n",
      "        [-0.2835,  0.8123,  0.8713]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6357, grad_fn=<DivBackward0>)\n",
      "i= 659\n",
      "w.grad= tensor([[ 0.4770,  0.7847, -1.9914],\n",
      "        [ 1.0237,  1.6839, -4.2744]])\n",
      "b.grad= tensor([-0.0057, -0.0104])\n",
      "new w tensor([[-0.3860,  0.8562,  0.6740],\n",
      "        [-0.2835,  0.8122,  0.8715]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6343, grad_fn=<DivBackward0>)\n",
      "i= 660\n",
      "w.grad= tensor([[ 0.4747,  0.7809, -1.9822],\n",
      "        [ 1.0191,  1.6762, -4.2543]])\n",
      "b.grad= tensor([-0.0057, -0.0103])\n",
      "new w tensor([[-0.3860,  0.8562,  0.6741],\n",
      "        [-0.2836,  0.8121,  0.8718]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6330, grad_fn=<DivBackward0>)\n",
      "i= 661\n",
      "w.grad= tensor([[ 0.4730,  0.7778, -1.9726],\n",
      "        [ 1.0146,  1.6687, -4.2343]])\n",
      "b.grad= tensor([-0.0057, -0.0102])\n",
      "new w tensor([[-0.3860,  0.8561,  0.6742],\n",
      "        [-0.2836,  0.8120,  0.8720]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6316, grad_fn=<DivBackward0>)\n",
      "i= 662\n",
      "w.grad= tensor([[ 0.4709,  0.7744, -1.9633],\n",
      "        [ 1.0100,  1.6611, -4.2144]])\n",
      "b.grad= tensor([-0.0056, -0.0102])\n",
      "new w tensor([[-0.3860,  0.8561,  0.6743],\n",
      "        [-0.2837,  0.8119,  0.8722]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6303, grad_fn=<DivBackward0>)\n",
      "i= 663\n",
      "w.grad= tensor([[ 0.4682,  0.7702, -1.9545],\n",
      "        [ 1.0057,  1.6537, -4.1945]])\n",
      "b.grad= tensor([-0.0056, -0.0101])\n",
      "new w tensor([[-0.3861,  0.8560,  0.6744],\n",
      "        [-0.2837,  0.8118,  0.8724]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6290, grad_fn=<DivBackward0>)\n",
      "i= 664\n",
      "w.grad= tensor([[ 0.4662,  0.7668, -1.9452],\n",
      "        [ 1.0005,  1.6454, -4.1752]])\n",
      "b.grad= tensor([-0.0055, -0.0100])\n",
      "new w tensor([[-0.3861,  0.8560,  0.6745],\n",
      "        [-0.2838,  0.8118,  0.8726]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6277, grad_fn=<DivBackward0>)\n",
      "i= 665\n",
      "w.grad= tensor([[ 0.4643,  0.7635, -1.9359],\n",
      "        [ 0.9961,  1.6381, -4.1555]])\n",
      "b.grad= tensor([-0.0055, -0.0099])\n",
      "new w tensor([[-0.3861,  0.8560,  0.6746],\n",
      "        [-0.2838,  0.8117,  0.8728]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6264, grad_fn=<DivBackward0>)\n",
      "i= 666\n",
      "w.grad= tensor([[ 0.4618,  0.7595, -1.9271],\n",
      "        [ 0.9913,  1.6303, -4.1362]])\n",
      "b.grad= tensor([-0.0054, -0.0098])\n",
      "new w tensor([[-0.3861,  0.8559,  0.6747],\n",
      "        [-0.2839,  0.8116,  0.8730]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6252, grad_fn=<DivBackward0>)\n",
      "i= 667\n",
      "w.grad= tensor([[ 0.4598,  0.7562, -1.9180],\n",
      "        [ 0.9863,  1.6221, -4.1172]])\n",
      "b.grad= tensor([-0.0054, -0.0097])\n",
      "new w tensor([[-0.3862,  0.8559,  0.6748],\n",
      "        [-0.2839,  0.8115,  0.8732]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6239, grad_fn=<DivBackward0>)\n",
      "i= 668\n",
      "w.grad= tensor([[ 0.4575,  0.7525, -1.9092],\n",
      "        [ 0.9823,  1.6155, -4.0975]])\n",
      "b.grad= tensor([-0.0054, -0.0096])\n",
      "new w tensor([[-0.3862,  0.8559,  0.6749],\n",
      "        [-0.2840,  0.8114,  0.8734]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6226, grad_fn=<DivBackward0>)\n",
      "i= 669\n",
      "w.grad= tensor([[ 0.4556,  0.7493, -1.9001],\n",
      "        [ 0.9769,  1.6070, -4.0790]])\n",
      "b.grad= tensor([-0.0053, -0.0095])\n",
      "new w tensor([[-0.3862,  0.8558,  0.6750],\n",
      "        [-0.2840,  0.8113,  0.8736]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6214, grad_fn=<DivBackward0>)\n",
      "i= 670\n",
      "w.grad= tensor([[ 0.4531,  0.7452, -1.8915],\n",
      "        [ 0.9728,  1.5999, -4.0597]])\n",
      "b.grad= tensor([-0.0053, -0.0094])\n",
      "new w tensor([[-0.3862,  0.8558,  0.6751],\n",
      "        [-0.2841,  0.8113,  0.8738]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6202, grad_fn=<DivBackward0>)\n",
      "i= 671\n",
      "w.grad= tensor([[ 0.4514,  0.7423, -1.8824],\n",
      "        [ 0.9689,  1.5932, -4.0403]])\n",
      "b.grad= tensor([-0.0052, -0.0093])\n",
      "new w tensor([[-0.3863,  0.8557,  0.6751],\n",
      "        [-0.2841,  0.8112,  0.8740]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6190, grad_fn=<DivBackward0>)\n",
      "i= 672\n",
      "w.grad= tensor([[ 0.4489,  0.7384, -1.8739],\n",
      "        [ 0.9635,  1.5849, -4.0220]])\n",
      "b.grad= tensor([-0.0052, -0.0092])\n",
      "new w tensor([[-0.3863,  0.8557,  0.6752],\n",
      "        [-0.2842,  0.8111,  0.8742]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6178, grad_fn=<DivBackward0>)\n",
      "i= 673\n",
      "w.grad= tensor([[ 0.4470,  0.7351, -1.8651],\n",
      "        [ 0.9593,  1.5777, -4.0031]])\n",
      "b.grad= tensor([-0.0051, -0.0091])\n",
      "new w tensor([[-0.3863,  0.8557,  0.6753],\n",
      "        [-0.2842,  0.8110,  0.8744]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6166, grad_fn=<DivBackward0>)\n",
      "i= 674\n",
      "w.grad= tensor([[ 0.4449,  0.7316, -1.8563],\n",
      "        [ 0.9549,  1.5705, -3.9843]])\n",
      "b.grad= tensor([-0.0051, -0.0091])\n",
      "new w tensor([[-0.3863,  0.8556,  0.6754],\n",
      "        [-0.2843,  0.8110,  0.8746]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6154, grad_fn=<DivBackward0>)\n",
      "i= 675\n",
      "w.grad= tensor([[ 0.4427,  0.7282, -1.8477],\n",
      "        [ 0.9500,  1.5627, -3.9660]])\n",
      "b.grad= tensor([-0.0051, -0.0090])\n",
      "new w tensor([[-0.3863,  0.8556,  0.6755],\n",
      "        [-0.2843,  0.8109,  0.8748]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6142, grad_fn=<DivBackward0>)\n",
      "i= 676\n",
      "w.grad= tensor([[ 0.4405,  0.7246, -1.8392],\n",
      "        [ 0.9458,  1.5557, -3.9474]])\n",
      "b.grad= tensor([-0.0050, -0.0089])\n",
      "new w tensor([[-0.3864,  0.8556,  0.6756],\n",
      "        [-0.2843,  0.8108,  0.8750]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6131, grad_fn=<DivBackward0>)\n",
      "i= 677\n",
      "w.grad= tensor([[ 0.4388,  0.7216, -1.8304],\n",
      "        [ 0.9413,  1.5483, -3.9290]])\n",
      "b.grad= tensor([-0.0050, -0.0088])\n",
      "new w tensor([[-0.3864,  0.8555,  0.6757],\n",
      "        [-0.2844,  0.8107,  0.8752]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6119, grad_fn=<DivBackward0>)\n",
      "i= 678\n",
      "w.grad= tensor([[ 0.4363,  0.7177, -1.8222],\n",
      "        [ 0.9366,  1.5407, -3.9109]])\n",
      "b.grad= tensor([-0.0049, -0.0087])\n",
      "new w tensor([[-0.3864,  0.8555,  0.6758],\n",
      "        [-0.2844,  0.8106,  0.8754]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6108, grad_fn=<DivBackward0>)\n",
      "i= 679\n",
      "w.grad= tensor([[ 0.4347,  0.7148, -1.8134],\n",
      "        [ 0.9338,  1.5352, -3.8916]])\n",
      "b.grad= tensor([-0.0049, -0.0086])\n",
      "new w tensor([[-0.3864,  0.8555,  0.6759],\n",
      "        [-0.2845,  0.8106,  0.8756]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.6097, grad_fn=<DivBackward0>)\n",
      "i= 680\n",
      "w.grad= tensor([[ 0.4326,  0.7115, -1.8049],\n",
      "        [ 0.9280,  1.5265, -3.8744]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.grad= tensor([-0.0049, -0.0085])\n",
      "new w tensor([[-0.3865,  0.8554,  0.6760],\n",
      "        [-0.2845,  0.8105,  0.8758]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.6085, grad_fn=<DivBackward0>)\n",
      "i= 681\n",
      "w.grad= tensor([[ 0.4302,  0.7077, -1.7968],\n",
      "        [ 0.9236,  1.5193, -3.8563]])\n",
      "b.grad= tensor([-0.0048, -0.0085])\n",
      "new w tensor([[-0.3865,  0.8554,  0.6761],\n",
      "        [-0.2846,  0.8104,  0.8760]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.6074, grad_fn=<DivBackward0>)\n",
      "i= 682\n",
      "w.grad= tensor([[ 0.4282,  0.7043, -1.7884],\n",
      "        [ 0.9197,  1.5127, -3.8381]])\n",
      "b.grad= tensor([-0.0048, -0.0084])\n",
      "new w tensor([[-0.3865,  0.8553,  0.6762],\n",
      "        [-0.2846,  0.8103,  0.8762]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.6064, grad_fn=<DivBackward0>)\n",
      "i= 683\n",
      "w.grad= tensor([[ 0.4265,  0.7015, -1.7798],\n",
      "        [ 0.9155,  1.5057, -3.8201]])\n",
      "b.grad= tensor([-0.0047, -0.0083])\n",
      "new w tensor([[-0.3865,  0.8553,  0.6762],\n",
      "        [-0.2847,  0.8103,  0.8764]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.6053, grad_fn=<DivBackward0>)\n",
      "i= 684\n",
      "w.grad= tensor([[ 0.4245,  0.6981, -1.7716],\n",
      "        [ 0.9113,  1.4987, -3.8022]])\n",
      "b.grad= tensor([-0.0047, -0.0082])\n",
      "new w tensor([[-0.3865,  0.8553,  0.6763],\n",
      "        [-0.2847,  0.8102,  0.8766]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.6042, grad_fn=<DivBackward0>)\n",
      "i= 685\n",
      "w.grad= tensor([[ 0.4228,  0.6952, -1.7631],\n",
      "        [ 0.9069,  1.4915, -3.7846]])\n",
      "b.grad= tensor([-0.0047, -0.0081])\n",
      "new w tensor([[-0.3866,  0.8552,  0.6764],\n",
      "        [-0.2848,  0.8101,  0.8768]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.6031, grad_fn=<DivBackward0>)\n",
      "i= 686\n",
      "w.grad= tensor([[ 0.4206,  0.6917, -1.7551],\n",
      "        [ 0.9027,  1.4848, -3.7668]])\n",
      "b.grad= tensor([-0.0046, -0.0080])\n",
      "new w tensor([[-0.3866,  0.8552,  0.6765],\n",
      "        [-0.2848,  0.8100,  0.8770]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.6021, grad_fn=<DivBackward0>)\n",
      "i= 687\n",
      "w.grad= tensor([[ 0.4187,  0.6886, -1.7467],\n",
      "        [ 0.8981,  1.4773, -3.7495]])\n",
      "b.grad= tensor([-0.0046, -0.0079])\n",
      "new w tensor([[-0.3866,  0.8552,  0.6766],\n",
      "        [-0.2849,  0.8100,  0.8771]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.6010, grad_fn=<DivBackward0>)\n",
      "i= 688\n",
      "w.grad= tensor([[ 0.4163,  0.6849, -1.7389],\n",
      "        [ 0.8941,  1.4706, -3.7319]])\n",
      "b.grad= tensor([-0.0045, -0.0079])\n",
      "new w tensor([[-0.3866,  0.8551,  0.6767],\n",
      "        [-0.2849,  0.8099,  0.8773]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.6000, grad_fn=<DivBackward0>)\n",
      "i= 689\n",
      "w.grad= tensor([[ 0.4146,  0.6820, -1.7306],\n",
      "        [ 0.8900,  1.4638, -3.7144]])\n",
      "b.grad= tensor([-0.0045, -0.0078])\n",
      "new w tensor([[-0.3866,  0.8551,  0.6768],\n",
      "        [-0.2849,  0.8098,  0.8775]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5990, grad_fn=<DivBackward0>)\n",
      "i= 690\n",
      "w.grad= tensor([[ 0.4127,  0.6788, -1.7225],\n",
      "        [ 0.8859,  1.4570, -3.6971]])\n",
      "b.grad= tensor([-0.0045, -0.0077])\n",
      "new w tensor([[-0.3867,  0.8551,  0.6769],\n",
      "        [-0.2850,  0.8097,  0.8777]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5979, grad_fn=<DivBackward0>)\n",
      "i= 691\n",
      "w.grad= tensor([[ 0.4108,  0.6756, -1.7145],\n",
      "        [ 0.8817,  1.4501, -3.6799]])\n",
      "b.grad= tensor([-0.0044, -0.0076])\n",
      "new w tensor([[-0.3867,  0.8550,  0.6769],\n",
      "        [-0.2850,  0.8097,  0.8779]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5969, grad_fn=<DivBackward0>)\n",
      "i= 692\n",
      "w.grad= tensor([[ 0.4091,  0.6727, -1.7063],\n",
      "        [ 0.8779,  1.4438, -3.6625]])\n",
      "b.grad= tensor([-0.0044, -0.0075])\n",
      "new w tensor([[-0.3867,  0.8550,  0.6770],\n",
      "        [-0.2851,  0.8096,  0.8781]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5959, grad_fn=<DivBackward0>)\n",
      "i= 693\n",
      "w.grad= tensor([[ 0.4071,  0.6694, -1.6984],\n",
      "        [ 0.8733,  1.4365, -3.6456]])\n",
      "b.grad= tensor([-0.0044, -0.0075])\n",
      "new w tensor([[-0.3867,  0.8550,  0.6771],\n",
      "        [-0.2851,  0.8095,  0.8783]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5950, grad_fn=<DivBackward0>)\n",
      "i= 694\n",
      "w.grad= tensor([[ 0.4048,  0.6659, -1.6908],\n",
      "        [ 0.8696,  1.4303, -3.6284]])\n",
      "b.grad= tensor([-0.0043, -0.0074])\n",
      "new w tensor([[-0.3867,  0.8549,  0.6772],\n",
      "        [-0.2852,  0.8095,  0.8784]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5940, grad_fn=<DivBackward0>)\n",
      "i= 695\n",
      "w.grad= tensor([[ 0.4031,  0.6630, -1.6827],\n",
      "        [ 0.8653,  1.4234, -3.6116]])\n",
      "b.grad= tensor([-0.0043, -0.0073])\n",
      "new w tensor([[-0.3868,  0.8549,  0.6773],\n",
      "        [-0.2852,  0.8094,  0.8786]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5930, grad_fn=<DivBackward0>)\n",
      "i= 696\n",
      "w.grad= tensor([[ 0.4014,  0.6600, -1.6748],\n",
      "        [ 0.8613,  1.4166, -3.5948]])\n",
      "b.grad= tensor([-0.0042, -0.0072])\n",
      "new w tensor([[-0.3868,  0.8549,  0.6774],\n",
      "        [-0.2852,  0.8093,  0.8788]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5921, grad_fn=<DivBackward0>)\n",
      "i= 697\n",
      "w.grad= tensor([[ 0.3998,  0.6573, -1.6668],\n",
      "        [ 0.8577,  1.4105, -3.5777]])\n",
      "b.grad= tensor([-0.0042, -0.0071])\n",
      "new w tensor([[-0.3868,  0.8548,  0.6774],\n",
      "        [-0.2853,  0.8092,  0.8790]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5911, grad_fn=<DivBackward0>)\n",
      "i= 698\n",
      "w.grad= tensor([[ 0.3973,  0.6534, -1.6595],\n",
      "        [ 0.8534,  1.4038, -3.5612]])\n",
      "b.grad= tensor([-0.0042, -0.0071])\n",
      "new w tensor([[-0.3868,  0.8548,  0.6775],\n",
      "        [-0.2853,  0.8092,  0.8791]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5902, grad_fn=<DivBackward0>)\n",
      "i= 699\n",
      "w.grad= tensor([[ 0.3960,  0.6510, -1.6514],\n",
      "        [ 0.8493,  1.3969, -3.5446]])\n",
      "b.grad= tensor([-0.0041, -0.0070])\n",
      "new w tensor([[-0.3868,  0.8548,  0.6776],\n",
      "        [-0.2854,  0.8091,  0.8793]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5892, grad_fn=<DivBackward0>)\n",
      "i= 700\n",
      "w.grad= tensor([[ 0.3939,  0.6479, -1.6438],\n",
      "        [ 0.8450,  1.3900, -3.5283]])\n",
      "b.grad= tensor([-0.0041, -0.0069])\n",
      "new w tensor([[-0.3869,  0.8547,  0.6777],\n",
      "        [-0.2854,  0.8090,  0.8795]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5883, grad_fn=<DivBackward0>)\n",
      "i= 701\n",
      "w.grad= tensor([[ 0.3923,  0.6450, -1.6360],\n",
      "        [ 0.8415,  1.3841, -3.5116]])\n",
      "b.grad= tensor([-0.0041, -0.0068])\n",
      "new w tensor([[-0.3869,  0.8547,  0.6778],\n",
      "        [-0.2855,  0.8090,  0.8797]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5874, grad_fn=<DivBackward0>)\n",
      "i= 702\n",
      "w.grad= tensor([[ 0.3902,  0.6419, -1.6284],\n",
      "        [ 0.8372,  1.3771, -3.4954]])\n",
      "b.grad= tensor([-0.0040, -0.0067])\n",
      "new w tensor([[-0.3869,  0.8547,  0.6779],\n",
      "        [-0.2855,  0.8089,  0.8799]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5865, grad_fn=<DivBackward0>)\n",
      "i= 703\n",
      "w.grad= tensor([[ 0.3882,  0.6386, -1.6210],\n",
      "        [ 0.8337,  1.3712, -3.4788]])\n",
      "b.grad= tensor([-0.0040, -0.0067])\n",
      "new w tensor([[-0.3869,  0.8546,  0.6779],\n",
      "        [-0.2855,  0.8088,  0.8800]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5856, grad_fn=<DivBackward0>)\n",
      "i= 704\n",
      "w.grad= tensor([[ 0.3868,  0.6360, -1.6132],\n",
      "        [ 0.8294,  1.3642, -3.4629]])\n",
      "b.grad= tensor([-0.0039, -0.0066])\n",
      "new w tensor([[-0.3869,  0.8546,  0.6780],\n",
      "        [-0.2856,  0.8088,  0.8802]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5847, grad_fn=<DivBackward0>)\n",
      "i= 705\n",
      "w.grad= tensor([[ 0.3851,  0.6332, -1.6056],\n",
      "        [ 0.8260,  1.3585, -3.4464]])\n",
      "b.grad= tensor([-0.0039, -0.0065])\n",
      "new w tensor([[-0.3870,  0.8546,  0.6781],\n",
      "        [-0.2856,  0.8087,  0.8804]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5838, grad_fn=<DivBackward0>)\n",
      "i= 706\n",
      "w.grad= tensor([[ 0.3830,  0.6299, -1.5983],\n",
      "        [ 0.8219,  1.3520, -3.4305]])\n",
      "b.grad= tensor([-0.0039, -0.0064])\n",
      "new w tensor([[-0.3870,  0.8545,  0.6782],\n",
      "        [-0.2857,  0.8086,  0.8805]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5829, grad_fn=<DivBackward0>)\n",
      "i= 707\n",
      "w.grad= tensor([[ 0.3812,  0.6270, -1.5908],\n",
      "        [ 0.8184,  1.3461, -3.4142]])\n",
      "b.grad= tensor([-0.0038, -0.0064])\n",
      "new w tensor([[-0.3870,  0.8545,  0.6783],\n",
      "        [-0.2857,  0.8086,  0.8807]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5821, grad_fn=<DivBackward0>)\n",
      "i= 708\n",
      "w.grad= tensor([[ 0.3792,  0.6238, -1.5835],\n",
      "        [ 0.8143,  1.3393, -3.3985]])\n",
      "b.grad= tensor([-0.0038, -0.0063])\n",
      "new w tensor([[-0.3870,  0.8545,  0.6783],\n",
      "        [-0.2858,  0.8085,  0.8809]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5812, grad_fn=<DivBackward0>)\n",
      "i= 709\n",
      "w.grad= tensor([[ 0.3777,  0.6213, -1.5759],\n",
      "        [ 0.8102,  1.3329, -3.3828]])\n",
      "b.grad= tensor([-0.0038, -0.0062])\n",
      "new w tensor([[-0.3870,  0.8545,  0.6784],\n",
      "        [-0.2858,  0.8084,  0.8811]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5803, grad_fn=<DivBackward0>)\n",
      "i= 710\n",
      "w.grad= tensor([[ 0.3760,  0.6184, -1.5686],\n",
      "        [ 0.8068,  1.3271, -3.3668]])\n",
      "b.grad= tensor([-0.0037, -0.0061])\n",
      "new w tensor([[-0.3871,  0.8544,  0.6785],\n",
      "        [-0.2858,  0.8084,  0.8812]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5795, grad_fn=<DivBackward0>)\n",
      "i= 711\n",
      "w.grad= tensor([[ 0.3739,  0.6151, -1.5615],\n",
      "        [ 0.8033,  1.3212, -3.3509]])\n",
      "b.grad= tensor([-0.0037, -0.0061])\n",
      "new w tensor([[-0.3871,  0.8544,  0.6786],\n",
      "        [-0.2859,  0.8083,  0.8814]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5787, grad_fn=<DivBackward0>)\n",
      "i= 712\n",
      "w.grad= tensor([[ 0.3725,  0.6126, -1.5540],\n",
      "        [ 0.7993,  1.3148, -3.3354]])\n",
      "b.grad= tensor([-0.0037, -0.0060])\n",
      "new w tensor([[-0.3871,  0.8544,  0.6787],\n",
      "        [-0.2859,  0.8082,  0.8816]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5778, grad_fn=<DivBackward0>)\n",
      "i= 713\n",
      "w.grad= tensor([[ 0.3706,  0.6096, -1.5468],\n",
      "        [ 0.7950,  1.3081, -3.3202]])\n",
      "b.grad= tensor([-0.0036, -0.0059])\n",
      "new w tensor([[-0.3871,  0.8543,  0.6787],\n",
      "        [-0.2860,  0.8082,  0.8817]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5770, grad_fn=<DivBackward0>)\n",
      "i= 714\n",
      "w.grad= tensor([[ 0.3691,  0.6069, -1.5395],\n",
      "        [ 0.7918,  1.3025, -3.3043]])\n",
      "b.grad= tensor([-0.0036, -0.0058])\n",
      "new w tensor([[-0.3871,  0.8543,  0.6788],\n",
      "        [-0.2860,  0.8081,  0.8819]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5762, grad_fn=<DivBackward0>)\n",
      "i= 715\n",
      "w.grad= tensor([[ 0.3674,  0.6042, -1.5323],\n",
      "        [ 0.7884,  1.2967, -3.2888]])\n",
      "b.grad= tensor([-0.0036, -0.0058])\n",
      "new w tensor([[-0.3872,  0.8543,  0.6789],\n",
      "        [-0.2860,  0.8080,  0.8821]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5754, grad_fn=<DivBackward0>)\n",
      "i= 716\n",
      "w.grad= tensor([[ 0.3653,  0.6009, -1.5253],\n",
      "        [ 0.7841,  1.2900, -3.2739]])\n",
      "b.grad= tensor([-0.0035, -0.0057])\n",
      "new w tensor([[-0.3872,  0.8542,  0.6790],\n",
      "        [-0.2861,  0.8080,  0.8822]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5746, grad_fn=<DivBackward0>)\n",
      "i= 717\n",
      "w.grad= tensor([[ 0.3639,  0.5985, -1.5180],\n",
      "        [ 0.7803,  1.2839, -3.2586]])\n",
      "b.grad= tensor([-0.0035, -0.0056])\n",
      "new w tensor([[-0.3872,  0.8542,  0.6790],\n",
      "        [-0.2861,  0.8079,  0.8824]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5738, grad_fn=<DivBackward0>)\n",
      "i= 718\n",
      "w.grad= tensor([[ 0.3619,  0.5953, -1.5112],\n",
      "        [ 0.7768,  1.2778, -3.2434]])\n",
      "b.grad= tensor([-0.0035, -0.0055])\n",
      "new w tensor([[-0.3872,  0.8542,  0.6791],\n",
      "        [-0.2861,  0.8078,  0.8825]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5730, grad_fn=<DivBackward0>)\n",
      "i= 719\n",
      "w.grad= tensor([[ 0.3607,  0.5931, -1.5037],\n",
      "        [ 0.7736,  1.2723, -3.2280]])\n",
      "b.grad= tensor([-0.0034, -0.0055])\n",
      "new w tensor([[-0.3872,  0.8542,  0.6792],\n",
      "        [-0.2862,  0.8078,  0.8827]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5723, grad_fn=<DivBackward0>)\n",
      "i= 720\n",
      "w.grad= tensor([[ 0.3586,  0.5900, -1.4970],\n",
      "        [ 0.7702,  1.2667, -3.2127]])\n",
      "b.grad= tensor([-0.0034, -0.0054])\n",
      "new w tensor([[-0.3872,  0.8541,  0.6793],\n",
      "        [-0.2862,  0.8077,  0.8829]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5715, grad_fn=<DivBackward0>)\n",
      "i= 721\n",
      "w.grad= tensor([[ 0.3568,  0.5871, -1.4901],\n",
      "        [ 0.7662,  1.2604, -3.1980]])\n",
      "b.grad= tensor([-0.0034, -0.0053])\n",
      "new w tensor([[-0.3873,  0.8541,  0.6793],\n",
      "        [-0.2863,  0.8076,  0.8830]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5707, grad_fn=<DivBackward0>)\n",
      "i= 722\n",
      "w.grad= tensor([[ 0.3554,  0.5846, -1.4830],\n",
      "        [ 0.7626,  1.2545, -3.1831]])\n",
      "b.grad= tensor([-0.0033, -0.0053])\n",
      "new w tensor([[-0.3873,  0.8541,  0.6794],\n",
      "        [-0.2863,  0.8076,  0.8832]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5700, grad_fn=<DivBackward0>)\n",
      "i= 723\n",
      "w.grad= tensor([[ 0.3539,  0.5822, -1.4759],\n",
      "        [ 0.7594,  1.2489, -3.1679]])\n",
      "b.grad= tensor([-0.0033, -0.0052])\n",
      "new w tensor([[-0.3873,  0.8540,  0.6795],\n",
      "        [-0.2863,  0.8075,  0.8833]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5692, grad_fn=<DivBackward0>)\n",
      "i= 724\n",
      "w.grad= tensor([[ 0.3520,  0.5791, -1.4692],\n",
      "        [ 0.7554,  1.2426, -3.1534]])\n",
      "b.grad= tensor([-0.0033, -0.0051])\n",
      "new w tensor([[-0.3873,  0.8540,  0.6796],\n",
      "        [-0.2864,  0.8075,  0.8835]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5685, grad_fn=<DivBackward0>)\n",
      "i= 725\n",
      "w.grad= tensor([[ 0.3500,  0.5760, -1.4626],\n",
      "        [ 0.7521,  1.2371, -3.1386]])\n",
      "b.grad= tensor([-0.0032, -0.0050])\n",
      "new w tensor([[-0.3873,  0.8540,  0.6796],\n",
      "        [-0.2864,  0.8074,  0.8837]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5678, grad_fn=<DivBackward0>)\n",
      "i= 726\n",
      "w.grad= tensor([[ 0.3490,  0.5740, -1.4554],\n",
      "        [ 0.7486,  1.2314, -3.1239]])\n",
      "b.grad= tensor([-0.0032, -0.0050])\n",
      "new w tensor([[-0.3873,  0.8539,  0.6797],\n",
      "        [-0.2865,  0.8073,  0.8838]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5670, grad_fn=<DivBackward0>)\n",
      "i= 727\n",
      "w.grad= tensor([[ 0.3473,  0.5712, -1.4486],\n",
      "        [ 0.7448,  1.2252, -3.1095]])\n",
      "b.grad= tensor([-0.0032, -0.0049])\n",
      "new w tensor([[-0.3874,  0.8539,  0.6798],\n",
      "        [-0.2865,  0.8073,  0.8840]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5663, grad_fn=<DivBackward0>)\n",
      "i= 728\n",
      "w.grad= tensor([[ 0.3457,  0.5687, -1.4418],\n",
      "        [ 0.7416,  1.2198, -3.0949]])\n",
      "b.grad= tensor([-0.0031, -0.0048])\n",
      "new w tensor([[-0.3874,  0.8539,  0.6798],\n",
      "        [-0.2865,  0.8072,  0.8841]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5656, grad_fn=<DivBackward0>)\n",
      "i= 729\n",
      "w.grad= tensor([[ 0.3436,  0.5653, -1.4354],\n",
      "        [ 0.7380,  1.2140, -3.0805]])\n",
      "b.grad= tensor([-0.0031, -0.0048])\n",
      "new w tensor([[-0.3874,  0.8539,  0.6799],\n",
      "        [-0.2866,  0.8072,  0.8843]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5649, grad_fn=<DivBackward0>)\n",
      "i= 730\n",
      "w.grad= tensor([[ 0.3425,  0.5633, -1.4284],\n",
      "        [ 0.7351,  1.2089, -3.0658]])\n",
      "b.grad= tensor([-0.0031, -0.0047])\n",
      "new w tensor([[-0.3874,  0.8538,  0.6800],\n",
      "        [-0.2866,  0.8071,  0.8844]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5642, grad_fn=<DivBackward0>)\n",
      "i= 731\n",
      "w.grad= tensor([[ 0.3403,  0.5599, -1.4222],\n",
      "        [ 0.7312,  1.2027, -3.0518]])\n",
      "b.grad= tensor([-0.0030, -0.0046])\n",
      "new w tensor([[-0.3874,  0.8538,  0.6801],\n",
      "        [-0.2866,  0.8070,  0.8846]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5635, grad_fn=<DivBackward0>)\n",
      "i= 732\n",
      "w.grad= tensor([[ 0.3390,  0.5577, -1.4153],\n",
      "        [ 0.7278,  1.1972, -3.0376]])\n",
      "b.grad= tensor([-0.0030, -0.0046])\n",
      "new w tensor([[-0.3875,  0.8538,  0.6801],\n",
      "        [-0.2867,  0.8070,  0.8847]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5628, grad_fn=<DivBackward0>)\n",
      "i= 733\n",
      "w.grad= tensor([[ 0.3369,  0.5546, -1.4090],\n",
      "        [ 0.7242,  1.1914, -3.0236]])\n",
      "b.grad= tensor([-0.0030, -0.0045])\n",
      "new w tensor([[-0.3875,  0.8538,  0.6802],\n",
      "        [-0.2867,  0.8069,  0.8849]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5622, grad_fn=<DivBackward0>)\n",
      "i= 734\n",
      "w.grad= tensor([[ 0.3360,  0.5526, -1.4020],\n",
      "        [ 0.7211,  1.1861, -3.0092]])\n",
      "b.grad= tensor([-0.0029, -0.0044])\n",
      "new w tensor([[-0.3875,  0.8537,  0.6803],\n",
      "        [-0.2867,  0.8069,  0.8850]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5615, grad_fn=<DivBackward0>)\n",
      "i= 735\n",
      "w.grad= tensor([[ 0.3345,  0.5502, -1.3955],\n",
      "        [ 0.7176,  1.1804, -2.9953]])\n",
      "b.grad= tensor([-0.0029, -0.0044])\n",
      "new w tensor([[-0.3875,  0.8537,  0.6803],\n",
      "        [-0.2868,  0.8068,  0.8852]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5608, grad_fn=<DivBackward0>)\n",
      "i= 736\n",
      "w.grad= tensor([[ 0.3327,  0.5474, -1.3891],\n",
      "        [ 0.7144,  1.1750, -2.9812]])\n",
      "b.grad= tensor([-0.0029, -0.0043])\n",
      "new w tensor([[-0.3875,  0.8537,  0.6804],\n",
      "        [-0.2868,  0.8067,  0.8853]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5602, grad_fn=<DivBackward0>)\n",
      "i= 737\n",
      "w.grad= tensor([[ 0.3311,  0.5447, -1.3827],\n",
      "        [ 0.7110,  1.1695, -2.9673]])\n",
      "b.grad= tensor([-0.0029, -0.0042])\n",
      "new w tensor([[-0.3875,  0.8536,  0.6805],\n",
      "        [-0.2869,  0.8067,  0.8855]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5595, grad_fn=<DivBackward0>)\n",
      "i= 738\n",
      "w.grad= tensor([[ 0.3299,  0.5427, -1.3759],\n",
      "        [ 0.7077,  1.1642, -2.9534]])\n",
      "b.grad= tensor([-0.0028, -0.0042])\n",
      "new w tensor([[-0.3876,  0.8536,  0.6805],\n",
      "        [-0.2869,  0.8066,  0.8856]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5589, grad_fn=<DivBackward0>)\n",
      "i= 739\n",
      "w.grad= tensor([[ 0.3283,  0.5400, -1.3695],\n",
      "        [ 0.7042,  1.1584, -2.9398]])\n",
      "b.grad= tensor([-0.0028, -0.0041])\n",
      "new w tensor([[-0.3876,  0.8536,  0.6806],\n",
      "        [-0.2869,  0.8066,  0.8858]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5582, grad_fn=<DivBackward0>)\n",
      "i= 740\n",
      "w.grad= tensor([[ 0.3263,  0.5370, -1.3635],\n",
      "        [ 0.7009,  1.1530, -2.9261]])\n",
      "b.grad= tensor([-0.0028, -0.0040])\n",
      "new w tensor([[-0.3876,  0.8536,  0.6807],\n",
      "        [-0.2870,  0.8065,  0.8859]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5576, grad_fn=<DivBackward0>)\n",
      "i= 741\n",
      "w.grad= tensor([[ 0.3256,  0.5353, -1.3566],\n",
      "        [ 0.6980,  1.1480, -2.9122]])\n",
      "b.grad= tensor([-0.0027, -0.0040])\n",
      "new w tensor([[-0.3876,  0.8535,  0.6808],\n",
      "        [-0.2870,  0.8064,  0.8861]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5569, grad_fn=<DivBackward0>)\n",
      "i= 742\n",
      "w.grad= tensor([[ 0.3235,  0.5321, -1.3506],\n",
      "        [ 0.6945,  1.1424, -2.8988]])\n",
      "b.grad= tensor([-0.0027, -0.0039])\n",
      "new w tensor([[-0.3876,  0.8535,  0.6808],\n",
      "        [-0.2870,  0.8064,  0.8862]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5563, grad_fn=<DivBackward0>)\n",
      "i= 743\n",
      "w.grad= tensor([[ 0.3223,  0.5300, -1.3442],\n",
      "        [ 0.6916,  1.1374, -2.8850]])\n",
      "b.grad= tensor([-0.0027, -0.0038])\n",
      "new w tensor([[-0.3876,  0.8535,  0.6809],\n",
      "        [-0.2871,  0.8063,  0.8864]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5557, grad_fn=<DivBackward0>)\n",
      "i= 744\n",
      "w.grad= tensor([[ 0.3203,  0.5269, -1.3382],\n",
      "        [ 0.6881,  1.1318, -2.8717]])\n",
      "b.grad= tensor([-0.0026, -0.0038])\n",
      "new w tensor([[-0.3876,  0.8535,  0.6810],\n",
      "        [-0.2871,  0.8063,  0.8865]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5551, grad_fn=<DivBackward0>)\n",
      "i= 745\n",
      "w.grad= tensor([[ 0.3194,  0.5252, -1.3315],\n",
      "        [ 0.6849,  1.1266, -2.8583]])\n",
      "b.grad= tensor([-0.0026, -0.0037])\n",
      "new w tensor([[-0.3877,  0.8534,  0.6810],\n",
      "        [-0.2871,  0.8062,  0.8866]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5545, grad_fn=<DivBackward0>)\n",
      "i= 746\n",
      "w.grad= tensor([[ 0.3173,  0.5221, -1.3257],\n",
      "        [ 0.6818,  1.1213, -2.8449]])\n",
      "b.grad= tensor([-0.0026, -0.0037])\n",
      "new w tensor([[-0.3877,  0.8534,  0.6811],\n",
      "        [-0.2872,  0.8062,  0.8868]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5539, grad_fn=<DivBackward0>)\n",
      "i= 747\n",
      "w.grad= tensor([[ 0.3164,  0.5203, -1.3191],\n",
      "        [ 0.6785,  1.1160, -2.8317]])\n",
      "b.grad= tensor([-0.0026, -0.0036])\n",
      "new w tensor([[-0.3877,  0.8534,  0.6812],\n",
      "        [-0.2872,  0.8061,  0.8869]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5533, grad_fn=<DivBackward0>)\n",
      "i= 748\n",
      "w.grad= tensor([[ 0.3148,  0.5177, -1.3131],\n",
      "        [ 0.6756,  1.1111, -2.8183]])\n",
      "b.grad= tensor([-0.0025, -0.0035])\n",
      "new w tensor([[-0.3877,  0.8534,  0.6812],\n",
      "        [-0.2872,  0.8061,  0.8871]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5527, grad_fn=<DivBackward0>)\n",
      "i= 749\n",
      "w.grad= tensor([[ 0.3134,  0.5154, -1.3069],\n",
      "        [ 0.6721,  1.1055, -2.8054]])\n",
      "b.grad= tensor([-0.0025, -0.0035])\n",
      "new w tensor([[-0.3877,  0.8533,  0.6813],\n",
      "        [-0.2873,  0.8060,  0.8872]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5521, grad_fn=<DivBackward0>)\n",
      "i= 750\n",
      "w.grad= tensor([[ 0.3118,  0.5129, -1.3009],\n",
      "        [ 0.6690,  1.1003, -2.7923]])\n",
      "b.grad= tensor([-0.0025, -0.0034])\n",
      "new w tensor([[-0.3877,  0.8533,  0.6814],\n",
      "        [-0.2873,  0.8059,  0.8873]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5515, grad_fn=<DivBackward0>)\n",
      "i= 751\n",
      "w.grad= tensor([[ 0.3100,  0.5102, -1.2950],\n",
      "        [ 0.6659,  1.0953, -2.7792]])\n",
      "b.grad= tensor([-0.0024, -0.0033])\n",
      "new w tensor([[-0.3878,  0.8533,  0.6814],\n",
      "        [-0.2873,  0.8059,  0.8875]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5510, grad_fn=<DivBackward0>)\n",
      "i= 752\n",
      "w.grad= tensor([[ 0.3088,  0.5079, -1.2888],\n",
      "        [ 0.6629,  1.0903, -2.7662]])\n",
      "b.grad= tensor([-0.0024, -0.0033])\n",
      "new w tensor([[-0.3878,  0.8532,  0.6815],\n",
      "        [-0.2874,  0.8058,  0.8876]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5504, grad_fn=<DivBackward0>)\n",
      "i= 753\n",
      "w.grad= tensor([[ 0.3071,  0.5053, -1.2830],\n",
      "        [ 0.6592,  1.0846, -2.7536]])\n",
      "b.grad= tensor([-0.0024, -0.0032])\n",
      "new w tensor([[-0.3878,  0.8532,  0.6815],\n",
      "        [-0.2874,  0.8058,  0.8878]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5498, grad_fn=<DivBackward0>)\n",
      "i= 754\n",
      "w.grad= tensor([[ 0.3060,  0.5033, -1.2768],\n",
      "        [ 0.6565,  1.0799, -2.7406]])\n",
      "b.grad= tensor([-0.0024, -0.0032])\n",
      "new w tensor([[-0.3878,  0.8532,  0.6816],\n",
      "        [-0.2874,  0.8057,  0.8879]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5493, grad_fn=<DivBackward0>)\n",
      "i= 755\n",
      "w.grad= tensor([[ 0.3045,  0.5009, -1.2709],\n",
      "        [ 0.6535,  1.0748, -2.7277]])\n",
      "b.grad= tensor([-0.0023, -0.0031])\n",
      "new w tensor([[-0.3878,  0.8532,  0.6817],\n",
      "        [-0.2875,  0.8057,  0.8880]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5487, grad_fn=<DivBackward0>)\n",
      "i= 756\n",
      "w.grad= tensor([[ 0.3031,  0.4986, -1.2649],\n",
      "        [ 0.6507,  1.0701, -2.7149]])\n",
      "b.grad= tensor([-0.0023, -0.0030])\n",
      "new w tensor([[-0.3878,  0.8531,  0.6817],\n",
      "        [-0.2875,  0.8056,  0.8882]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5482, grad_fn=<DivBackward0>)\n",
      "i= 757\n",
      "w.grad= tensor([[ 0.3018,  0.4964, -1.2589],\n",
      "        [ 0.6469,  1.0644, -2.7027]])\n",
      "b.grad= tensor([-0.0023, -0.0030])\n",
      "new w tensor([[-0.3878,  0.8531,  0.6818],\n",
      "        [-0.2875,  0.8056,  0.8883]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5476, grad_fn=<DivBackward0>)\n",
      "i= 758\n",
      "w.grad= tensor([[ 0.3003,  0.4940, -1.2531],\n",
      "        [ 0.6444,  1.0599, -2.6898]])\n",
      "b.grad= tensor([-0.0022, -0.0029])\n",
      "new w tensor([[-0.3879,  0.8531,  0.6819],\n",
      "        [-0.2876,  0.8055,  0.8884]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5471, grad_fn=<DivBackward0>)\n",
      "i= 759\n",
      "w.grad= tensor([[ 0.2991,  0.4919, -1.2471],\n",
      "        [ 0.6413,  1.0549, -2.6773]])\n",
      "b.grad= tensor([-0.0022, -0.0029])\n",
      "new w tensor([[-0.3879,  0.8531,  0.6819],\n",
      "        [-0.2876,  0.8055,  0.8886]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5465, grad_fn=<DivBackward0>)\n",
      "i= 760\n",
      "w.grad= tensor([[ 0.2971,  0.4889, -1.2417],\n",
      "        [ 0.6389,  1.0505, -2.6644]])\n",
      "b.grad= tensor([-0.0022, -0.0028])\n",
      "new w tensor([[-0.3879,  0.8531,  0.6820],\n",
      "        [-0.2876,  0.8054,  0.8887]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5460, grad_fn=<DivBackward0>)\n",
      "i= 761\n",
      "w.grad= tensor([[ 0.2963,  0.4873, -1.2355],\n",
      "        [ 0.6354,  1.0451, -2.6523]])\n",
      "b.grad= tensor([-0.0022, -0.0027])\n",
      "new w tensor([[-0.3879,  0.8530,  0.6820],\n",
      "        [-0.2877,  0.8054,  0.8888]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5455, grad_fn=<DivBackward0>)\n",
      "i= 762\n",
      "w.grad= tensor([[ 0.2946,  0.4847, -1.2300],\n",
      "        [ 0.6325,  1.0404, -2.6398]])\n",
      "b.grad= tensor([-0.0021, -0.0027])\n",
      "new w tensor([[-0.3879,  0.8530,  0.6821],\n",
      "        [-0.2877,  0.8053,  0.8890]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5450, grad_fn=<DivBackward0>)\n",
      "i= 763\n",
      "w.grad= tensor([[ 0.2934,  0.4825, -1.2242],\n",
      "        [ 0.6294,  1.0353, -2.6276]])\n",
      "b.grad= tensor([-0.0021, -0.0026])\n",
      "new w tensor([[-0.3879,  0.8530,  0.6822],\n",
      "        [-0.2877,  0.8053,  0.8891]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5445, grad_fn=<DivBackward0>)\n",
      "i= 764\n",
      "w.grad= tensor([[ 0.2917,  0.4799, -1.2187],\n",
      "        [ 0.6267,  1.0306, -2.6152]])\n",
      "b.grad= tensor([-0.0021, -0.0026])\n",
      "new w tensor([[-0.3880,  0.8530,  0.6822],\n",
      "        [-0.2878,  0.8052,  0.8892]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5440, grad_fn=<DivBackward0>)\n",
      "i= 765\n",
      "w.grad= tensor([[ 0.2904,  0.4777, -1.2129],\n",
      "        [ 0.6239,  1.0258, -2.6030]])\n",
      "b.grad= tensor([-0.0021, -0.0025])\n",
      "new w tensor([[-0.3880,  0.8529,  0.6823],\n",
      "        [-0.2878,  0.8051,  0.8894]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5435, grad_fn=<DivBackward0>)\n",
      "i= 766\n",
      "w.grad= tensor([[ 0.2888,  0.4752, -1.2075],\n",
      "        [ 0.6210,  1.0214, -2.5907]])\n",
      "b.grad= tensor([-0.0020, -0.0024])\n",
      "new w tensor([[-0.3880,  0.8529,  0.6824],\n",
      "        [-0.2878,  0.8051,  0.8895]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5430, grad_fn=<DivBackward0>)\n",
      "i= 767\n",
      "w.grad= tensor([[ 0.2875,  0.4731, -1.2018],\n",
      "        [ 0.6173,  1.0156, -2.5792]])\n",
      "b.grad= tensor([-0.0020, -0.0024])\n",
      "new w tensor([[-0.3880,  0.8529,  0.6824],\n",
      "        [-0.2878,  0.8050,  0.8896]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5425, grad_fn=<DivBackward0>)\n",
      "i= 768\n",
      "w.grad= tensor([[ 0.2868,  0.4715, -1.1958],\n",
      "        [ 0.6151,  1.0115, -2.5667]])\n",
      "b.grad= tensor([-0.0020, -0.0023])\n",
      "new w tensor([[-0.3880,  0.8529,  0.6825],\n",
      "        [-0.2879,  0.8050,  0.8898]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5420, grad_fn=<DivBackward0>)\n",
      "i= 769\n",
      "w.grad= tensor([[ 0.2852,  0.4691, -1.1903],\n",
      "        [ 0.6122,  1.0069, -2.5547]])\n",
      "b.grad= tensor([-0.0019, -0.0023])\n",
      "new w tensor([[-0.3880,  0.8528,  0.6825],\n",
      "        [-0.2879,  0.8049,  0.8899]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5415, grad_fn=<DivBackward0>)\n",
      "i= 770\n",
      "w.grad= tensor([[ 0.2837,  0.4666, -1.1849],\n",
      "        [ 0.6086,  1.0013, -2.5434]])\n",
      "b.grad= tensor([-0.0019, -0.0022])\n",
      "new w tensor([[-0.3880,  0.8528,  0.6826],\n",
      "        [-0.2879,  0.8049,  0.8900]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5410, grad_fn=<DivBackward0>)\n",
      "i= 771\n",
      "w.grad= tensor([[ 0.2827,  0.4648, -1.1792],\n",
      "        [ 0.6068,  0.9978, -2.5308]])\n",
      "b.grad= tensor([-0.0019, -0.0022])\n",
      "new w tensor([[-0.3881,  0.8528,  0.6826],\n",
      "        [-0.2880,  0.8048,  0.8901]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5405, grad_fn=<DivBackward0>)\n",
      "i= 772\n",
      "w.grad= tensor([[ 0.2812,  0.4625, -1.1738],\n",
      "        [ 0.6037,  0.9927, -2.5191]])\n",
      "b.grad= tensor([-0.0019, -0.0021])\n",
      "new w tensor([[-0.3881,  0.8528,  0.6827],\n",
      "        [-0.2880,  0.8048,  0.8903]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5401, grad_fn=<DivBackward0>)\n",
      "i= 773\n",
      "w.grad= tensor([[ 0.2802,  0.4607, -1.1680],\n",
      "        [ 0.6007,  0.9880, -2.5075]])\n",
      "b.grad= tensor([-0.0018, -0.0021])\n",
      "new w tensor([[-0.3881,  0.8527,  0.6828],\n",
      "        [-0.2880,  0.8047,  0.8904]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5396, grad_fn=<DivBackward0>)\n",
      "i= 774\n",
      "w.grad= tensor([[ 0.2784,  0.4580, -1.1630],\n",
      "        [ 0.5978,  0.9833, -2.4958]])\n",
      "b.grad= tensor([-0.0018, -0.0020])\n",
      "new w tensor([[-0.3881,  0.8527,  0.6828],\n",
      "        [-0.2881,  0.8047,  0.8905]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5391, grad_fn=<DivBackward0>)\n",
      "i= 775\n",
      "w.grad= tensor([[ 0.2775,  0.4563, -1.1573],\n",
      "        [ 0.5953,  0.9791, -2.4840]])\n",
      "b.grad= tensor([-0.0018, -0.0019])\n",
      "new w tensor([[-0.3881,  0.8527,  0.6829],\n",
      "        [-0.2881,  0.8046,  0.8906]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5387, grad_fn=<DivBackward0>)\n",
      "i= 776\n",
      "w.grad= tensor([[ 0.2761,  0.4541, -1.1519],\n",
      "        [ 0.5922,  0.9741, -2.4726]])\n",
      "b.grad= tensor([-0.0018, -0.0019])\n",
      "new w tensor([[-0.3881,  0.8527,  0.6829],\n",
      "        [-0.2881,  0.8046,  0.8908]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5382, grad_fn=<DivBackward0>)\n",
      "i= 777\n",
      "w.grad= tensor([[ 0.2747,  0.4518, -1.1466],\n",
      "        [ 0.5896,  0.9696, -2.4610]])\n",
      "b.grad= tensor([-0.0017, -0.0018])\n",
      "new w tensor([[-0.3881,  0.8527,  0.6830],\n",
      "        [-0.2881,  0.8046,  0.8909]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5378, grad_fn=<DivBackward0>)\n",
      "i= 778\n",
      "w.grad= tensor([[ 0.2733,  0.4495, -1.1414],\n",
      "        [ 0.5871,  0.9654, -2.4493]])\n",
      "b.grad= tensor([-0.0017, -0.0018])\n",
      "new w tensor([[-0.3882,  0.8526,  0.6831],\n",
      "        [-0.2882,  0.8045,  0.8910]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5373, grad_fn=<DivBackward0>)\n",
      "i= 779\n",
      "w.grad= tensor([[ 0.2720,  0.4473, -1.1361],\n",
      "        [ 0.5844,  0.9611, -2.4379]])\n",
      "b.grad= tensor([-0.0017, -0.0017])\n",
      "new w tensor([[-0.3882,  0.8526,  0.6831],\n",
      "        [-0.2882,  0.8045,  0.8911]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5369, grad_fn=<DivBackward0>)\n",
      "i= 780\n",
      "w.grad= tensor([[ 0.2714,  0.4459, -1.1304],\n",
      "        [ 0.5814,  0.9562, -2.4267]])\n",
      "b.grad= tensor([-0.0017, -0.0017])\n",
      "new w tensor([[-0.3882,  0.8526,  0.6832],\n",
      "        [-0.2882,  0.8044,  0.8912]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5364, grad_fn=<DivBackward0>)\n",
      "i= 781\n",
      "w.grad= tensor([[ 0.2696,  0.4433, -1.1254],\n",
      "        [ 0.5792,  0.9524, -2.4150]])\n",
      "b.grad= tensor([-0.0016, -0.0016])\n",
      "new w tensor([[-0.3882,  0.8526,  0.6832],\n",
      "        [-0.2883,  0.8044,  0.8914]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5360, grad_fn=<DivBackward0>)\n",
      "i= 782\n",
      "w.grad= tensor([[ 0.2685,  0.4415, -1.1200],\n",
      "        [ 0.5758,  0.9471, -2.4042]])\n",
      "b.grad= tensor([-0.0016, -0.0016])\n",
      "new w tensor([[-0.3882,  0.8525,  0.6833],\n",
      "        [-0.2883,  0.8043,  0.8915]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5356, grad_fn=<DivBackward0>)\n",
      "i= 783\n",
      "w.grad= tensor([[ 0.2677,  0.4399, -1.1145],\n",
      "        [ 0.5734,  0.9431, -2.3928]])\n",
      "b.grad= tensor([-0.0016, -0.0015])\n",
      "new w tensor([[-0.3882,  0.8525,  0.6833],\n",
      "        [-0.2883,  0.8043,  0.8916]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5352, grad_fn=<DivBackward0>)\n",
      "i= 784\n",
      "w.grad= tensor([[ 0.2660,  0.4374, -1.1096],\n",
      "        [ 0.5703,  0.9382, -2.3819]])\n",
      "b.grad= tensor([-0.0016, -0.0015])\n",
      "new w tensor([[-0.3882,  0.8525,  0.6834],\n",
      "        [-0.2883,  0.8042,  0.8917]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5347, grad_fn=<DivBackward0>)\n",
      "i= 785\n",
      "w.grad= tensor([[ 0.2648,  0.4355, -1.1043],\n",
      "        [ 0.5681,  0.9343, -2.3704]])\n",
      "b.grad= tensor([-0.0015, -0.0014])\n",
      "new w tensor([[-0.3882,  0.8525,  0.6834],\n",
      "        [-0.2884,  0.8042,  0.8918]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5343, grad_fn=<DivBackward0>)\n",
      "i= 786\n",
      "w.grad= tensor([[ 0.2633,  0.4331, -1.0994],\n",
      "        [ 0.5651,  0.9296, -2.3596]])\n",
      "b.grad= tensor([-0.0015, -0.0014])\n",
      "new w tensor([[-0.3883,  0.8525,  0.6835],\n",
      "        [-0.2884,  0.8041,  0.8920]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5339, grad_fn=<DivBackward0>)\n",
      "i= 787\n",
      "w.grad= tensor([[ 0.2623,  0.4314, -1.0941],\n",
      "        [ 0.5626,  0.9254, -2.3485]])\n",
      "b.grad= tensor([-0.0015, -0.0013])\n",
      "new w tensor([[-0.3883,  0.8524,  0.6836],\n",
      "        [-0.2884,  0.8041,  0.8921]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5335, grad_fn=<DivBackward0>)\n",
      "i= 788\n",
      "w.grad= tensor([[ 0.2611,  0.4294, -1.0890],\n",
      "        [ 0.5601,  0.9212, -2.3375]])\n",
      "b.grad= tensor([-0.0015, -0.0012])\n",
      "new w tensor([[-0.3883,  0.8524,  0.6836],\n",
      "        [-0.2885,  0.8040,  0.8922]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5331, grad_fn=<DivBackward0>)\n",
      "i= 789\n",
      "w.grad= tensor([[ 0.2596,  0.4270, -1.0841],\n",
      "        [ 0.5573,  0.9167, -2.3267]])\n",
      "b.grad= tensor([-0.0014, -0.0012])\n",
      "new w tensor([[-0.3883,  0.8524,  0.6837],\n",
      "        [-0.2885,  0.8040,  0.8923]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5327, grad_fn=<DivBackward0>)\n",
      "i= 790\n",
      "w.grad= tensor([[ 0.2587,  0.4253, -1.0789],\n",
      "        [ 0.5549,  0.9127, -2.3157]])\n",
      "b.grad= tensor([-0.0014, -0.0011])\n",
      "new w tensor([[-0.3883,  0.8524,  0.6837],\n",
      "        [-0.2885,  0.8039,  0.8924]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5323, grad_fn=<DivBackward0>)\n",
      "i= 791\n",
      "w.grad= tensor([[ 0.2572,  0.4232, -1.0740],\n",
      "        [ 0.5525,  0.9086, -2.3048]])\n",
      "b.grad= tensor([-0.0014, -0.0011])\n",
      "new w tensor([[-0.3883,  0.8523,  0.6838],\n",
      "        [-0.2885,  0.8039,  0.8925]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5319, grad_fn=<DivBackward0>)\n",
      "i= 792\n",
      "w.grad= tensor([[ 0.2562,  0.4213, -1.0688],\n",
      "        [ 0.5495,  0.9038, -2.2943]])\n",
      "b.grad= tensor([-0.0014, -0.0010])\n",
      "new w tensor([[-0.3883,  0.8523,  0.6838],\n",
      "        [-0.2886,  0.8038,  0.8927]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5315, grad_fn=<DivBackward0>)\n",
      "i= 793\n",
      "w.grad= tensor([[ 0.2549,  0.4194, -1.0639],\n",
      "        [ 0.5474,  0.9001, -2.2833]])\n",
      "b.grad= tensor([-0.0013, -0.0010])\n",
      "new w tensor([[-0.3883,  0.8523,  0.6839],\n",
      "        [-0.2886,  0.8038,  0.8928]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5311, grad_fn=<DivBackward0>)\n",
      "i= 794\n",
      "w.grad= tensor([[ 0.2535,  0.4171, -1.0591],\n",
      "        [ 0.5443,  0.8953, -2.2730]])\n",
      "b.grad= tensor([-0.0013, -0.0009])\n",
      "new w tensor([[-0.3884,  0.8523,  0.6839],\n",
      "        [-0.2886,  0.8038,  0.8929]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5307, grad_fn=<DivBackward0>)\n",
      "i= 795\n",
      "w.grad= tensor([[ 0.2523,  0.4151, -1.0541],\n",
      "        [ 0.5422,  0.8916, -2.2621]])\n",
      "b.grad= tensor([-0.0013, -0.0009])\n",
      "new w tensor([[-0.3884,  0.8523,  0.6840],\n",
      "        [-0.2887,  0.8037,  0.8930]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5303, grad_fn=<DivBackward0>)\n",
      "i= 796\n",
      "w.grad= tensor([[ 0.2514,  0.4135, -1.0491],\n",
      "        [ 0.5396,  0.8875, -2.2515]])\n",
      "b.grad= tensor([-0.0013, -0.0008])\n",
      "new w tensor([[-0.3884,  0.8522,  0.6840],\n",
      "        [-0.2887,  0.8037,  0.8931]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5300, grad_fn=<DivBackward0>)\n",
      "i= 797\n",
      "w.grad= tensor([[ 0.2501,  0.4113, -1.0443],\n",
      "        [ 0.5376,  0.8838, -2.2408]])\n",
      "b.grad= tensor([-0.0012, -0.0008])\n",
      "new w tensor([[-0.3884,  0.8522,  0.6841],\n",
      "        [-0.2887,  0.8036,  0.8932]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5296, grad_fn=<DivBackward0>)\n",
      "i= 798\n",
      "w.grad= tensor([[ 0.2492,  0.4098, -1.0392],\n",
      "        [ 0.5345,  0.8792, -2.2306]])\n",
      "b.grad= tensor([-0.0012, -0.0007])\n",
      "new w tensor([[-0.3884,  0.8522,  0.6841],\n",
      "        [-0.2887,  0.8036,  0.8933]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5292, grad_fn=<DivBackward0>)\n",
      "i= 799\n",
      "w.grad= tensor([[ 0.2478,  0.4076, -1.0345],\n",
      "        [ 0.5318,  0.8748, -2.2204]])\n",
      "b.grad= tensor([-0.0012, -0.0007])\n",
      "new w tensor([[-0.3884,  0.8522,  0.6842],\n",
      "        [-0.2888,  0.8035,  0.8934]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5289, grad_fn=<DivBackward0>)\n",
      "i= 800\n",
      "w.grad= tensor([[ 0.2467,  0.4058, -1.0296],\n",
      "        [ 0.5297,  0.8712, -2.2097]])\n",
      "b.grad= tensor([-0.0012, -0.0006])\n",
      "new w tensor([[-0.3884,  0.8522,  0.6842],\n",
      "        [-0.2888,  0.8035,  0.8936]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5285, grad_fn=<DivBackward0>)\n",
      "i= 801\n",
      "w.grad= tensor([[ 0.2458,  0.4042, -1.0247],\n",
      "        [ 0.5266,  0.8663, -2.1998]])\n",
      "b.grad= tensor([-0.0012, -0.0006])\n",
      "new w tensor([[-0.3884,  0.8521,  0.6843],\n",
      "        [-0.2888,  0.8035,  0.8937]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5281, grad_fn=<DivBackward0>)\n",
      "i= 802\n",
      "w.grad= tensor([[ 0.2449,  0.4025, -1.0197],\n",
      "        [ 0.5246,  0.8628, -2.1893]])\n",
      "b.grad= tensor([-0.0011, -0.0005])\n",
      "new w tensor([[-0.3885,  0.8521,  0.6843],\n",
      "        [-0.2888,  0.8034,  0.8938]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5278, grad_fn=<DivBackward0>)\n",
      "i= 803\n",
      "w.grad= tensor([[ 0.2427,  0.3995, -1.0157],\n",
      "        [ 0.5216,  0.8582, -2.1794]])\n",
      "b.grad= tensor([-0.0011, -0.0005])\n",
      "new w tensor([[-0.3885,  0.8521,  0.6844],\n",
      "        [-0.2889,  0.8034,  0.8939]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5274, grad_fn=<DivBackward0>)\n",
      "i= 804\n",
      "w.grad= tensor([[ 0.2422,  0.3983, -1.0105],\n",
      "        [ 0.5196,  0.8547, -2.1690]])\n",
      "b.grad= tensor([-0.0011, -0.0004])\n",
      "new w tensor([[-0.3885,  0.8521,  0.6844],\n",
      "        [-0.2889,  0.8033,  0.8940]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5271, grad_fn=<DivBackward0>)\n",
      "i= 805\n",
      "w.grad= tensor([[ 0.2414,  0.3969, -1.0055],\n",
      "        [ 0.5171,  0.8507, -2.1589]])\n",
      "b.grad= tensor([-0.0011, -0.0004])\n",
      "new w tensor([[-0.3885,  0.8521,  0.6845],\n",
      "        [-0.2889,  0.8033,  0.8941]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5267, grad_fn=<DivBackward0>)\n",
      "i= 806\n",
      "w.grad= tensor([[ 0.2397,  0.3945, -1.0012],\n",
      "        [ 0.5143,  0.8461, -2.1491]])\n",
      "b.grad= tensor([-0.0010, -0.0004])\n",
      "new w tensor([[-0.3885,  0.8520,  0.6845],\n",
      "        [-0.2889,  0.8032,  0.8942]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5264, grad_fn=<DivBackward0>)\n",
      "i= 807\n",
      "w.grad= tensor([[ 0.2386,  0.3925, -0.9965],\n",
      "        [ 0.5127,  0.8431, -2.1385]])\n",
      "b.grad= tensor([-0.0010, -0.0003])\n",
      "new w tensor([[-0.3885,  0.8520,  0.6846],\n",
      "        [-0.2890,  0.8032,  0.8943]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5260, grad_fn=<DivBackward0>)\n",
      "i= 808\n",
      "w.grad= tensor([[ 0.2377,  0.3910, -0.9918],\n",
      "        [ 0.5100,  0.8388, -2.1287]])\n",
      "b.grad= tensor([-0.0010, -0.0003])\n",
      "new w tensor([[-0.3885,  0.8520,  0.6846],\n",
      "        [-0.2890,  0.8032,  0.8944]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5257, grad_fn=<DivBackward0>)\n",
      "i= 809\n",
      "w.grad= tensor([[ 0.2367,  0.3892, -0.9871],\n",
      "        [ 0.5079,  0.8352, -2.1186]])\n",
      "b.grad= tensor([-0.0010, -0.0002])\n",
      "new w tensor([[-0.3885,  0.8520,  0.6847],\n",
      "        [-0.2890,  0.8031,  0.8945]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5254, grad_fn=<DivBackward0>)\n",
      "i= 810\n",
      "w.grad= tensor([[ 0.2355,  0.3873, -0.9825],\n",
      "        [ 0.5046,  0.8302, -2.1094]])\n",
      "b.grad= tensor([-0.0010, -0.0002])\n",
      "new w tensor([[-0.3886,  0.8520,  0.6847],\n",
      "        [-0.2890,  0.8031,  0.8946]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5250, grad_fn=<DivBackward0>)\n",
      "i= 811\n",
      "w.grad= tensor([[ 0.2341,  0.3852, -0.9781],\n",
      "        [ 0.5033,  0.8275, -2.0988]])\n",
      "b.grad= tensor([-0.0009, -0.0001])\n",
      "new w tensor([[-0.3886,  0.8519,  0.6848],\n",
      "        [-0.2891,  0.8030,  0.8947]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5247, grad_fn=<DivBackward0>)\n",
      "i= 812\n",
      "w.grad= tensor([[ 0.2332,  0.3836, -0.9734],\n",
      "        [ 0.5006,  0.8233, -2.0892]])\n",
      "b.grad= tensor([-9.1210e-04, -6.5617e-05])\n",
      "new w tensor([[-0.3886,  0.8519,  0.6848],\n",
      "        [-0.2891,  0.8030,  0.8948]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5244, grad_fn=<DivBackward0>)\n",
      "i= 813\n",
      "w.grad= tensor([[ 0.2322,  0.3819, -0.9688],\n",
      "        [ 0.4983,  0.8195, -2.0794]])\n",
      "b.grad= tensor([-8.8957e-04, -1.8314e-05])\n",
      "new w tensor([[-0.3886,  0.8519,  0.6849],\n",
      "        [-0.2891,  0.8029,  0.8949]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5241, grad_fn=<DivBackward0>)\n",
      "i= 814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([[ 0.2315,  0.3805, -0.9641],\n",
      "        [ 0.4961,  0.8159, -2.0697]])\n",
      "b.grad= tensor([-8.6364e-04,  2.8990e-05])\n",
      "new w tensor([[-0.3886,  0.8519,  0.6849],\n",
      "        [-0.2891,  0.8029,  0.8951]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5238, grad_fn=<DivBackward0>)\n",
      "i= 815\n",
      "w.grad= tensor([[ 0.2300,  0.3782, -0.9599],\n",
      "        [ 0.4940,  0.8124, -2.0598]])\n",
      "b.grad= tensor([-8.4800e-04,  7.7819e-05])\n",
      "new w tensor([[-0.3886,  0.8519,  0.6850],\n",
      "        [-0.2892,  0.8029,  0.8952]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5234, grad_fn=<DivBackward0>)\n",
      "i= 816\n",
      "w.grad= tensor([[ 0.2291,  0.3767, -0.9553],\n",
      "        [ 0.4913,  0.8082, -2.0505]])\n",
      "b.grad= tensor([-0.0008,  0.0001])\n",
      "new w tensor([[-0.3886,  0.8518,  0.6850],\n",
      "        [-0.2892,  0.8028,  0.8953]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5231, grad_fn=<DivBackward0>)\n",
      "i= 817\n",
      "w.grad= tensor([[ 0.2276,  0.3746, -0.9510],\n",
      "        [ 0.4891,  0.8045, -2.0409]])\n",
      "b.grad= tensor([-0.0008,  0.0002])\n",
      "new w tensor([[-0.3886,  0.8518,  0.6851],\n",
      "        [-0.2892,  0.8028,  0.8954]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5228, grad_fn=<DivBackward0>)\n",
      "i= 818\n",
      "w.grad= tensor([[ 0.2270,  0.3732, -0.9464],\n",
      "        [ 0.4865,  0.8005, -2.0315]])\n",
      "b.grad= tensor([-0.0008,  0.0002])\n",
      "new w tensor([[-0.3886,  0.8518,  0.6851],\n",
      "        [-0.2892,  0.8027,  0.8955]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5225, grad_fn=<DivBackward0>)\n",
      "i= 819\n",
      "w.grad= tensor([[ 0.2259,  0.3716, -0.9419],\n",
      "        [ 0.4840,  0.7963, -2.0222]])\n",
      "b.grad= tensor([-0.0008,  0.0002])\n",
      "new w tensor([[-0.3887,  0.8518,  0.6852],\n",
      "        [-0.2893,  0.8027,  0.8956]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5222, grad_fn=<DivBackward0>)\n",
      "i= 820\n",
      "w.grad= tensor([[ 0.2244,  0.3694, -0.9378],\n",
      "        [ 0.4825,  0.7934, -2.0123]])\n",
      "b.grad= tensor([-0.0007,  0.0003])\n",
      "new w tensor([[-0.3887,  0.8518,  0.6852],\n",
      "        [-0.2893,  0.8027,  0.8957]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5219, grad_fn=<DivBackward0>)\n",
      "i= 821\n",
      "w.grad= tensor([[ 0.2234,  0.3678, -0.9334],\n",
      "        [ 0.4795,  0.7889, -2.0034]])\n",
      "b.grad= tensor([-0.0007,  0.0003])\n",
      "new w tensor([[-0.3887,  0.8518,  0.6853],\n",
      "        [-0.2893,  0.8026,  0.8958]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5216, grad_fn=<DivBackward0>)\n",
      "i= 822\n",
      "w.grad= tensor([[ 0.2227,  0.3662, -0.9289],\n",
      "        [ 0.4778,  0.7859, -1.9937]])\n",
      "b.grad= tensor([-0.0007,  0.0004])\n",
      "new w tensor([[-0.3887,  0.8517,  0.6853],\n",
      "        [-0.2893,  0.8026,  0.8959]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5213, grad_fn=<DivBackward0>)\n",
      "i= 823\n",
      "w.grad= tensor([[ 0.2214,  0.3642, -0.9247],\n",
      "        [ 0.4753,  0.7819, -1.9846]])\n",
      "b.grad= tensor([-0.0007,  0.0004])\n",
      "new w tensor([[-0.3887,  0.8517,  0.6854],\n",
      "        [-0.2894,  0.8025,  0.8960]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5210, grad_fn=<DivBackward0>)\n",
      "i= 824\n",
      "w.grad= tensor([[ 0.2204,  0.3626, -0.9203],\n",
      "        [ 0.4733,  0.7784, -1.9752]])\n",
      "b.grad= tensor([-0.0007,  0.0005])\n",
      "new w tensor([[-0.3887,  0.8517,  0.6854],\n",
      "        [-0.2894,  0.8025,  0.8961]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5207, grad_fn=<DivBackward0>)\n",
      "i= 825\n",
      "w.grad= tensor([[ 0.2196,  0.3612, -0.9158],\n",
      "        [ 0.4711,  0.7747, -1.9659]])\n",
      "b.grad= tensor([-0.0006,  0.0005])\n",
      "new w tensor([[-0.3887,  0.8517,  0.6855],\n",
      "        [-0.2894,  0.8025,  0.8962]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5204, grad_fn=<DivBackward0>)\n",
      "i= 826\n",
      "w.grad= tensor([[ 0.2184,  0.3594, -0.9117],\n",
      "        [ 0.4690,  0.7713, -1.9568]])\n",
      "b.grad= tensor([-0.0006,  0.0006])\n",
      "new w tensor([[-0.3887,  0.8517,  0.6855],\n",
      "        [-0.2894,  0.8024,  0.8963]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5202, grad_fn=<DivBackward0>)\n",
      "i= 827\n",
      "w.grad= tensor([[ 0.2178,  0.3581, -0.9072],\n",
      "        [ 0.4669,  0.7679, -1.9475]])\n",
      "b.grad= tensor([-0.0006,  0.0006])\n",
      "new w tensor([[-0.3887,  0.8516,  0.6855],\n",
      "        [-0.2895,  0.8024,  0.8964]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5199, grad_fn=<DivBackward0>)\n",
      "i= 828\n",
      "w.grad= tensor([[ 0.2166,  0.3562, -0.9031],\n",
      "        [ 0.4644,  0.7638, -1.9386]])\n",
      "b.grad= tensor([-0.0006,  0.0006])\n",
      "new w tensor([[-0.3888,  0.8516,  0.6856],\n",
      "        [-0.2895,  0.8024,  0.8965]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5196, grad_fn=<DivBackward0>)\n",
      "i= 829\n",
      "w.grad= tensor([[ 0.2155,  0.3545, -0.8989],\n",
      "        [ 0.4622,  0.7603, -1.9296]])\n",
      "b.grad= tensor([-0.0006,  0.0007])\n",
      "new w tensor([[-0.3888,  0.8516,  0.6856],\n",
      "        [-0.2895,  0.8023,  0.8965]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5193, grad_fn=<DivBackward0>)\n",
      "i= 830\n",
      "w.grad= tensor([[ 0.2146,  0.3529, -0.8947],\n",
      "        [ 0.4600,  0.7567, -1.9206]])\n",
      "b.grad= tensor([-0.0005,  0.0007])\n",
      "new w tensor([[-0.3888,  0.8516,  0.6857],\n",
      "        [-0.2895,  0.8023,  0.8966]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5190, grad_fn=<DivBackward0>)\n",
      "i= 831\n",
      "w.grad= tensor([[ 0.2134,  0.3511, -0.8906],\n",
      "        [ 0.4578,  0.7530, -1.9117]])\n",
      "b.grad= tensor([-0.0005,  0.0008])\n",
      "new w tensor([[-0.3888,  0.8516,  0.6857],\n",
      "        [-0.2895,  0.8022,  0.8967]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5188, grad_fn=<DivBackward0>)\n",
      "i= 832\n",
      "w.grad= tensor([[ 0.2119,  0.3489, -0.8868],\n",
      "        [ 0.4558,  0.7496, -1.9027]])\n",
      "b.grad= tensor([-0.0005,  0.0008])\n",
      "new w tensor([[-0.3888,  0.8516,  0.6858],\n",
      "        [-0.2896,  0.8022,  0.8968]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5185, grad_fn=<DivBackward0>)\n",
      "i= 833\n",
      "w.grad= tensor([[ 0.2113,  0.3477, -0.8824],\n",
      "        [ 0.4537,  0.7463, -1.8938]])\n",
      "b.grad= tensor([-0.0005,  0.0009])\n",
      "new w tensor([[-0.3888,  0.8515,  0.6858],\n",
      "        [-0.2896,  0.8022,  0.8969]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5182, grad_fn=<DivBackward0>)\n",
      "i= 834\n",
      "w.grad= tensor([[ 0.2103,  0.3461, -0.8783],\n",
      "        [ 0.4519,  0.7431, -1.8847]])\n",
      "b.grad= tensor([-0.0005,  0.0009])\n",
      "new w tensor([[-0.3888,  0.8515,  0.6859],\n",
      "        [-0.2896,  0.8021,  0.8970]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5180, grad_fn=<DivBackward0>)\n",
      "i= 835\n",
      "w.grad= tensor([[ 0.2095,  0.3446, -0.8741],\n",
      "        [ 0.4492,  0.7389, -1.8763]])\n",
      "b.grad= tensor([-0.0004,  0.0009])\n",
      "new w tensor([[-0.3888,  0.8515,  0.6859],\n",
      "        [-0.2896,  0.8021,  0.8971]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5177, grad_fn=<DivBackward0>)\n",
      "i= 836\n",
      "w.grad= tensor([[ 0.2084,  0.3429, -0.8701],\n",
      "        [ 0.4475,  0.7359, -1.8673]])\n",
      "b.grad= tensor([-0.0004,  0.0010])\n",
      "new w tensor([[-0.3888,  0.8515,  0.6859],\n",
      "        [-0.2897,  0.8021,  0.8972]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5175, grad_fn=<DivBackward0>)\n",
      "i= 837\n",
      "w.grad= tensor([[ 0.2072,  0.3409, -0.8662],\n",
      "        [ 0.4454,  0.7325, -1.8587]])\n",
      "b.grad= tensor([-0.0004,  0.0010])\n",
      "new w tensor([[-0.3889,  0.8515,  0.6860],\n",
      "        [-0.2897,  0.8020,  0.8973]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5172, grad_fn=<DivBackward0>)\n",
      "i= 838\n",
      "w.grad= tensor([[ 0.2066,  0.3398, -0.8619],\n",
      "        [ 0.4437,  0.7295, -1.8497]])\n",
      "b.grad= tensor([-0.0004,  0.0011])\n",
      "new w tensor([[-0.3889,  0.8515,  0.6860],\n",
      "        [-0.2897,  0.8020,  0.8974]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5169, grad_fn=<DivBackward0>)\n",
      "i= 839\n",
      "w.grad= tensor([[ 0.2054,  0.3379, -0.8580],\n",
      "        [ 0.4413,  0.7259, -1.8412]])\n",
      "b.grad= tensor([-0.0004,  0.0011])\n",
      "new w tensor([[-0.3889,  0.8514,  0.6861],\n",
      "        [-0.2897,  0.8019,  0.8975]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "i= 840\n",
      "w.grad= tensor([[ 0.2048,  0.3369, -0.8537],\n",
      "        [ 0.4391,  0.7223, -1.8327]])\n",
      "b.grad= tensor([-0.0003,  0.0012])\n",
      "new w tensor([[-0.3889,  0.8514,  0.6861],\n",
      "        [-0.2898,  0.8019,  0.8976]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5164, grad_fn=<DivBackward0>)\n",
      "i= 841\n",
      "w.grad= tensor([[ 0.2035,  0.3349, -0.8500],\n",
      "        [ 0.4369,  0.7186, -1.8244]])\n",
      "b.grad= tensor([-0.0003,  0.0012])\n",
      "new w tensor([[-0.3889,  0.8514,  0.6862],\n",
      "        [-0.2898,  0.8019,  0.8977]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5162, grad_fn=<DivBackward0>)\n",
      "i= 842\n",
      "w.grad= tensor([[ 0.2026,  0.3334, -0.8460],\n",
      "        [ 0.4351,  0.7157, -1.8156]])\n",
      "b.grad= tensor([-0.0003,  0.0012])\n",
      "new w tensor([[-0.3889,  0.8514,  0.6862],\n",
      "        [-0.2898,  0.8018,  0.8978]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5159, grad_fn=<DivBackward0>)\n",
      "i= 843\n",
      "w.grad= tensor([[ 0.2017,  0.3319, -0.8420],\n",
      "        [ 0.4332,  0.7125, -1.8071]])\n",
      "b.grad= tensor([-0.0003,  0.0013])\n",
      "new w tensor([[-0.3889,  0.8514,  0.6862],\n",
      "        [-0.2898,  0.8018,  0.8979]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5157, grad_fn=<DivBackward0>)\n",
      "i= 844\n",
      "w.grad= tensor([[ 0.2010,  0.3305, -0.8380],\n",
      "        [ 0.4305,  0.7083, -1.7991]])\n",
      "b.grad= tensor([-0.0003,  0.0013])\n",
      "new w tensor([[-0.3889,  0.8514,  0.6863],\n",
      "        [-0.2898,  0.8018,  0.8979]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5155, grad_fn=<DivBackward0>)\n",
      "i= 845\n",
      "w.grad= tensor([[ 0.1999,  0.3289, -0.8341],\n",
      "        [ 0.4290,  0.7055, -1.7903]])\n",
      "b.grad= tensor([-0.0003,  0.0014])\n",
      "new w tensor([[-0.3889,  0.8513,  0.6863],\n",
      "        [-0.2899,  0.8017,  0.8980]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5152, grad_fn=<DivBackward0>)\n",
      "i= 846\n",
      "w.grad= tensor([[ 0.1990,  0.3273, -0.8303],\n",
      "        [ 0.4271,  0.7025, -1.7819]])\n",
      "b.grad= tensor([-0.0002,  0.0014])\n",
      "new w tensor([[-0.3889,  0.8513,  0.6864],\n",
      "        [-0.2899,  0.8017,  0.8981]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5150, grad_fn=<DivBackward0>)\n",
      "i= 847\n",
      "w.grad= tensor([[ 0.1977,  0.3254, -0.8266],\n",
      "        [ 0.4248,  0.6988, -1.7738]])\n",
      "b.grad= tensor([-0.0002,  0.0014])\n",
      "new w tensor([[-0.3890,  0.8513,  0.6864],\n",
      "        [-0.2899,  0.8017,  0.8982]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5148, grad_fn=<DivBackward0>)\n",
      "i= 848\n",
      "w.grad= tensor([[ 0.1970,  0.3241, -0.8226],\n",
      "        [ 0.4234,  0.6963, -1.7651]])\n",
      "b.grad= tensor([-0.0002,  0.0015])\n",
      "new w tensor([[-0.3890,  0.8513,  0.6865],\n",
      "        [-0.2899,  0.8016,  0.8983]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5145, grad_fn=<DivBackward0>)\n",
      "i= 849\n",
      "w.grad= tensor([[ 0.1961,  0.3226, -0.8187],\n",
      "        [ 0.4208,  0.6922, -1.7573]])\n",
      "b.grad= tensor([-0.0002,  0.0015])\n",
      "new w tensor([[-0.3890,  0.8513,  0.6865],\n",
      "        [-0.2899,  0.8016,  0.8984]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5143, grad_fn=<DivBackward0>)\n",
      "i= 850\n",
      "w.grad= tensor([[ 0.1951,  0.3210, -0.8150],\n",
      "        [ 0.4191,  0.6893, -1.7489]])\n",
      "b.grad= tensor([-0.0002,  0.0016])\n",
      "new w tensor([[-0.3890,  0.8513,  0.6865],\n",
      "        [-0.2900,  0.8016,  0.8985]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5141, grad_fn=<DivBackward0>)\n",
      "i= 851\n",
      "w.grad= tensor([[ 0.1945,  0.3197, -0.8110],\n",
      "        [ 0.4170,  0.6861, -1.7408]])\n",
      "b.grad= tensor([-0.0001,  0.0016])\n",
      "new w tensor([[-0.3890,  0.8512,  0.6866],\n",
      "        [-0.2900,  0.8015,  0.8986]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5138, grad_fn=<DivBackward0>)\n",
      "i= 852\n",
      "w.grad= tensor([[ 0.1934,  0.3180, -0.8073],\n",
      "        [ 0.4149,  0.6826, -1.7328]])\n",
      "b.grad= tensor([-0.0001,  0.0016])\n",
      "new w tensor([[-0.3890,  0.8512,  0.6866],\n",
      "        [-0.2900,  0.8015,  0.8986]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5136, grad_fn=<DivBackward0>)\n",
      "i= 853\n",
      "w.grad= tensor([[ 0.1926,  0.3167, -0.8035],\n",
      "        [ 0.4129,  0.6793, -1.7248]])\n",
      "b.grad= tensor([-0.0001,  0.0017])\n",
      "new w tensor([[-0.3890,  0.8512,  0.6867],\n",
      "        [-0.2900,  0.8015,  0.8987]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5134, grad_fn=<DivBackward0>)\n",
      "i= 854\n",
      "w.grad= tensor([[ 0.1915,  0.3150, -0.7998],\n",
      "        [ 0.4115,  0.6768, -1.7164]])\n",
      "b.grad= tensor([-8.8885e-05,  1.7059e-03])\n",
      "new w tensor([[-0.3890,  0.8512,  0.6867],\n",
      "        [-0.2900,  0.8014,  0.8988]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5132, grad_fn=<DivBackward0>)\n",
      "i= 855\n",
      "w.grad= tensor([[ 0.1910,  0.3141, -0.7958],\n",
      "        [ 0.4093,  0.6733, -1.7085]])\n",
      "b.grad= tensor([-6.5982e-05,  1.7418e-03])\n",
      "new w tensor([[-0.3890,  0.8512,  0.6867],\n",
      "        [-0.2901,  0.8014,  0.8989]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5130, grad_fn=<DivBackward0>)\n",
      "i= 856\n",
      "w.grad= tensor([[ 0.1900,  0.3125, -0.7922],\n",
      "        [ 0.4068,  0.6695, -1.7010]])\n",
      "b.grad= tensor([-4.9964e-05,  1.7723e-03])\n",
      "new w tensor([[-0.3890,  0.8512,  0.6868],\n",
      "        [-0.2901,  0.8014,  0.8990]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5128, grad_fn=<DivBackward0>)\n",
      "i= 857\n",
      "w.grad= tensor([[ 0.1890,  0.3109, -0.7885],\n",
      "        [ 0.4054,  0.6669, -1.6927]])\n",
      "b.grad= tensor([-3.3572e-05,  1.8158e-03])\n",
      "new w tensor([[-0.3891,  0.8511,  0.6868],\n",
      "        [-0.2901,  0.8013,  0.8991]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5125, grad_fn=<DivBackward0>)\n",
      "i= 858\n",
      "w.grad= tensor([[ 0.1879,  0.3092, -0.7851],\n",
      "        [ 0.4042,  0.6645, -1.6844]])\n",
      "b.grad= tensor([-1.9059e-05,  1.8608e-03])\n",
      "new w tensor([[-0.3891,  0.8511,  0.6869],\n",
      "        [-0.2901,  0.8013,  0.8992]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5123, grad_fn=<DivBackward0>)\n",
      "i= 859\n",
      "w.grad= tensor([[ 0.1875,  0.3082, -0.7811],\n",
      "        [ 0.4018,  0.6609, -1.6768]])\n",
      "b.grad= tensor([3.8445e-06, 1.8929e-03])\n",
      "new w tensor([[-0.3891,  0.8511,  0.6869],\n",
      "        [-0.2901,  0.8013,  0.8992]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5121, grad_fn=<DivBackward0>)\n",
      "i= 860\n",
      "w.grad= tensor([[ 0.1860,  0.3061, -0.7779],\n",
      "        [ 0.4004,  0.6584, -1.6687]])\n",
      "b.grad= tensor([1.3724e-05, 1.9356e-03])\n",
      "new w tensor([[-0.3891,  0.8511,  0.6869],\n",
      "        [-0.2902,  0.8012,  0.8993]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5119, grad_fn=<DivBackward0>)\n",
      "i= 861\n",
      "w.grad= tensor([[ 0.1857,  0.3053, -0.7739],\n",
      "        [ 0.3981,  0.6549, -1.6612]])\n",
      "b.grad= tensor([3.7000e-05, 1.9676e-03])\n",
      "new w tensor([[-0.3891,  0.8511,  0.6870],\n",
      "        [-0.2902,  0.8012,  0.8994]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5117, grad_fn=<DivBackward0>)\n",
      "i= 862\n",
      "w.grad= tensor([[ 0.1847,  0.3038, -0.7703],\n",
      "        [ 0.3959,  0.6514, -1.6536]])\n",
      "b.grad= tensor([5.3763e-05, 2.0004e-03])\n",
      "new w tensor([[-0.3891,  0.8511,  0.6870],\n",
      "        [-0.2902,  0.8012,  0.8995]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5115, grad_fn=<DivBackward0>)\n",
      "i= 863\n",
      "w.grad= tensor([[ 0.1834,  0.3019, -0.7670],\n",
      "        [ 0.3948,  0.6492, -1.6454]])\n",
      "b.grad= tensor([6.5625e-05, 2.0462e-03])\n",
      "new w tensor([[-0.3891,  0.8511,  0.6870],\n",
      "        [-0.2902,  0.8011,  0.8996]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5113, grad_fn=<DivBackward0>)\n",
      "i= 864\n",
      "w.grad= tensor([[ 0.1829,  0.3009, -0.7632],\n",
      "        [ 0.3922,  0.6453, -1.6383]])\n",
      "b.grad= tensor([8.6218e-05, 2.0729e-03])\n",
      "new w tensor([[-0.3891,  0.8510,  0.6871],\n",
      "        [-0.2902,  0.8011,  0.8997]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5111, grad_fn=<DivBackward0>)\n",
      "i= 865\n",
      "w.grad= tensor([[ 0.1821,  0.2996, -0.7596],\n",
      "        [ 0.3910,  0.6431, -1.6302]])\n",
      "b.grad= tensor([0.0001, 0.0021])\n",
      "new w tensor([[-0.3891,  0.8510,  0.6871],\n",
      "        [-0.2903,  0.8011,  0.8997]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5109, grad_fn=<DivBackward0>)\n",
      "i= 866\n",
      "w.grad= tensor([[ 0.1807,  0.2975, -0.7564],\n",
      "        [ 0.3887,  0.6395, -1.6229]])\n",
      "b.grad= tensor([0.0001, 0.0021])\n",
      "new w tensor([[-0.3891,  0.8510,  0.6872],\n",
      "        [-0.2903,  0.8010,  0.8998]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5107, grad_fn=<DivBackward0>)\n",
      "i= 867\n",
      "w.grad= tensor([[ 0.1804,  0.2967, -0.7525],\n",
      "        [ 0.3870,  0.6366, -1.6152]])\n",
      "b.grad= tensor([0.0001, 0.0022])\n",
      "new w tensor([[-0.3891,  0.8510,  0.6872],\n",
      "        [-0.2903,  0.8010,  0.8999]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5105, grad_fn=<DivBackward0>)\n",
      "i= 868\n",
      "w.grad= tensor([[ 0.1792,  0.2948, -0.7493],\n",
      "        [ 0.3855,  0.6341, -1.6075]])\n",
      "b.grad= tensor([0.0001, 0.0022])\n",
      "new w tensor([[-0.3892,  0.8510,  0.6872],\n",
      "        [-0.2903,  0.8010,  0.9000]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5103, grad_fn=<DivBackward0>)\n",
      "i= 869\n",
      "w.grad= tensor([[ 0.1790,  0.2942, -0.7454],\n",
      "        [ 0.3829,  0.6301, -1.6005]])\n",
      "b.grad= tensor([0.0002, 0.0023])\n",
      "new w tensor([[-0.3892,  0.8510,  0.6873],\n",
      "        [-0.2903,  0.8009,  0.9001]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5101, grad_fn=<DivBackward0>)\n",
      "i= 870\n",
      "w.grad= tensor([[ 0.1779,  0.2926, -0.7420],\n",
      "        [ 0.3817,  0.6280, -1.5927]])\n",
      "b.grad= tensor([0.0002, 0.0023])\n",
      "new w tensor([[-0.3892,  0.8510,  0.6873],\n",
      "        [-0.2904,  0.8009,  0.9001]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5099, grad_fn=<DivBackward0>)\n",
      "i= 871\n",
      "w.grad= tensor([[ 0.1766,  0.2908, -0.7388],\n",
      "        [ 0.3793,  0.6242, -1.5857]])\n",
      "b.grad= tensor([0.0002, 0.0023])\n",
      "new w tensor([[-0.3892,  0.8509,  0.6873],\n",
      "        [-0.2904,  0.8009,  0.9002]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5098, grad_fn=<DivBackward0>)\n",
      "i= 872\n",
      "w.grad= tensor([[ 0.1764,  0.2900, -0.7350],\n",
      "        [ 0.3779,  0.6217, -1.5781]])\n",
      "b.grad= tensor([0.0002, 0.0024])\n",
      "new w tensor([[-0.3892,  0.8509,  0.6874],\n",
      "        [-0.2904,  0.8008,  0.9003]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5096, grad_fn=<DivBackward0>)\n",
      "i= 873\n",
      "w.grad= tensor([[ 0.1754,  0.2885, -0.7317],\n",
      "        [ 0.3762,  0.6188, -1.5707]])\n",
      "b.grad= tensor([0.0002, 0.0024])\n",
      "new w tensor([[-0.3892,  0.8509,  0.6874],\n",
      "        [-0.2904,  0.8008,  0.9004]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5094, grad_fn=<DivBackward0>)\n",
      "i= 874\n",
      "w.grad= tensor([[ 0.1745,  0.2870, -0.7283],\n",
      "        [ 0.3747,  0.6161, -1.5632]])\n",
      "b.grad= tensor([0.0003, 0.0024])\n",
      "new w tensor([[-0.3892,  0.8509,  0.6875],\n",
      "        [-0.2904,  0.8008,  0.9005]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5092, grad_fn=<DivBackward0>)\n",
      "i= 875\n",
      "w.grad= tensor([[ 0.1738,  0.2858, -0.7248],\n",
      "        [ 0.3726,  0.6129, -1.5561]])\n",
      "b.grad= tensor([0.0003, 0.0025])\n",
      "new w tensor([[-0.3892,  0.8509,  0.6875],\n",
      "        [-0.2905,  0.8007,  0.9005]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5090, grad_fn=<DivBackward0>)\n",
      "i= 876\n",
      "w.grad= tensor([[ 0.1726,  0.2842, -0.7217],\n",
      "        [ 0.3712,  0.6105, -1.5486]])\n",
      "b.grad= tensor([0.0003, 0.0025])\n",
      "new w tensor([[-0.3892,  0.8509,  0.6875],\n",
      "        [-0.2905,  0.8007,  0.9006]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5088, grad_fn=<DivBackward0>)\n",
      "i= 877\n",
      "w.grad= tensor([[ 0.1722,  0.2833, -0.7181],\n",
      "        [ 0.3692,  0.6073, -1.5416]])\n",
      "b.grad= tensor([0.0003, 0.0025])\n",
      "new w tensor([[-0.3892,  0.8509,  0.6876],\n",
      "        [-0.2905,  0.8007,  0.9007]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5087, grad_fn=<DivBackward0>)\n",
      "i= 878\n",
      "w.grad= tensor([[ 0.1708,  0.2812, -0.7151],\n",
      "        [ 0.3677,  0.6047, -1.5342]])\n",
      "b.grad= tensor([0.0003, 0.0026])\n",
      "new w tensor([[-0.3892,  0.8508,  0.6876],\n",
      "        [-0.2905,  0.8007,  0.9008]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5085, grad_fn=<DivBackward0>)\n",
      "i= 879\n",
      "w.grad= tensor([[ 0.1706,  0.2805, -0.7114],\n",
      "        [ 0.3665,  0.6026, -1.5267]])\n",
      "b.grad= tensor([0.0003, 0.0026])\n",
      "new w tensor([[-0.3892,  0.8508,  0.6876],\n",
      "        [-0.2905,  0.8006,  0.9008]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5083, grad_fn=<DivBackward0>)\n",
      "i= 880\n",
      "w.grad= tensor([[ 0.1697,  0.2792, -0.7081],\n",
      "        [ 0.3642,  0.5991, -1.5199]])\n",
      "b.grad= tensor([0.0003, 0.0026])\n",
      "new w tensor([[-0.3893,  0.8508,  0.6877],\n",
      "        [-0.2906,  0.8006,  0.9009]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5082, grad_fn=<DivBackward0>)\n",
      "i= 881\n",
      "w.grad= tensor([[ 0.1689,  0.2779, -0.7048],\n",
      "        [ 0.3620,  0.5957, -1.5132]])\n",
      "b.grad= tensor([0.0004, 0.0027])\n",
      "new w tensor([[-0.3893,  0.8508,  0.6877],\n",
      "        [-0.2906,  0.8006,  0.9010]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5080, grad_fn=<DivBackward0>)\n",
      "i= 882\n",
      "w.grad= tensor([[ 0.1682,  0.2766, -0.7015],\n",
      "        [ 0.3611,  0.5939, -1.5056]])\n",
      "b.grad= tensor([0.0004, 0.0027])\n",
      "new w tensor([[-0.3893,  0.8508,  0.6877],\n",
      "        [-0.2906,  0.8005,  0.9011]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5078, grad_fn=<DivBackward0>)\n",
      "i= 883\n",
      "w.grad= tensor([[ 0.1674,  0.2754, -0.6982],\n",
      "        [ 0.3589,  0.5905, -1.4989]])\n",
      "b.grad= tensor([0.0004, 0.0027])\n",
      "new w tensor([[-0.3893,  0.8508,  0.6878],\n",
      "        [-0.2906,  0.8005,  0.9011]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5076, grad_fn=<DivBackward0>)\n",
      "i= 884\n",
      "w.grad= tensor([[ 0.1666,  0.2741, -0.6950],\n",
      "        [ 0.3573,  0.5877, -1.4919]])\n",
      "b.grad= tensor([0.0004, 0.0028])\n",
      "new w tensor([[-0.3893,  0.8508,  0.6878],\n",
      "        [-0.2906,  0.8005,  0.9012]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5075, grad_fn=<DivBackward0>)\n",
      "i= 885\n",
      "w.grad= tensor([[ 0.1656,  0.2725, -0.6919],\n",
      "        [ 0.3562,  0.5855, -1.4846]])\n",
      "b.grad= tensor([0.0004, 0.0028])\n",
      "new w tensor([[-0.3893,  0.8507,  0.6878],\n",
      "        [-0.2906,  0.8004,  0.9013]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5073, grad_fn=<DivBackward0>)\n",
      "i= 886\n",
      "w.grad= tensor([[ 0.1645,  0.2709, -0.6889],\n",
      "        [ 0.3541,  0.5826, -1.4779]])\n",
      "b.grad= tensor([0.0004, 0.0028])\n",
      "new w tensor([[-0.3893,  0.8507,  0.6879],\n",
      "        [-0.2907,  0.8004,  0.9014]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5072, grad_fn=<DivBackward0>)\n",
      "i= 887\n",
      "w.grad= tensor([[ 0.1645,  0.2705, -0.6852],\n",
      "        [ 0.3522,  0.5797, -1.4711]])\n",
      "b.grad= tensor([0.0005, 0.0029])\n",
      "new w tensor([[-0.3893,  0.8507,  0.6879],\n",
      "        [-0.2907,  0.8004,  0.9014]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5070, grad_fn=<DivBackward0>)\n",
      "i= 888\n",
      "w.grad= tensor([[ 0.1634,  0.2688, -0.6822],\n",
      "        [ 0.3506,  0.5768, -1.4643]])\n",
      "b.grad= tensor([0.0005, 0.0029])\n",
      "new w tensor([[-0.3893,  0.8507,  0.6879],\n",
      "        [-0.2907,  0.8004,  0.9015]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5068, grad_fn=<DivBackward0>)\n",
      "i= 889\n",
      "w.grad= tensor([[ 0.1624,  0.2673, -0.6792],\n",
      "        [ 0.3493,  0.5744, -1.4572]])\n",
      "b.grad= tensor([0.0005, 0.0029])\n",
      "new w tensor([[-0.3893,  0.8507,  0.6880],\n",
      "        [-0.2907,  0.8003,  0.9016]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5067, grad_fn=<DivBackward0>)\n",
      "i= 890\n",
      "w.grad= tensor([[ 0.1621,  0.2666, -0.6756],\n",
      "        [ 0.3473,  0.5714, -1.4506]])\n",
      "b.grad= tensor([0.0005, 0.0030])\n",
      "new w tensor([[-0.3893,  0.8507,  0.6880],\n",
      "        [-0.2907,  0.8003,  0.9017]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5065, grad_fn=<DivBackward0>)\n",
      "i= 891\n",
      "w.grad= tensor([[ 0.1614,  0.2653, -0.6725],\n",
      "        [ 0.3464,  0.5696, -1.4434]])\n",
      "b.grad= tensor([0.0005, 0.0030])\n",
      "new w tensor([[-0.3893,  0.8507,  0.6880],\n",
      "        [-0.2907,  0.8003,  0.9017]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5064, grad_fn=<DivBackward0>)\n",
      "i= 892\n",
      "w.grad= tensor([[ 0.1598,  0.2631, -0.6699],\n",
      "        [ 0.3435,  0.5654, -1.4375]])\n",
      "b.grad= tensor([0.0005, 0.0030])\n",
      "new w tensor([[-0.3894,  0.8506,  0.6881],\n",
      "        [-0.2908,  0.8002,  0.9018]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5062, grad_fn=<DivBackward0>)\n",
      "i= 893\n",
      "w.grad= tensor([[ 0.1598,  0.2628, -0.6663],\n",
      "        [ 0.3433,  0.5644, -1.4299]])\n",
      "b.grad= tensor([0.0005, 0.0031])\n",
      "new w tensor([[-0.3894,  0.8506,  0.6881],\n",
      "        [-0.2908,  0.8002,  0.9019]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5061, grad_fn=<DivBackward0>)\n",
      "i= 894\n",
      "w.grad= tensor([[ 0.1589,  0.2613, -0.6633],\n",
      "        [ 0.3411,  0.5611, -1.4236]])\n",
      "b.grad= tensor([0.0006, 0.0031])\n",
      "new w tensor([[-0.3894,  0.8506,  0.6881],\n",
      "        [-0.2908,  0.8002,  0.9019]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5059, grad_fn=<DivBackward0>)\n",
      "i= 895\n",
      "w.grad= tensor([[ 0.1581,  0.2599, -0.6603],\n",
      "        [ 0.3394,  0.5584, -1.4170]])\n",
      "b.grad= tensor([0.0006, 0.0031])\n",
      "new w tensor([[-0.3894,  0.8506,  0.6882],\n",
      "        [-0.2908,  0.8002,  0.9020]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5058, grad_fn=<DivBackward0>)\n",
      "i= 896\n",
      "w.grad= tensor([[ 0.1579,  0.2595, -0.6568],\n",
      "        [ 0.3383,  0.5563, -1.4101]])\n",
      "b.grad= tensor([0.0006, 0.0032])\n",
      "new w tensor([[-0.3894,  0.8506,  0.6882],\n",
      "        [-0.2908,  0.8001,  0.9021]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5056, grad_fn=<DivBackward0>)\n",
      "i= 897\n",
      "w.grad= tensor([[ 0.1567,  0.2578, -0.6540],\n",
      "        [ 0.3363,  0.5532, -1.4039]])\n",
      "b.grad= tensor([0.0006, 0.0032])\n",
      "new w tensor([[-0.3894,  0.8506,  0.6882],\n",
      "        [-0.2908,  0.8001,  0.9022]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5055, grad_fn=<DivBackward0>)\n",
      "i= 898\n",
      "w.grad= tensor([[ 0.1559,  0.2565, -0.6510],\n",
      "        [ 0.3347,  0.5506, -1.3973]])\n",
      "b.grad= tensor([0.0006, 0.0032])\n",
      "new w tensor([[-0.3894,  0.8506,  0.6883],\n",
      "        [-0.2909,  0.8001,  0.9022]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5053, grad_fn=<DivBackward0>)\n",
      "i= 899\n",
      "w.grad= tensor([[ 0.1552,  0.2554, -0.6480],\n",
      "        [ 0.3333,  0.5484, -1.3906]])\n",
      "b.grad= tensor([0.0006, 0.0033])\n",
      "new w tensor([[-0.3894,  0.8506,  0.6883],\n",
      "        [-0.2909,  0.8001,  0.9023]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5052, grad_fn=<DivBackward0>)\n",
      "i= 900\n",
      "w.grad= tensor([[ 0.1547,  0.2544, -0.6448],\n",
      "        [ 0.3316,  0.5456, -1.3842]])\n",
      "b.grad= tensor([0.0007, 0.0033])\n",
      "new w tensor([[-0.3894,  0.8505,  0.6883],\n",
      "        [-0.2909,  0.8000,  0.9024]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5050, grad_fn=<DivBackward0>)\n",
      "i= 901\n",
      "w.grad= tensor([[ 0.1539,  0.2530, -0.6418],\n",
      "        [ 0.3301,  0.5430, -1.3778]])\n",
      "b.grad= tensor([0.0007, 0.0033])\n",
      "new w tensor([[-0.3894,  0.8505,  0.6884],\n",
      "        [-0.2909,  0.8000,  0.9024]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5049, grad_fn=<DivBackward0>)\n",
      "i= 902\n",
      "w.grad= tensor([[ 0.1530,  0.2517, -0.6390],\n",
      "        [ 0.3280,  0.5397, -1.3717]])\n",
      "b.grad= tensor([0.0007, 0.0033])\n",
      "new w tensor([[-0.3894,  0.8505,  0.6884],\n",
      "        [-0.2909,  0.8000,  0.9025]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5048, grad_fn=<DivBackward0>)\n",
      "i= 903\n",
      "w.grad= tensor([[ 0.1523,  0.2506, -0.6359],\n",
      "        [ 0.3272,  0.5382, -1.3648]])\n",
      "b.grad= tensor([0.0007, 0.0034])\n",
      "new w tensor([[-0.3894,  0.8505,  0.6884],\n",
      "        [-0.2909,  0.7999,  0.9026]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5046, grad_fn=<DivBackward0>)\n",
      "i= 904\n",
      "w.grad= tensor([[ 0.1517,  0.2496, -0.6330],\n",
      "        [ 0.3256,  0.5355, -1.3585]])\n",
      "b.grad= tensor([0.0007, 0.0034])\n",
      "new w tensor([[-0.3894,  0.8505,  0.6885],\n",
      "        [-0.2910,  0.7999,  0.9026]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5045, grad_fn=<DivBackward0>)\n",
      "i= 905\n",
      "w.grad= tensor([[ 0.1514,  0.2488, -0.6297],\n",
      "        [ 0.3246,  0.5337, -1.3518]])\n",
      "b.grad= tensor([0.0007, 0.0034])\n",
      "new w tensor([[-0.3895,  0.8505,  0.6885],\n",
      "        [-0.2910,  0.7999,  0.9027]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5043, grad_fn=<DivBackward0>)\n",
      "i= 906\n",
      "w.grad= tensor([[ 0.1498,  0.2468, -0.6273],\n",
      "        [ 0.3221,  0.5301, -1.3462]])\n",
      "b.grad= tensor([0.0007, 0.0035])\n",
      "new w tensor([[-0.3895,  0.8505,  0.6885],\n",
      "        [-0.2910,  0.7999,  0.9028]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5042, grad_fn=<DivBackward0>)\n",
      "i= 907\n",
      "w.grad= tensor([[ 0.1496,  0.2462, -0.6241],\n",
      "        [ 0.3205,  0.5275, -1.3399]])\n",
      "b.grad= tensor([0.0007, 0.0035])\n",
      "new w tensor([[-0.3895,  0.8505,  0.6886],\n",
      "        [-0.2910,  0.7998,  0.9028]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5041, grad_fn=<DivBackward0>)\n",
      "i= 908\n",
      "w.grad= tensor([[ 0.1492,  0.2453, -0.6210],\n",
      "        [ 0.3199,  0.5259, -1.3331]])\n",
      "b.grad= tensor([0.0008, 0.0035])\n",
      "new w tensor([[-0.3895,  0.8504,  0.6886],\n",
      "        [-0.2910,  0.7998,  0.9029]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5039, grad_fn=<DivBackward0>)\n",
      "i= 909\n",
      "w.grad= tensor([[ 0.1479,  0.2435, -0.6185],\n",
      "        [ 0.3176,  0.5228, -1.3274]])\n",
      "b.grad= tensor([0.0008, 0.0035])\n",
      "new w tensor([[-0.3895,  0.8504,  0.6886],\n",
      "        [-0.2910,  0.7998,  0.9030]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5038, grad_fn=<DivBackward0>)\n",
      "i= 910\n",
      "w.grad= tensor([[ 0.1478,  0.2429, -0.6152],\n",
      "        [ 0.3162,  0.5202, -1.3211]])\n",
      "b.grad= tensor([0.0008, 0.0036])\n",
      "new w tensor([[-0.3895,  0.8504,  0.6887],\n",
      "        [-0.2911,  0.7998,  0.9030]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5037, grad_fn=<DivBackward0>)\n",
      "i= 911\n",
      "w.grad= tensor([[ 0.1464,  0.2411, -0.6128],\n",
      "        [ 0.3153,  0.5186, -1.3145]])\n",
      "b.grad= tensor([0.0008, 0.0036])\n",
      "new w tensor([[-0.3895,  0.8504,  0.6887],\n",
      "        [-0.2911,  0.7997,  0.9031]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5036, grad_fn=<DivBackward0>)\n",
      "i= 912\n",
      "w.grad= tensor([[ 0.1462,  0.2404, -0.6096],\n",
      "        [ 0.3134,  0.5157, -1.3087]])\n",
      "b.grad= tensor([0.0008, 0.0036])\n",
      "new w tensor([[-0.3895,  0.8504,  0.6887],\n",
      "        [-0.2911,  0.7997,  0.9032]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5034, grad_fn=<DivBackward0>)\n",
      "i= 913\n",
      "w.grad= tensor([[ 0.1452,  0.2388, -0.6071],\n",
      "        [ 0.3119,  0.5131, -1.3027]])\n",
      "b.grad= tensor([0.0008, 0.0037])\n",
      "new w tensor([[-0.3895,  0.8504,  0.6888],\n",
      "        [-0.2911,  0.7997,  0.9032]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5033, grad_fn=<DivBackward0>)\n",
      "i= 914\n",
      "w.grad= tensor([[ 0.1449,  0.2382, -0.6039],\n",
      "        [ 0.3111,  0.5115, -1.2962]])\n",
      "b.grad= tensor([0.0008, 0.0037])\n",
      "new w tensor([[-0.3895,  0.8504,  0.6888],\n",
      "        [-0.2911,  0.7997,  0.9033]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5032, grad_fn=<DivBackward0>)\n",
      "i= 915\n",
      "w.grad= tensor([[ 0.1441,  0.2370, -0.6012],\n",
      "        [ 0.3090,  0.5085, -1.2905]])\n",
      "b.grad= tensor([0.0009, 0.0037])\n",
      "new w tensor([[-0.3895,  0.8504,  0.6888],\n",
      "        [-0.2911,  0.7996,  0.9034]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5031, grad_fn=<DivBackward0>)\n",
      "i= 916\n",
      "w.grad= tensor([[ 0.1431,  0.2356, -0.5986],\n",
      "        [ 0.3079,  0.5065, -1.2842]])\n",
      "b.grad= tensor([0.0009, 0.0038])\n",
      "new w tensor([[-0.3895,  0.8503,  0.6888],\n",
      "        [-0.2912,  0.7996,  0.9034]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5029, grad_fn=<DivBackward0>)\n",
      "i= 917\n",
      "w.grad= tensor([[ 0.1426,  0.2347, -0.5957],\n",
      "        [ 0.3058,  0.5033, -1.2787]])\n",
      "b.grad= tensor([0.0009, 0.0038])\n",
      "new w tensor([[-0.3895,  0.8503,  0.6889],\n",
      "        [-0.2912,  0.7996,  0.9035]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5028, grad_fn=<DivBackward0>)\n",
      "i= 918\n",
      "w.grad= tensor([[ 0.1418,  0.2334, -0.5930],\n",
      "        [ 0.3049,  0.5016, -1.2723]])\n",
      "b.grad= tensor([0.0009, 0.0038])\n",
      "new w tensor([[-0.3896,  0.8503,  0.6889],\n",
      "        [-0.2912,  0.7996,  0.9036]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5027, grad_fn=<DivBackward0>)\n",
      "i= 919\n",
      "w.grad= tensor([[ 0.1412,  0.2324, -0.5902],\n",
      "        [ 0.3032,  0.4990, -1.2666]])\n",
      "b.grad= tensor([0.0009, 0.0038])\n",
      "new w tensor([[-0.3896,  0.8503,  0.6889],\n",
      "        [-0.2912,  0.7995,  0.9036]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5026, grad_fn=<DivBackward0>)\n",
      "i= 920\n",
      "w.grad= tensor([[ 0.1406,  0.2313, -0.5874],\n",
      "        [ 0.3014,  0.4961, -1.2610]])\n",
      "b.grad= tensor([0.0009, 0.0039])\n",
      "new w tensor([[-0.3896,  0.8503,  0.6890],\n",
      "        [-0.2912,  0.7995,  0.9037]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5024, grad_fn=<DivBackward0>)\n",
      "i= 921\n",
      "w.grad= tensor([[ 0.1403,  0.2307, -0.5844],\n",
      "        [ 0.3010,  0.4950, -1.2544]])\n",
      "b.grad= tensor([0.0009, 0.0039])\n",
      "new w tensor([[-0.3896,  0.8503,  0.6890],\n",
      "        [-0.2912,  0.7995,  0.9037]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5023, grad_fn=<DivBackward0>)\n",
      "i= 922\n",
      "w.grad= tensor([[ 0.1392,  0.2291, -0.5819],\n",
      "        [ 0.2991,  0.4921, -1.2489]])\n",
      "b.grad= tensor([0.0009, 0.0039])\n",
      "new w tensor([[-0.3896,  0.8503,  0.6890],\n",
      "        [-0.2912,  0.7995,  0.9038]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5022, grad_fn=<DivBackward0>)\n",
      "i= 923\n",
      "w.grad= tensor([[ 0.1387,  0.2282, -0.5792],\n",
      "        [ 0.2975,  0.4896, -1.2432]])\n",
      "b.grad= tensor([0.0010, 0.0039])\n",
      "new w tensor([[-0.3896,  0.8503,  0.6890],\n",
      "        [-0.2913,  0.7994,  0.9039]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5021, grad_fn=<DivBackward0>)\n",
      "i= 924\n",
      "w.grad= tensor([[ 0.1380,  0.2271, -0.5765],\n",
      "        [ 0.2963,  0.4876, -1.2373]])\n",
      "b.grad= tensor([0.0010, 0.0040])\n",
      "new w tensor([[-0.3896,  0.8503,  0.6891],\n",
      "        [-0.2913,  0.7994,  0.9039]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5020, grad_fn=<DivBackward0>)\n",
      "i= 925\n",
      "w.grad= tensor([[ 0.1373,  0.2260, -0.5739],\n",
      "        [ 0.2951,  0.4854, -1.2314]])\n",
      "b.grad= tensor([0.0010, 0.0040])\n",
      "new w tensor([[-0.3896,  0.8502,  0.6891],\n",
      "        [-0.2913,  0.7994,  0.9040]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5019, grad_fn=<DivBackward0>)\n",
      "i= 926\n",
      "w.grad= tensor([[ 0.1365,  0.2248, -0.5713],\n",
      "        [ 0.2935,  0.4830, -1.2258]])\n",
      "b.grad= tensor([0.0010, 0.0040])\n",
      "new w tensor([[-0.3896,  0.8502,  0.6891],\n",
      "        [-0.2913,  0.7994,  0.9041]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5018, grad_fn=<DivBackward0>)\n",
      "i= 927\n",
      "w.grad= tensor([[ 0.1359,  0.2237, -0.5686],\n",
      "        [ 0.2925,  0.4810, -1.2199]])\n",
      "b.grad= tensor([0.0010, 0.0041])\n",
      "new w tensor([[-0.3896,  0.8502,  0.6892],\n",
      "        [-0.2913,  0.7993,  0.9041]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5016, grad_fn=<DivBackward0>)\n",
      "i= 928\n",
      "w.grad= tensor([[ 0.1353,  0.2226, -0.5659],\n",
      "        [ 0.2911,  0.4789, -1.2142]])\n",
      "b.grad= tensor([0.0010, 0.0041])\n",
      "new w tensor([[-0.3896,  0.8502,  0.6892],\n",
      "        [-0.2913,  0.7993,  0.9042]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5015, grad_fn=<DivBackward0>)\n",
      "i= 929\n",
      "w.grad= tensor([[ 0.1352,  0.2222, -0.5629],\n",
      "        [ 0.2893,  0.4761, -1.2088]])\n",
      "b.grad= tensor([0.0010, 0.0041])\n",
      "new w tensor([[-0.3896,  0.8502,  0.6892],\n",
      "        [-0.2913,  0.7993,  0.9042]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5014, grad_fn=<DivBackward0>)\n",
      "i= 930\n",
      "w.grad= tensor([[ 0.1343,  0.2208, -0.5605],\n",
      "        [ 0.2885,  0.4745, -1.2027]])\n",
      "b.grad= tensor([0.0010, 0.0041])\n",
      "new w tensor([[-0.3896,  0.8502,  0.6892],\n",
      "        [-0.2914,  0.7993,  0.9043]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5013, grad_fn=<DivBackward0>)\n",
      "i= 931\n",
      "w.grad= tensor([[ 0.1339,  0.2201, -0.5577],\n",
      "        [ 0.2871,  0.4723, -1.1971]])\n",
      "b.grad= tensor([0.0011, 0.0042])\n",
      "new w tensor([[-0.3896,  0.8502,  0.6893],\n",
      "        [-0.2914,  0.7992,  0.9044]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5012, grad_fn=<DivBackward0>)\n",
      "i= 932\n",
      "w.grad= tensor([[ 0.1328,  0.2186, -0.5554],\n",
      "        [ 0.2854,  0.4696, -1.1919]])\n",
      "b.grad= tensor([0.0011, 0.0042])\n",
      "new w tensor([[-0.3896,  0.8502,  0.6893],\n",
      "        [-0.2914,  0.7992,  0.9044]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5011, grad_fn=<DivBackward0>)\n",
      "i= 933\n",
      "w.grad= tensor([[ 0.1324,  0.2177, -0.5527],\n",
      "        [ 0.2840,  0.4674, -1.1863]])\n",
      "b.grad= tensor([0.0011, 0.0042])\n",
      "new w tensor([[-0.3897,  0.8502,  0.6893],\n",
      "        [-0.2914,  0.7992,  0.9045]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5010, grad_fn=<DivBackward0>)\n",
      "i= 934\n",
      "w.grad= tensor([[ 0.1318,  0.2167, -0.5501],\n",
      "        [ 0.2829,  0.4654, -1.1806]])\n",
      "b.grad= tensor([0.0011, 0.0042])\n",
      "new w tensor([[-0.3897,  0.8501,  0.6894],\n",
      "        [-0.2914,  0.7992,  0.9045]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5009, grad_fn=<DivBackward0>)\n",
      "i= 935\n",
      "w.grad= tensor([[ 0.1311,  0.2156, -0.5476],\n",
      "        [ 0.2815,  0.4632, -1.1752]])\n",
      "b.grad= tensor([0.0011, 0.0043])\n",
      "new w tensor([[-0.3897,  0.8501,  0.6894],\n",
      "        [-0.2914,  0.7991,  0.9046]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5008, grad_fn=<DivBackward0>)\n",
      "i= 936\n",
      "w.grad= tensor([[ 0.1303,  0.2145, -0.5451],\n",
      "        [ 0.2803,  0.4610, -1.1697]])\n",
      "b.grad= tensor([0.0011, 0.0043])\n",
      "new w tensor([[-0.3897,  0.8501,  0.6894],\n",
      "        [-0.2914,  0.7991,  0.9047]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5007, grad_fn=<DivBackward0>)\n",
      "i= 937\n",
      "w.grad= tensor([[ 0.1299,  0.2137, -0.5425],\n",
      "        [ 0.2784,  0.4582, -1.1646]])\n",
      "b.grad= tensor([0.0011, 0.0043])\n",
      "new w tensor([[-0.3897,  0.8501,  0.6894],\n",
      "        [-0.2915,  0.7991,  0.9047]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5006, grad_fn=<DivBackward0>)\n",
      "i= 938\n",
      "w.grad= tensor([[ 0.1293,  0.2126, -0.5400],\n",
      "        [ 0.2778,  0.4568, -1.1587]])\n",
      "b.grad= tensor([0.0011, 0.0044])\n",
      "new w tensor([[-0.3897,  0.8501,  0.6895],\n",
      "        [-0.2915,  0.7991,  0.9048]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5005, grad_fn=<DivBackward0>)\n",
      "i= 939\n",
      "w.grad= tensor([[ 0.1291,  0.2122, -0.5371],\n",
      "        [ 0.2764,  0.4545, -1.1534]])\n",
      "b.grad= tensor([0.0012, 0.0044])\n",
      "new w tensor([[-0.3897,  0.8501,  0.6895],\n",
      "        [-0.2915,  0.7991,  0.9048]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5004, grad_fn=<DivBackward0>)\n",
      "i= 940\n",
      "w.grad= tensor([[ 0.1282,  0.2109, -0.5348],\n",
      "        [ 0.2756,  0.4530, -1.1476]])\n",
      "b.grad= tensor([0.0012, 0.0044])\n",
      "new w tensor([[-0.3897,  0.8501,  0.6895],\n",
      "        [-0.2915,  0.7990,  0.9049]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5003, grad_fn=<DivBackward0>)\n",
      "i= 941\n",
      "w.grad= tensor([[ 0.1274,  0.2096, -0.5324],\n",
      "        [ 0.2737,  0.4503, -1.1426]])\n",
      "b.grad= tensor([0.0012, 0.0044])\n",
      "new w tensor([[-0.3897,  0.8501,  0.6895],\n",
      "        [-0.2915,  0.7990,  0.9049]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5002, grad_fn=<DivBackward0>)\n",
      "i= 942\n",
      "w.grad= tensor([[ 0.1270,  0.2089, -0.5298],\n",
      "        [ 0.2722,  0.4478, -1.1375]])\n",
      "b.grad= tensor([0.0012, 0.0045])\n",
      "new w tensor([[-0.3897,  0.8501,  0.6896],\n",
      "        [-0.2915,  0.7990,  0.9050]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5001, grad_fn=<DivBackward0>)\n",
      "i= 943\n",
      "w.grad= tensor([[ 0.1263,  0.2078, -0.5274],\n",
      "        [ 0.2708,  0.4457, -1.1322]])\n",
      "b.grad= tensor([0.0012, 0.0045])\n",
      "new w tensor([[-0.3897,  0.8500,  0.6896],\n",
      "        [-0.2915,  0.7990,  0.9051]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.5000, grad_fn=<DivBackward0>)\n",
      "i= 944\n",
      "w.grad= tensor([[ 0.1258,  0.2068, -0.5250],\n",
      "        [ 0.2704,  0.4445, -1.1264]])\n",
      "b.grad= tensor([0.0012, 0.0045])\n",
      "new w tensor([[-0.3897,  0.8500,  0.6896],\n",
      "        [-0.2916,  0.7989,  0.9051]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4999, grad_fn=<DivBackward0>)\n",
      "i= 945\n",
      "w.grad= tensor([[ 0.1250,  0.2057, -0.5226],\n",
      "        [ 0.2688,  0.4421, -1.1214]])\n",
      "b.grad= tensor([0.0012, 0.0045])\n",
      "new w tensor([[-0.3897,  0.8500,  0.6897],\n",
      "        [-0.2916,  0.7989,  0.9052]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4998, grad_fn=<DivBackward0>)\n",
      "i= 946\n",
      "w.grad= tensor([[ 0.1240,  0.2043, -0.5205],\n",
      "        [ 0.2669,  0.4392, -1.1165]])\n",
      "b.grad= tensor([0.0012, 0.0045])\n",
      "new w tensor([[-0.3897,  0.8500,  0.6897],\n",
      "        [-0.2916,  0.7989,  0.9052]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4997, grad_fn=<DivBackward0>)\n",
      "i= 947\n",
      "w.grad= tensor([[ 0.1245,  0.2045, -0.5173],\n",
      "        [ 0.2666,  0.4383, -1.1107]])\n",
      "b.grad= tensor([0.0013, 0.0046])\n",
      "new w tensor([[-0.3897,  0.8500,  0.6897],\n",
      "        [-0.2916,  0.7989,  0.9053]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4996, grad_fn=<DivBackward0>)\n",
      "i= 948\n",
      "w.grad= tensor([[ 0.1234,  0.2030, -0.5152],\n",
      "        [ 0.2650,  0.4360, -1.1057]])\n",
      "b.grad= tensor([0.0013, 0.0046])\n",
      "new w tensor([[-0.3897,  0.8500,  0.6897],\n",
      "        [-0.2916,  0.7989,  0.9053]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4996, grad_fn=<DivBackward0>)\n",
      "i= 949\n",
      "w.grad= tensor([[ 0.1228,  0.2021, -0.5129],\n",
      "        [ 0.2633,  0.4332, -1.1009]])\n",
      "b.grad= tensor([0.0013, 0.0046])\n",
      "new w tensor([[-0.3898,  0.8500,  0.6898],\n",
      "        [-0.2916,  0.7988,  0.9054]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4995, grad_fn=<DivBackward0>)\n",
      "i= 950\n",
      "w.grad= tensor([[ 0.1221,  0.2010, -0.5106],\n",
      "        [ 0.2630,  0.4323, -1.0952]])\n",
      "b.grad= tensor([0.0013, 0.0047])\n",
      "new w tensor([[-0.3898,  0.8500,  0.6898],\n",
      "        [-0.2916,  0.7988,  0.9054]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4994, grad_fn=<DivBackward0>)\n",
      "i= 951\n",
      "w.grad= tensor([[ 0.1214,  0.1998, -0.5083],\n",
      "        [ 0.2611,  0.4296, -1.0905]])\n",
      "b.grad= tensor([0.0013, 0.0047])\n",
      "new w tensor([[-0.3898,  0.8500,  0.6898],\n",
      "        [-0.2916,  0.7988,  0.9055]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4993, grad_fn=<DivBackward0>)\n",
      "i= 952\n",
      "w.grad= tensor([[ 0.1205,  0.1987, -0.5061],\n",
      "        [ 0.2603,  0.4281, -1.0851]])\n",
      "b.grad= tensor([0.0013, 0.0047])\n",
      "new w tensor([[-0.3898,  0.8500,  0.6898],\n",
      "        [-0.2917,  0.7988,  0.9056]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss= tensor(0.4992, grad_fn=<DivBackward0>)\n",
      "i= 953\n",
      "w.grad= tensor([[ 0.1205,  0.1983, -0.5033],\n",
      "        [ 0.2587,  0.4256, -1.0803]])\n",
      "b.grad= tensor([0.0013, 0.0047])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6899],\n",
      "        [-0.2917,  0.7987,  0.9056]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4991, grad_fn=<DivBackward0>)\n",
      "i= 954\n",
      "w.grad= tensor([[ 0.1201,  0.1974, -0.5009],\n",
      "        [ 0.2573,  0.4234, -1.0754]])\n",
      "b.grad= tensor([0.0013, 0.0047])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6899],\n",
      "        [-0.2917,  0.7987,  0.9057]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4990, grad_fn=<DivBackward0>)\n",
      "i= 955\n",
      "w.grad= tensor([[ 0.1193,  0.1964, -0.4987],\n",
      "        [ 0.2561,  0.4214, -1.0704]])\n",
      "b.grad= tensor([0.0013, 0.0048])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6899],\n",
      "        [-0.2917,  0.7987,  0.9057]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4989, grad_fn=<DivBackward0>)\n",
      "i= 956\n",
      "w.grad= tensor([[ 0.1187,  0.1952, -0.4965],\n",
      "        [ 0.2554,  0.4201, -1.0651]])\n",
      "b.grad= tensor([0.0013, 0.0048])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6899],\n",
      "        [-0.2917,  0.7987,  0.9058]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4989, grad_fn=<DivBackward0>)\n",
      "i= 957\n",
      "w.grad= tensor([[ 0.1187,  0.1950, -0.4938],\n",
      "        [ 0.2541,  0.4180, -1.0601]])\n",
      "b.grad= tensor([0.0014, 0.0048])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6900],\n",
      "        [-0.2917,  0.7987,  0.9058]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4988, grad_fn=<DivBackward0>)\n",
      "i= 958\n",
      "w.grad= tensor([[ 0.1173,  0.1931, -0.4921],\n",
      "        [ 0.2529,  0.4161, -1.0552]])\n",
      "b.grad= tensor([0.0014, 0.0048])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6900],\n",
      "        [-0.2917,  0.7986,  0.9059]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4987, grad_fn=<DivBackward0>)\n",
      "i= 959\n",
      "w.grad= tensor([[ 0.1175,  0.1931, -0.4892],\n",
      "        [ 0.2514,  0.4137, -1.0505]])\n",
      "b.grad= tensor([0.0014, 0.0049])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6900],\n",
      "        [-0.2918,  0.7986,  0.9059]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4986, grad_fn=<DivBackward0>)\n",
      "i= 960\n",
      "w.grad= tensor([[ 0.1161,  0.1912, -0.4876],\n",
      "        [ 0.2502,  0.4118, -1.0456]])\n",
      "b.grad= tensor([0.0014, 0.0049])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6900],\n",
      "        [-0.2918,  0.7986,  0.9060]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4985, grad_fn=<DivBackward0>)\n",
      "i= 961\n",
      "w.grad= tensor([[ 0.1163,  0.1911, -0.4848],\n",
      "        [ 0.2496,  0.4104, -1.0404]])\n",
      "b.grad= tensor([0.0014, 0.0049])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6901],\n",
      "        [-0.2918,  0.7986,  0.9060]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4984, grad_fn=<DivBackward0>)\n",
      "i= 962\n",
      "w.grad= tensor([[ 0.1159,  0.1904, -0.4824],\n",
      "        [ 0.2486,  0.4087, -1.0354]])\n",
      "b.grad= tensor([0.0014, 0.0049])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6901],\n",
      "        [-0.2918,  0.7986,  0.9061]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4984, grad_fn=<DivBackward0>)\n",
      "i= 963\n",
      "w.grad= tensor([[ 0.1152,  0.1894, -0.4803],\n",
      "        [ 0.2470,  0.4065, -1.0308]])\n",
      "b.grad= tensor([0.0014, 0.0050])\n",
      "new w tensor([[-0.3898,  0.8499,  0.6901],\n",
      "        [-0.2918,  0.7985,  0.9061]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4983, grad_fn=<DivBackward0>)\n",
      "i= 964\n",
      "w.grad= tensor([[ 0.1146,  0.1883, -0.4781],\n",
      "        [ 0.2460,  0.4047, -1.0259]])\n",
      "b.grad= tensor([0.0014, 0.0050])\n",
      "new w tensor([[-0.3898,  0.8498,  0.6901],\n",
      "        [-0.2918,  0.7985,  0.9062]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4982, grad_fn=<DivBackward0>)\n",
      "i= 965\n",
      "w.grad= tensor([[ 0.1142,  0.1876, -0.4758],\n",
      "        [ 0.2445,  0.4024, -1.0214]])\n",
      "b.grad= tensor([0.0015, 0.0050])\n",
      "new w tensor([[-0.3899,  0.8498,  0.6901],\n",
      "        [-0.2918,  0.7985,  0.9062]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4981, grad_fn=<DivBackward0>)\n",
      "i= 966\n",
      "w.grad= tensor([[ 0.1135,  0.1865, -0.4736],\n",
      "        [ 0.2433,  0.4004, -1.0166]])\n",
      "b.grad= tensor([0.0015, 0.0050])\n",
      "new w tensor([[-0.3899,  0.8498,  0.6902],\n",
      "        [-0.2918,  0.7985,  0.9063]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4981, grad_fn=<DivBackward0>)\n",
      "i= 967\n",
      "w.grad= tensor([[ 0.1131,  0.1859, -0.4713],\n",
      "        [ 0.2424,  0.3988, -1.0117]])\n",
      "b.grad= tensor([0.0015, 0.0050])\n",
      "new w tensor([[-0.3899,  0.8498,  0.6902],\n",
      "        [-0.2918,  0.7985,  0.9063]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4980, grad_fn=<DivBackward0>)\n",
      "i= 968\n",
      "w.grad= tensor([[ 0.1123,  0.1846, -0.4693],\n",
      "        [ 0.2413,  0.3970, -1.0070]])\n",
      "b.grad= tensor([0.0015, 0.0051])\n",
      "new w tensor([[-0.3899,  0.8498,  0.6902],\n",
      "        [-0.2919,  0.7984,  0.9064]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4979, grad_fn=<DivBackward0>)\n",
      "i= 969\n",
      "w.grad= tensor([[ 0.1120,  0.1841, -0.4670],\n",
      "        [ 0.2401,  0.3951, -1.0023]])\n",
      "b.grad= tensor([0.0015, 0.0051])\n",
      "new w tensor([[-0.3899,  0.8498,  0.6902],\n",
      "        [-0.2919,  0.7984,  0.9064]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4978, grad_fn=<DivBackward0>)\n",
      "i= 970\n",
      "w.grad= tensor([[ 0.1118,  0.1836, -0.4646],\n",
      "        [ 0.2391,  0.3935, -0.9976]])\n",
      "b.grad= tensor([0.0015, 0.0051])\n",
      "new w tensor([[-0.3899,  0.8498,  0.6903],\n",
      "        [-0.2919,  0.7984,  0.9065]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4978, grad_fn=<DivBackward0>)\n",
      "i= 971\n",
      "w.grad= tensor([[ 0.1107,  0.1822, -0.4628],\n",
      "        [ 0.2380,  0.3915, -0.9929]])\n",
      "b.grad= tensor([0.0015, 0.0051])\n",
      "new w tensor([[-0.3899,  0.8498,  0.6903],\n",
      "        [-0.2919,  0.7984,  0.9065]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4977, grad_fn=<DivBackward0>)\n",
      "i= 972\n",
      "w.grad= tensor([[ 0.1104,  0.1815, -0.4604],\n",
      "        [ 0.2375,  0.3904, -0.9879]])\n",
      "b.grad= tensor([0.0015, 0.0052])\n",
      "new w tensor([[-0.3899,  0.8498,  0.6903],\n",
      "        [-0.2919,  0.7984,  0.9066]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4976, grad_fn=<DivBackward0>)\n",
      "i= 973\n",
      "w.grad= tensor([[ 0.1099,  0.1807, -0.4584],\n",
      "        [ 0.2354,  0.3875, -0.9839]])\n",
      "b.grad= tensor([0.0015, 0.0052])\n",
      "new w tensor([[-0.3899,  0.8498,  0.6903],\n",
      "        [-0.2919,  0.7983,  0.9066]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4975, grad_fn=<DivBackward0>)\n",
      "i= 974\n",
      "w.grad= tensor([[ 0.1092,  0.1797, -0.4563],\n",
      "        [ 0.2346,  0.3859, -0.9792]])\n",
      "b.grad= tensor([0.0015, 0.0052])\n",
      "new w tensor([[-0.3899,  0.8497,  0.6904],\n",
      "        [-0.2919,  0.7983,  0.9067]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4975, grad_fn=<DivBackward0>)\n",
      "i= 975\n",
      "w.grad= tensor([[ 0.1088,  0.1790, -0.4541],\n",
      "        [ 0.2335,  0.3842, -0.9745]])\n",
      "b.grad= tensor([0.0016, 0.0052])\n",
      "new w tensor([[-0.3899,  0.8497,  0.6904],\n",
      "        [-0.2919,  0.7983,  0.9067]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4974, grad_fn=<DivBackward0>)\n",
      "i= 976\n",
      "w.grad= tensor([[ 0.1086,  0.1785, -0.4517],\n",
      "        [ 0.2323,  0.3822, -0.9701]])\n",
      "b.grad= tensor([0.0016, 0.0052])\n",
      "new w tensor([[-0.3899,  0.8497,  0.6904],\n",
      "        [-0.2920,  0.7983,  0.9068]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4973, grad_fn=<DivBackward0>)\n",
      "i= 977\n",
      "w.grad= tensor([[ 0.1075,  0.1770, -0.4500],\n",
      "        [ 0.2311,  0.3804, -0.9656]])\n",
      "b.grad= tensor([0.0016, 0.0053])\n",
      "new w tensor([[-0.3899,  0.8497,  0.6904],\n",
      "        [-0.2920,  0.7983,  0.9068]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4973, grad_fn=<DivBackward0>)\n",
      "i= 978\n",
      "w.grad= tensor([[ 0.1069,  0.1760, -0.4481],\n",
      "        [ 0.2303,  0.3790, -0.9609]])\n",
      "b.grad= tensor([0.0016, 0.0053])\n",
      "new w tensor([[-0.3899,  0.8497,  0.6904],\n",
      "        [-0.2920,  0.7982,  0.9069]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4972, grad_fn=<DivBackward0>)\n",
      "i= 979\n",
      "w.grad= tensor([[ 0.1067,  0.1755, -0.4457],\n",
      "        [ 0.2288,  0.3767, -0.9567]])\n",
      "b.grad= tensor([0.0016, 0.0053])\n",
      "new w tensor([[-0.3899,  0.8497,  0.6905],\n",
      "        [-0.2920,  0.7982,  0.9069]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4971, grad_fn=<DivBackward0>)\n",
      "i= 980\n",
      "w.grad= tensor([[ 0.1067,  0.1753, -0.4433],\n",
      "        [ 0.2281,  0.3753, -0.9520]])\n",
      "b.grad= tensor([0.0016, 0.0053])\n",
      "new w tensor([[-0.3899,  0.8497,  0.6905],\n",
      "        [-0.2920,  0.7982,  0.9070]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4970, grad_fn=<DivBackward0>)\n",
      "i= 981\n",
      "w.grad= tensor([[ 0.1054,  0.1735, -0.4418],\n",
      "        [ 0.2267,  0.3732, -0.9478]])\n",
      "b.grad= tensor([0.0016, 0.0054])\n",
      "new w tensor([[-0.3899,  0.8497,  0.6905],\n",
      "        [-0.2920,  0.7982,  0.9070]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0157], requires_grad=True)\n",
      "Loss= tensor(0.4970, grad_fn=<DivBackward0>)\n",
      "i= 982\n",
      "w.grad= tensor([[ 0.1055,  0.1733, -0.4394],\n",
      "        [ 0.2264,  0.3725, -0.9428]])\n",
      "b.grad= tensor([0.0016, 0.0054])\n",
      "new w tensor([[-0.3899,  0.8497,  0.6905],\n",
      "        [-0.2920,  0.7982,  0.9071]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4969, grad_fn=<DivBackward0>)\n",
      "i= 983\n",
      "w.grad= tensor([[ 0.1049,  0.1724, -0.4373],\n",
      "        [ 0.2246,  0.3697, -0.9390]])\n",
      "b.grad= tensor([0.0016, 0.0054])\n",
      "new w tensor([[-0.3899,  0.8497,  0.6906],\n",
      "        [-0.2920,  0.7982,  0.9071]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4968, grad_fn=<DivBackward0>)\n",
      "i= 984\n",
      "w.grad= tensor([[ 0.1044,  0.1716, -0.4353],\n",
      "        [ 0.2236,  0.3681, -0.9345]])\n",
      "b.grad= tensor([0.0016, 0.0054])\n",
      "new w tensor([[-0.3900,  0.8497,  0.6906],\n",
      "        [-0.2920,  0.7981,  0.9072]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4968, grad_fn=<DivBackward0>)\n",
      "i= 985\n",
      "w.grad= tensor([[ 0.1038,  0.1707, -0.4334],\n",
      "        [ 0.2227,  0.3664, -0.9302]])\n",
      "b.grad= tensor([0.0017, 0.0054])\n",
      "new w tensor([[-0.3900,  0.8497,  0.6906],\n",
      "        [-0.2921,  0.7981,  0.9072]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4967, grad_fn=<DivBackward0>)\n",
      "i= 986\n",
      "w.grad= tensor([[ 0.1037,  0.1703, -0.4310],\n",
      "        [ 0.2218,  0.3650, -0.9257]])\n",
      "b.grad= tensor([0.0017, 0.0055])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6906],\n",
      "        [-0.2921,  0.7981,  0.9073]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4967, grad_fn=<DivBackward0>)\n",
      "i= 987\n",
      "w.grad= tensor([[ 0.1028,  0.1691, -0.4293],\n",
      "        [ 0.2206,  0.3630, -0.9215]])\n",
      "b.grad= tensor([0.0017, 0.0055])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6906],\n",
      "        [-0.2921,  0.7981,  0.9073]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4966, grad_fn=<DivBackward0>)\n",
      "i= 988\n",
      "w.grad= tensor([[ 0.1023,  0.1682, -0.4274],\n",
      "        [ 0.2202,  0.3621, -0.9168]])\n",
      "b.grad= tensor([0.0017, 0.0055])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6907],\n",
      "        [-0.2921,  0.7981,  0.9073]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4965, grad_fn=<DivBackward0>)\n",
      "i= 989\n",
      "w.grad= tensor([[ 0.1021,  0.1678, -0.4252],\n",
      "        [ 0.2185,  0.3597, -0.9129]])\n",
      "b.grad= tensor([0.0017, 0.0055])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6907],\n",
      "        [-0.2921,  0.7980,  0.9074]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4965, grad_fn=<DivBackward0>)\n",
      "i= 990\n",
      "w.grad= tensor([[ 0.1015,  0.1668, -0.4233],\n",
      "        [ 0.2173,  0.3578, -0.9088]])\n",
      "b.grad= tensor([0.0017, 0.0055])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6907],\n",
      "        [-0.2921,  0.7980,  0.9074]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4964, grad_fn=<DivBackward0>)\n",
      "i= 991\n",
      "w.grad= tensor([[ 0.1010,  0.1661, -0.4213],\n",
      "        [ 0.2165,  0.3564, -0.9044]])\n",
      "b.grad= tensor([0.0017, 0.0056])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6907],\n",
      "        [-0.2921,  0.7980,  0.9075]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4963, grad_fn=<DivBackward0>)\n",
      "i= 992\n",
      "w.grad= tensor([[ 0.1004,  0.1651, -0.4195],\n",
      "        [ 0.2158,  0.3551, -0.9000]])\n",
      "b.grad= tensor([0.0017, 0.0056])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6908],\n",
      "        [-0.2921,  0.7980,  0.9075]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4963, grad_fn=<DivBackward0>)\n",
      "i= 993\n",
      "w.grad= tensor([[ 0.1000,  0.1645, -0.4174],\n",
      "        [ 0.2146,  0.3531, -0.8959]])\n",
      "b.grad= tensor([0.0017, 0.0056])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6908],\n",
      "        [-0.2921,  0.7980,  0.9076]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4962, grad_fn=<DivBackward0>)\n",
      "i= 994\n",
      "w.grad= tensor([[ 0.0993,  0.1634, -0.4157],\n",
      "        [ 0.2139,  0.3520, -0.8915]])\n",
      "b.grad= tensor([0.0017, 0.0056])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6908],\n",
      "        [-0.2922,  0.7980,  0.9076]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4962, grad_fn=<DivBackward0>)\n",
      "i= 995\n",
      "w.grad= tensor([[ 0.0994,  0.1633, -0.4133],\n",
      "        [ 0.2125,  0.3498, -0.8877]])\n",
      "b.grad= tensor([0.0017, 0.0056])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6908],\n",
      "        [-0.2922,  0.7979,  0.9077]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4961, grad_fn=<DivBackward0>)\n",
      "i= 996\n",
      "w.grad= tensor([[ 0.0985,  0.1620, -0.4116],\n",
      "        [ 0.2118,  0.3484, -0.8833]])\n",
      "b.grad= tensor([0.0018, 0.0057])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6908],\n",
      "        [-0.2922,  0.7979,  0.9077]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4961, grad_fn=<DivBackward0>)\n",
      "i= 997\n",
      "w.grad= tensor([[ 0.0981,  0.1613, -0.4097],\n",
      "        [ 0.2111,  0.3472, -0.8789]])\n",
      "b.grad= tensor([0.0018, 0.0057])\n",
      "new w tensor([[-0.3900,  0.8496,  0.6909],\n",
      "        [-0.2922,  0.7979,  0.9077]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4960, grad_fn=<DivBackward0>)\n",
      "i= 998\n",
      "w.grad= tensor([[ 0.0979,  0.1610, -0.4076],\n",
      "        [ 0.2096,  0.3451, -0.8752]])\n",
      "b.grad= tensor([0.0018, 0.0057])\n",
      "new w tensor([[-0.3900,  0.8495,  0.6909],\n",
      "        [-0.2922,  0.7979,  0.9078]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4959, grad_fn=<DivBackward0>)\n",
      "i= 999\n",
      "w.grad= tensor([[ 0.0972,  0.1599, -0.4059],\n",
      "        [ 0.2085,  0.3431, -0.8712]])\n",
      "b.grad= tensor([0.0018, 0.0057])\n",
      "new w tensor([[-0.3900,  0.8495,  0.6909],\n",
      "        [-0.2922,  0.7979,  0.9078]], requires_grad=True)\n",
      "new b tensor([-1.1849, -1.0158], requires_grad=True)\n",
      "Loss= tensor(0.4959, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = torch.tensor ([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]],)\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = torch.tensor ([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype=torch.float32)\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "print('inputs=', inputs)\n",
    "print('targets=', targets)\n",
    "\n",
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print('w=', w)\n",
    "print('b=', b)\n",
    "\n",
    "# We can define the model as follows:\n",
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "# MSE Cost function\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "#@ represents matrix multiplication in PyTorch, and the .t method returns the transpose of a tensor.\n",
    "# The matrix obtained by passing the input data into the model is a set of predictions for the target variables.\n",
    "\n",
    "for i in range(1000):\n",
    "    print('i=', i)\n",
    "\n",
    "    # Generate predictions\n",
    "    preds = model(inputs)\n",
    "    \n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = mse(preds, targets)\n",
    "    \n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    print('w.grad=', w.grad)\n",
    "    print('b.grad=', b.grad)\n",
    "\n",
    "    # Adjust weights & reset gradients\n",
    "    # We multiply the gradients with a very small number (10^-5 in this case) to ensure that we don't modify the weights by a very large amount. We want to take a small step in the downhill direction of the gradient, not a giant leap. This number is called the learning rate of the algorithm.\n",
    "    # We use torch.no_grad to indicate to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases.    \n",
    "    # Before we proceed, we reset the gradients to zero by invoking the .zero_() method. We need to do this because PyTorch accumulates gradients. Otherwise, the next time we invoke .backward on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results.   \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= 5e-5 * w.grad \n",
    "        b -= 5e-5 * b.grad  \n",
    "        w.grad.zero_()\n",
    "        w.grad.zero_()\n",
    "    \n",
    "    print('new w', w)\n",
    "    print('new b', b)\n",
    "\n",
    "    print('Loss=', loss)\n",
    "    w.requires_grad = True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415a3a7",
   "metadata": {},
   "source": [
    "* First we will create a more complex set of numbers. We can do this by using the **torch.randn()** function that will create 50 numbers that are randomly generated in the interval between 0 an 1. Then, we will stretch the values of x to the values in the interval between 0 to 10 by multiplying x with 10.\n",
    "* Now, we need to create the variable y. For example, we can say that one y is equal to x times 3 minus 4. We will also add some noise to the y. To do this we will use the **torch.randn()** function again, create 50 random numbers and add them to our equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35f6db0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.7964e+00,  5.2963e+00, -6.8928e+00, -8.4567e+00, -7.1102e+00,\n",
      "        -6.6222e+00,  1.6060e+01, -1.3752e+01,  5.0277e-01, -3.4966e+00,\n",
      "         1.2517e+00, -1.8718e-03,  7.3346e+00, -1.1177e+01, -1.0382e+01,\n",
      "        -7.6048e+00, -5.5428e+00, -7.8901e+00, -9.7484e+00, -6.9884e+00,\n",
      "        -1.6616e+01,  3.8944e+00,  1.4613e+01,  1.1088e+00, -4.2176e+00,\n",
      "        -2.3461e+01, -2.5312e+01,  9.0810e+00,  7.3149e+00, -1.6613e+01,\n",
      "        -2.0584e+00,  1.1362e+01, -6.4997e+00, -1.9655e+00, -5.0400e+00,\n",
      "        -1.0220e+00, -1.2070e+01,  1.0155e+01,  1.8335e+01,  3.9722e+00,\n",
      "        -8.7672e+00, -8.4887e+00, -2.5906e+01, -2.5739e+01, -1.0636e+01,\n",
      "        -5.3908e+00,  9.8170e+00,  6.2351e+00, -4.4915e+00,  1.3169e+01])\n",
      "tensor([ 10.7460,  10.9389, -25.6375, -29.2931, -25.4476, -24.7848,  44.6208,\n",
      "        -45.4501,  -2.2704, -15.1648,  -0.7518,  -2.6513,  18.9107, -37.5369,\n",
      "        -34.8107, -25.6331, -18.0966, -27.8332, -33.1873, -26.1507, -53.6277,\n",
      "          6.2770,  39.4459,  -2.7866, -15.9508, -75.8821, -78.7417,  24.9011,\n",
      "         16.8542, -54.6806, -10.5896,  29.1910, -22.1026, -11.1805, -18.2318,\n",
      "         -7.4932, -39.8000,  25.0180,  51.1663,   8.4568, -30.7240, -27.9435,\n",
      "        -81.0085, -80.7918, -35.1330, -19.5080,  24.7282,  16.8006, -18.3000,\n",
      "         33.7227])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "x = torch.randn(50)\n",
    "x = x * 10\n",
    "y = x * 3 - 4\n",
    "y += torch.randn(50)\n",
    "print (x)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad6107a",
   "metadata": {},
   "source": [
    "* Next step is to create a class called **LinearModel()**. To be able to use it as a PyTorch model, we will pass **torch. nn.Module** as a parameter. Now, we will define the **init function** or the constructor by passing parameter **self**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2650b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(LinearModel, self).__init__()\n",
    "    self.linear = torch.nn.Linear(1, 1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795eacc2",
   "metadata": {},
   "source": [
    "* Then we will call the **super()** function of the linear model. Note, that because this is a linear regression model, so it will have only **one layer**. That is why we will create a **self**. linear variable in which we‚Äôll call the **torch.nn.Linear()** function. This function takes two input parameters. The first one is the **size** of each input sample. This parameter will be equal to 1 because we have only numbers. The second parameter is the shape of the **output** which will also be equal to 1. \n",
    "* Now, we will create the **forward()** function which will take **self** and **x** as inputs. Finally, we will return **self. linear** and we‚Äôll pass in **x** as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "689af23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.7964e+00],\n",
      "        [ 5.2963e+00],\n",
      "        [-6.8928e+00],\n",
      "        [-8.4567e+00],\n",
      "        [-7.1102e+00],\n",
      "        [-6.6222e+00],\n",
      "        [ 1.6060e+01],\n",
      "        [-1.3752e+01],\n",
      "        [ 5.0277e-01],\n",
      "        [-3.4966e+00],\n",
      "        [ 1.2517e+00],\n",
      "        [-1.8718e-03],\n",
      "        [ 7.3346e+00],\n",
      "        [-1.1177e+01],\n",
      "        [-1.0382e+01],\n",
      "        [-7.6048e+00],\n",
      "        [-5.5428e+00],\n",
      "        [-7.8901e+00],\n",
      "        [-9.7484e+00],\n",
      "        [-6.9884e+00],\n",
      "        [-1.6616e+01],\n",
      "        [ 3.8944e+00],\n",
      "        [ 1.4613e+01],\n",
      "        [ 1.1088e+00],\n",
      "        [-4.2176e+00],\n",
      "        [-2.3461e+01],\n",
      "        [-2.5312e+01],\n",
      "        [ 9.0810e+00],\n",
      "        [ 7.3149e+00],\n",
      "        [-1.6613e+01],\n",
      "        [-2.0584e+00],\n",
      "        [ 1.1362e+01],\n",
      "        [-6.4997e+00],\n",
      "        [-1.9655e+00],\n",
      "        [-5.0400e+00],\n",
      "        [-1.0220e+00],\n",
      "        [-1.2070e+01],\n",
      "        [ 1.0155e+01],\n",
      "        [ 1.8335e+01],\n",
      "        [ 3.9722e+00],\n",
      "        [-8.7672e+00],\n",
      "        [-8.4887e+00],\n",
      "        [-2.5906e+01],\n",
      "        [-2.5739e+01],\n",
      "        [-1.0636e+01],\n",
      "        [-5.3908e+00],\n",
      "        [ 9.8170e+00],\n",
      "        [ 6.2351e+00],\n",
      "        [-4.4915e+00],\n",
      "        [ 1.3169e+01]])\n"
     ]
    }
   ],
   "source": [
    "x_torch = torch.FloatTensor(x).reshape(-1, 1)\n",
    "y_torch = torch.FloatTensor(y).reshape(-1, 1)\n",
    "\n",
    "print(x_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7671db9",
   "metadata": {},
   "source": [
    "* Now when we have defined our data, we can start coding the training part. We will start by creating a variable **model** which will be used for calling our **LinearModel class**. Then, we will calculate the loss by using the function **torch.nn.MSELoss()**. With this function we will calculate the mean squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8406a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005) # We will talk about this optimzer in next lexture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b69d5",
   "metadata": {},
   "source": [
    "* The next step is to train of our model. First we will create a for loop that will iterate in the range from 0 to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1383407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1315.2176513671875, 94.65514373779297, 24.212631225585938, 19.843212127685547, 19.275123596191406, 18.9311580657959, 18.60570526123047, 18.286863327026367, 17.973840713500977, 17.666501998901367, 17.364727020263672, 17.06842041015625, 16.77750015258789, 16.49184799194336, 16.211368560791016, 15.935981750488281, 15.66558837890625, 15.400091171264648, 15.13941764831543, 14.883461952209473, 14.63215446472168, 14.385400772094727, 14.143121719360352, 13.905233383178711, 13.671660423278809, 13.442320823669434, 13.217138290405273, 12.996037483215332, 12.778951644897461, 12.5657958984375, 12.3565092086792, 12.151015281677246, 11.949248313903809, 11.751140594482422, 11.556624412536621, 11.365630149841309, 11.178105354309082, 10.993982315063477, 10.813190460205078, 10.6356782913208, 10.461389541625977, 10.29025650024414, 10.122231483459473, 9.957247734069824, 9.795254707336426, 9.636202812194824, 9.480029106140137, 9.326693534851074, 9.176135063171387, 9.028308868408203, 8.883161544799805, 8.740642547607422, 8.600715637207031, 8.463319778442383, 8.328413009643555, 8.195954322814941, 8.065901756286621, 7.938205718994141, 7.8128228187561035, 7.689713001251221, 7.56883430480957, 7.450149059295654, 7.333616733551025, 7.219196796417236, 7.106850624084473, 6.996544361114502, 6.888236045837402, 6.781888484954834, 6.677473068237305, 6.574948787689209, 6.474286079406738, 6.375448226928711, 6.278402328491211, 6.183112621307373, 6.089553356170654, 5.997689247131348, 5.907492637634277, 5.8189287185668945, 5.731975078582764, 5.64659309387207, 5.562763214111328, 5.480451583862305, 5.399633884429932, 5.320275783538818, 5.242362022399902, 5.165858745574951, 5.090743541717529, 5.016990661621094, 4.944576740264893, 4.873471260070801, 4.803658962249756, 4.735113143920898, 4.6678080558776855, 4.601720809936523, 4.536835670471191, 4.473126411437988, 4.410569667816162, 4.349147796630859, 4.288839817047119, 4.229629039764404, 4.171488285064697, 4.114403247833252, 4.058352947235107, 4.003319263458252, 3.949281692504883, 3.8962249755859375, 3.844130516052246, 3.7929794788360596, 3.7427563667297363, 3.6934444904327393, 3.6450259685516357, 3.5974855422973633, 3.550809383392334, 3.504977226257324, 3.4599769115448, 3.4157910346984863, 3.3724071979522705, 3.329810857772827, 3.2879860401153564, 3.246919631958008, 3.2065975666046143, 3.1670074462890625, 3.1281347274780273, 3.089966058731079, 3.052490472793579, 3.015692710876465, 2.9795644283294678, 2.9440906047821045, 2.9092583656311035, 2.8750596046447754, 2.841479778289795, 2.80850887298584, 2.776137590408325, 2.744349956512451, 2.7131426334381104, 2.682497501373291, 2.6524109840393066, 2.6228692531585693, 2.5938611030578613, 2.565380334854126, 2.537416934967041, 2.509958267211914, 2.4830000400543213, 2.456529378890991, 2.4305379390716553, 2.4050185680389404, 2.379962682723999, 2.355360746383667, 2.3312039375305176, 2.307485342025757, 2.2841968536376953, 2.2613296508789062, 2.2388782501220703, 2.216834306716919, 2.195190191268921, 2.1739377975463867, 2.1530704498291016, 2.132582664489746, 2.1124656200408936, 2.0927135944366455, 2.0733182430267334, 2.054277181625366, 2.035579204559326, 2.0172219276428223, 1.9991958141326904, 1.9814982414245605, 1.9641193151474, 1.9470570087432861, 1.9303033351898193, 1.9138537645339966, 1.8977032899856567, 1.8818451166152954, 1.8662745952606201, 1.8509857654571533, 1.835973858833313, 1.82123601436615, 1.8067634105682373, 1.7925540208816528, 1.7786033153533936, 1.7649043798446655, 1.7514541149139404, 1.738247275352478, 1.7252793312072754, 1.7125482559204102, 1.7000468969345093, 1.687772512435913, 1.675721287727356, 1.663887619972229, 1.652268648147583, 1.6408610343933105, 1.6296584606170654, 1.6186609268188477, 1.6078622341156006, 1.5972588062286377, 1.5868486166000366, 1.5766267776489258, 1.56658935546875, 1.556735634803772, 1.547059178352356, 1.5375573635101318, 1.5282299518585205, 1.519071102142334, 1.5100780725479126, 1.5012484788894653, 1.492578387260437, 1.4840664863586426, 1.475706934928894, 1.4674999713897705, 1.4594416618347168, 1.451529860496521, 1.443761944770813, 1.4361339807510376, 1.4286446571350098, 1.4212923049926758, 1.4140704870224, 1.406982421875, 1.4000204801559448, 1.393186330795288, 1.3864758014678955, 1.3798866271972656, 1.3734169006347656, 1.3670653104782104, 1.3608283996582031, 1.3547048568725586, 1.3486921787261963, 1.3427879810333252, 1.3369919061660767, 1.3312994241714478, 1.3257113695144653, 1.320224404335022, 1.3148362636566162, 1.3095464706420898, 1.3043521642684937, 1.2992521524429321, 1.294244647026062, 1.2893275022506714, 1.2845008373260498, 1.279760718345642, 1.2751063108444214, 1.2705368995666504, 1.2660505771636963, 1.261644721031189, 1.2573193311691284, 1.253072738647461, 1.2489017248153687, 1.2448087930679321, 1.2407878637313843, 1.236839771270752, 1.2329645156860352, 1.2291591167449951, 1.2254230976104736, 1.221753716468811, 1.2181518077850342, 1.2146145105361938, 1.2111417055130005, 1.2077317237854004, 1.2043843269348145, 1.201096773147583, 1.1978693008422852, 1.1946994066238403, 1.1915878057479858, 1.1885333061218262, 1.185532569885254, 1.1825876235961914, 1.1796956062316895, 1.176855444908142, 1.1740679740905762, 1.1713305711746216, 1.1686424016952515, 1.166002631187439, 1.163411259651184, 1.1608667373657227, 1.1583689451217651, 1.1559157371520996, 1.1535073518753052, 1.1511424779891968, 1.1488211154937744, 1.146540880203247, 1.1443018913269043, 1.142104148864746, 1.1399465799331665, 1.1378278732299805, 1.1357464790344238, 1.1337045431137085, 1.131698489189148, 1.1297292709350586, 1.1277952194213867, 1.1258970499038696, 1.1240326166152954, 1.1222022771835327, 1.1204050779342651, 1.1186403036117554, 1.1169078350067139, 1.1152063608169556, 1.1135362386703491, 1.1118955612182617, 1.1102858781814575, 1.1087045669555664, 1.1071515083312988, 1.1056280136108398, 1.104130744934082, 1.1026614904403687, 1.1012182235717773, 1.0998021364212036, 1.0984104871749878, 1.0970444679260254, 1.095704197883606, 1.0943869352340698, 1.093094825744629, 1.0918246507644653, 1.090578317642212, 1.089354157447815, 1.0881528854370117, 1.086972713470459, 1.0858147144317627, 1.08467698097229, 1.083560585975647, 1.0824637413024902, 1.0813872814178467, 1.0803295373916626, 1.0792917013168335, 1.0782731771469116, 1.0772722959518433, 1.076289415359497, 1.0753240585327148, 1.0743769407272339, 1.073447585105896, 1.0725340843200684, 1.0716373920440674, 1.0707573890686035, 1.0698927640914917, 1.0690441131591797, 1.0682100057601929, 1.0673925876617432, 1.0665884017944336, 1.0657998323440552, 1.0650254487991333, 1.0642647743225098, 1.0635181665420532, 1.0627853870391846, 1.0620652437210083, 1.06135892868042, 1.0606638193130493, 1.059982419013977, 1.0593135356903076, 1.058656930923462, 1.0580116510391235, 1.0573782920837402, 1.056756615638733, 1.0561456680297852, 1.0555466413497925, 1.0549578666687012, 1.0543797016143799, 1.0538126230239868, 1.053255319595337, 1.0527082681655884, 1.052170991897583, 1.0516437292099, 1.0511260032653809, 1.0506170988082886, 1.050118088722229, 1.0496271848678589, 1.0491459369659424, 1.0486738681793213, 1.0482097864151, 1.0477540493011475, 1.047307014465332, 1.0468673706054688, 1.0464366674423218, 1.0460128784179688, 1.045596957206726, 1.0451887845993042, 1.0447876453399658, 1.0443947315216064, 1.0440083742141724, 1.0436283349990845, 1.0432559251785278, 1.0428900718688965, 1.0425310134887695, 1.042178750038147, 1.0418318510055542, 1.0414929389953613, 1.0411584377288818, 1.0408308506011963, 1.0405093431472778, 1.040192723274231, 1.039883017539978, 1.0395780801773071, 1.0392789840698242, 1.038985252380371, 1.038697361946106, 1.0384141206741333, 1.0381355285644531, 1.03786301612854, 1.0375947952270508, 1.0373313426971436, 1.0370732545852661, 1.0368198156356812, 1.03657066822052, 1.0363259315490723, 1.0360862016677856, 1.035850167274475, 1.0356181859970093, 1.0353913307189941, 1.0351678133010864, 1.0349493026733398, 1.034733533859253, 1.0345220565795898, 1.0343152284622192, 1.034111499786377, 1.0339115858078003, 1.0337153673171997, 1.0335224866867065, 1.0333329439163208, 1.0331473350524902, 1.0329643487930298, 1.0327855348587036, 1.0326093435287476, 1.0324368476867676, 1.0322669744491577, 1.032100796699524, 1.0319370031356812, 1.0317765474319458, 1.031618595123291, 1.0314640998840332, 1.031312108039856, 1.0311627388000488, 1.0310161113739014, 1.0308723449707031, 1.0307306051254272, 1.0305923223495483, 1.0304564237594604, 1.0303226709365845, 1.0301915407180786, 1.0300629138946533, 1.0299361944198608, 1.0298116207122803, 1.029689073562622, 1.0295699834823608, 1.0294523239135742, 1.0293364524841309, 1.029222846031189, 1.0291117429733276, 1.0290024280548096, 1.028895378112793, 1.0287892818450928, 1.0286861658096313, 1.0285849571228027, 1.0284843444824219, 1.0283865928649902, 1.0282903909683228, 1.028196096420288, 1.0281035900115967, 1.0280125141143799, 1.0279228687286377, 1.0278346538543701, 1.0277491807937622, 1.0276646614074707, 1.0275810956954956, 1.0274994373321533, 1.0274195671081543, 1.0273410081863403, 1.0272629261016846, 1.0271886587142944, 1.0271135568618774, 1.0270403623580933, 1.0269685983657837, 1.0268981456756592, 1.026828646659851, 1.0267610549926758, 1.0266941785812378, 1.026628851890564, 1.026564598083496, 1.0265015363693237, 1.0264389514923096, 1.0263785123825073, 1.0263187885284424, 1.0262600183486938, 1.02620267868042, 1.026145339012146, 1.026090145111084, 1.026035189628601, 1.0259824991226196, 1.025929570198059, 1.0258779525756836, 1.025827169418335, 1.0257774591445923, 1.0257288217544556, 1.0256808996200562, 1.0256333351135254, 1.0255869626998901, 1.0255420207977295, 1.025497317314148, 1.0254532098770142, 1.0254102945327759, 1.0253677368164062, 1.0253268480300903, 1.0252859592437744, 1.025246024131775, 1.0252071619033813, 1.0251684188842773, 1.0251303911209106, 1.0250933170318604, 1.025057077407837, 1.0250204801559448, 1.0249857902526855, 1.0249508619308472, 1.024917483329773, 1.0248852968215942, 1.0248512029647827, 1.0248196125030518, 1.0247879028320312, 1.024756908416748, 1.0247269868850708, 1.0246968269348145, 1.0246672630310059, 1.0246387720108032, 1.024611234664917, 1.0245823860168457, 1.0245559215545654, 1.0245287418365479, 1.0245022773742676, 1.0244768857955933, 1.024451732635498, 1.0244274139404297, 1.0244020223617554, 1.024378776550293, 1.0243550539016724, 1.0243316888809204, 1.0243091583251953, 1.0242869853973389, 1.0242654085159302, 1.0242427587509155, 1.0242226123809814, 1.0242018699645996, 1.0241820812225342, 1.0241619348526, 1.0241420269012451, 1.024122953414917, 1.0241039991378784, 1.0240849256515503, 1.0240672826766968, 1.0240494012832642, 1.024031400680542, 1.0240141153335571, 1.0239977836608887, 1.0239812135696411, 1.0239644050598145, 1.0239485502243042, 1.0239330530166626, 1.0239176750183105, 1.0239028930664062, 1.0238873958587646, 1.0238730907440186, 1.0238585472106934, 1.0238442420959473, 1.0238313674926758, 1.0238169431686401, 1.0238040685653687, 1.0237905979156494, 1.0237776041030884, 1.0237648487091064, 1.0237526893615723, 1.0237404108047485, 1.0237287282943726, 1.0237171649932861, 1.0237056016921997, 1.0236939191818237, 1.0236824750900269, 1.0236725807189941, 1.0236610174179077, 1.0236507654190063, 1.0236403942108154, 1.0236304998397827, 1.0236210823059082, 1.0236107110977173, 1.0236009359359741, 1.0235918760299683, 1.023582100868225, 1.023573398590088, 1.0235645771026611, 1.0235556364059448, 1.0235470533370972, 1.023538589477539, 1.0235302448272705, 1.0235226154327393, 1.0235141515731812, 1.0235066413879395, 1.0234988927841187, 1.0234915018081665, 1.0234841108322144, 1.023476481437683, 1.0234689712524414, 1.0234624147415161, 1.0234556198120117, 1.023447871208191, 1.0234419107437134, 1.023435354232788, 1.0234289169311523, 1.0234224796295166, 1.0234160423278809, 1.023410439491272, 1.0234044790267944, 1.023398756980896, 1.0233927965164185, 1.0233880281448364, 1.0233814716339111, 1.0233765840530396, 1.0233708620071411, 1.0233657360076904, 1.023360252380371, 1.0233553647994995, 1.0233498811721802, 1.0233460664749146, 1.0233410596847534, 1.02333664894104, 1.0233315229415894, 1.0233272314071655, 1.0233229398727417, 1.0233181715011597, 1.0233139991760254, 1.0233098268508911, 1.0233054161071777, 1.023301362991333, 1.0232971906661987, 1.0232932567596436, 1.0232899188995361, 1.0232856273651123, 1.0232826471328735, 1.023279070854187, 1.0232752561569214, 1.0232712030410767, 1.0232681035995483, 1.0232648849487305, 1.023261308670044, 1.0232582092285156, 1.0232548713684082, 1.0232517719268799, 1.0232490301132202, 1.0232460498809814, 1.0232434272766113, 1.0232398509979248, 1.023236870765686, 1.0232346057891846, 1.023230791091919, 1.023228645324707, 1.0232259035110474, 1.0232232809066772, 1.0232211351394653, 1.0232182741165161, 1.0232157707214355, 1.0232136249542236, 1.0232114791870117, 1.0232082605361938, 1.023206353187561, 1.0232040882110596, 1.0232020616531372, 1.0231999158859253, 1.023197889328003, 1.0231953859329224, 1.0231934785842896, 1.0231915712356567, 1.0231894254684448, 1.0231879949569702, 1.023185133934021, 1.0231834650039673, 1.0231819152832031, 1.023180365562439, 1.023177981376648, 1.0231765508651733, 1.0231744050979614, 1.0231733322143555, 1.0231716632843018, 1.023169755935669, 1.0231678485870361, 1.0231670141220093, 1.0231648683547974, 1.0231635570526123, 1.0231623649597168, 1.0231602191925049, 1.0231595039367676, 1.0231574773788452, 1.0231562852859497, 1.0231550931930542, 1.0231542587280273, 1.0231528282165527, 1.0231517553329468, 1.023149847984314, 1.0231491327285767, 1.0231469869613647, 1.023146152496338, 1.0231451988220215, 1.0231443643569946, 1.0231431722640991, 1.0231419801712036, 1.0231406688690186, 1.0231398344039917, 1.023138403892517, 1.0231372117996216, 1.0231361389160156, 1.0231356620788574, 1.023134708404541, 1.0231335163116455, 1.0231326818466187, 1.0231318473815918, 1.0231307744979858, 1.0231294631958008, 1.0231295824050903, 1.0231282711029053, 1.0231273174285889, 1.0231268405914307, 1.023126244544983, 1.0231249332427979, 1.0231237411499023, 1.0231231451034546, 1.0231224298477173, 1.0231220722198486, 1.0231209993362427, 1.0231205224990845, 1.0231198072433472, 1.0231190919876099, 1.023118495941162, 1.023118495941162, 1.0231168270111084, 1.0231163501739502, 1.023115634918213, 1.0231152772903442, 1.0231143236160278, 1.0231139659881592, 1.0231133699417114, 1.0231119394302368, 1.0231120586395264, 1.0231117010116577, 1.0231112241744995, 1.0231101512908936, 1.023110270500183, 1.0231095552444458, 1.0231095552444458, 1.0231077671051025, 1.0231082439422607, 1.0231075286865234, 1.0231068134307861, 1.023106336593628, 1.023106575012207, 1.023105502128601, 1.0231057405471802, 1.0231049060821533, 1.0231044292449951, 1.0231044292449951, 1.0231035947799683, 1.023102879524231, 1.0231026411056519, 1.0231027603149414, 1.0231016874313354, 1.0231016874313354, 1.023101568222046, 1.0231008529663086, 1.0231006145477295, 1.02310049533844, 1.0230997800827026, 1.0230997800827026, 1.023099422454834, 1.023099422454834, 1.0230990648269653, 1.0230987071990967, 1.023098349571228, 1.0230979919433594, 1.023098111152649, 1.0230976343154907, 1.0230975151062012, 1.0230969190597534, 1.0230967998504639, 1.023095965385437, 1.0230953693389893, 1.023095726966858, 1.0230953693389893, 1.0230953693389893, 1.0230947732925415, 1.0230947732925415, 1.0230944156646729, 1.0230940580368042, 1.0230941772460938, 1.023093819618225, 1.023093581199646, 1.023093581199646, 1.0230934619903564, 1.0230923891067505, 1.0230923891067505, 1.0230928659439087, 1.02309250831604, 1.0230916738510132, 1.0230915546417236, 1.0230920314788818, 1.023091197013855, 1.0230910778045654, 1.0230910778045654, 1.0230917930603027, 1.0230910778045654, 1.0230906009674072, 1.0230902433395386, 1.0230904817581177, 1.0230904817581177, 1.0230902433395386, 1.023090124130249, 1.0230896472930908, 1.0230896472930908, 1.0230894088745117, 1.0230900049209595, 1.0230894088745117, 1.023089051246643, 1.0230889320373535, 1.0230894088745117, 1.0230891704559326, 1.0230886936187744, 1.0230886936187744, 1.0230880975723267, 1.0230884552001953, 1.023087978363037, 1.0230876207351685, 1.0230883359909058, 1.023087978363037, 1.023087501525879, 1.0230878591537476, 1.023087739944458, 1.023087501525879, 1.0230871438980103, 1.0230872631072998, 1.023086428642273, 1.0230865478515625, 1.023087501525879, 1.0230869054794312, 1.0230861902236938, 1.023086667060852, 1.023086428642273, 1.0230865478515625, 1.0230863094329834, 1.0230861902236938, 1.0230861902236938, 1.023085594177246, 1.0230861902236938, 1.0230860710144043, 1.023085594177246, 1.0230859518051147, 1.023085594177246, 1.0230854749679565, 1.0230858325958252, 1.0230858325958252, 1.0230857133865356, 1.0230848789215088, 1.0230858325958252, 1.0230854749679565, 1.0230849981307983, 1.0230849981307983, 1.0230849981307983, 1.023085117340088, 1.023085117340088, 1.0230845212936401, 1.0230848789215088, 1.0230849981307983, 1.0230847597122192, 1.0230848789215088, 1.0230849981307983, 1.0230848789215088, 1.0230841636657715, 1.0230845212936401, 1.0230846405029297, 1.0230841636657715, 1.0230847597122192, 1.0230841636657715, 1.023084282875061, 1.0230841636657715, 1.0230841636657715, 1.0230836868286133, 1.0230841636657715, 1.023084044456482, 1.0230838060379028, 1.0230839252471924, 1.0230834484100342, 1.0230836868286133, 1.0230841636657715, 1.0230834484100342, 1.0230841636657715, 1.0230834484100342, 1.023084282875061, 1.0230839252471924, 1.0230839252471924, 1.0230836868286133, 1.023083209991455, 1.0230833292007446, 1.0230833292007446, 1.0230834484100342, 1.0230834484100342, 1.0230830907821655, 1.0230828523635864, 1.0230834484100342, 1.0230830907821655, 1.0230830907821655, 1.0230830907821655, 1.0230828523635864, 1.0230833292007446, 1.0230834484100342, 1.0230828523635864, 1.0230833292007446, 1.0230834484100342, 1.0230824947357178, 1.0230830907821655, 1.0230830907821655, 1.0230830907821655, 1.023083209991455, 1.023083209991455, 1.0230827331542969, 1.023083209991455, 1.0230833292007446, 1.023083209991455, 1.0230823755264282, 1.0230830907821655, 1.0230828523635864, 1.0230827331542969, 1.0230822563171387, 1.0230822563171387, 1.0230828523635864, 1.0230828523635864, 1.0230827331542969, 1.0230826139450073, 1.0230827331542969, 1.0230826139450073, 1.0230826139450073, 1.0230824947357178, 1.0230826139450073, 1.0230826139450073, 1.0230827331542969, 1.0230820178985596, 1.0230820178985596, 1.0230820178985596, 1.0230827331542969, 1.0230827331542969, 1.0230827331542969, 1.0230823755264282, 1.0230821371078491, 1.023082971572876, 1.0230817794799805, 1.0230826139450073, 1.0230827331542969, 1.0230827331542969, 1.0230823755264282, 1.0230823755264282, 1.0230820178985596, 1.0230823755264282, 1.0230817794799805, 1.0230820178985596, 1.0230820178985596, 1.0230824947357178, 1.0230823755264282, 1.0230823755264282, 1.0230823755264282, 1.0230826139450073, 1.0230823755264282, 1.0230824947357178, 1.0230822563171387, 1.0230821371078491, 1.0230821371078491, 1.0230823755264282, 1.0230823755264282, 1.0230824947357178, 1.0230823755264282, 1.0230824947357178, 1.0230821371078491, 1.0230817794799805, 1.0230820178985596, 1.0230821371078491, 1.0230822563171387, 1.0230821371078491, 1.0230822563171387, 1.0230823755264282, 1.0230826139450073, 1.023081660270691, 1.023081660270691, 1.0230820178985596, 1.0230822563171387, 1.0230822563171387, 1.0230820178985596, 1.02308189868927, 1.023081660270691, 1.023081660270691, 1.0230823755264282, 1.0230822563171387, 1.0230823755264282, 1.0230823755264282, 1.0230826139450073, 1.0230820178985596, 1.02308189868927, 1.0230820178985596, 1.0230824947357178, 1.023081660270691, 1.02308189868927, 1.0230823755264282, 1.0230823755264282, 1.0230822563171387, 1.0230827331542969, 1.0230822563171387]\n"
     ]
    }
   ],
   "source": [
    "all_loss = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "  y_hat = model(x_torch)\n",
    "\n",
    "  loss = criterion(y_hat, y_torch)\n",
    "  loss.backward()\n",
    "  all_loss.append(loss.item())\n",
    "\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "    \n",
    "print (all_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c09565",
   "metadata": {},
   "source": [
    "* Now, let‚Äôs make some predictions with the model.forward() function and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "552eb94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10.2612],\n",
      "        [ 11.7534],\n",
      "        [-24.6331],\n",
      "        [-29.3015],\n",
      "        [-25.2820],\n",
      "        [-23.8252],\n",
      "        [ 43.8842],\n",
      "        [-45.1089],\n",
      "        [ -2.5561],\n",
      "        [-14.4948],\n",
      "        [ -0.3203],\n",
      "        [ -4.0625],\n",
      "        [ 17.8380],\n",
      "        [-37.4216],\n",
      "        [-35.0492],\n",
      "        [-26.7584],\n",
      "        [-20.6029],\n",
      "        [-27.6101],\n",
      "        [-33.1574],\n",
      "        [-24.9184],\n",
      "        [-53.6597],\n",
      "        [  7.5684],\n",
      "        [ 39.5659],\n",
      "        [ -0.7469],\n",
      "        [-16.6470],\n",
      "        [-74.0922],\n",
      "        [-79.6171],\n",
      "        [ 23.0512],\n",
      "        [ 17.7792],\n",
      "        [-53.6481],\n",
      "        [-10.2015],\n",
      "        [ 29.8591],\n",
      "        [-23.4595],\n",
      "        [ -9.9243],\n",
      "        [-19.1022],\n",
      "        [ -7.1077],\n",
      "        [-40.0873],\n",
      "        [ 26.2582],\n",
      "        [ 50.6745],\n",
      "        [  7.8008],\n",
      "        [-30.2284],\n",
      "        [-29.3970],\n",
      "        [-81.3910],\n",
      "        [-80.8933],\n",
      "        [-35.8081],\n",
      "        [-20.1494],\n",
      "        [ 25.2484],\n",
      "        [ 14.5559],\n",
      "        [-17.4648],\n",
      "        [ 35.2545]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.forward(x_torch)\n",
    "print (y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d9446",
   "metadata": {},
   "source": [
    "## Multivariable linear regression\n",
    "* Given a dataset ${\\cal D} = \\{(x^{(i)},y^{(i)})\\}_{i=1}^N$, Each observation $i$ includes a scalar response $y_{i}$ and a column vector ${x}_{i}$ of $p$ parameters (regressors), $x^{(i)}=\\left\\{ x_{i1},x_{i2},\\cdots,x_{im}\\right\\} ^{T}$\n",
    "* Data $x^{(i)} \\in \\mathbb{R}^d$\n",
    "\n",
    "* Want to predict a scalar $\\hat y$ as a function of a scalar $x$<br>\n",
    "* Model: $\\hat y$ is a linear function of $x$:<br>\n",
    "> $\\it\\hat y = wx + b$<br>\n",
    "> $\\hat y$ is the prediction<br>\n",
    "> $w$ is the weight<br>\n",
    "> $b$ is the bias<br>\n",
    "* $w$ and $b$ together are the parameters<br>\n",
    "* In linear regression square loss is the most common loss function:\n",
    ">$L_s(y,\\hat y)=(y^{(i)}-\\hat y^{(i)})^{2}$\n",
    "* In a linear regression model, the response variable, $y_{i}$, is a linear function of the regressors and can be presented in the form of a general equation as follows::\n",
    "$y^{(i)}=w_{1}x_{i1}+w_{2}x_{i2}+\\cdots+w_{m}x_{im}+e_{i}$ \n",
    "* The general equation can be written in vector form  \n",
    "$\\left[\\begin{array}{c}\n",
    "y_{1}\\\\\n",
    "y_{2}\\\\\n",
    "\\vdots\\\\\n",
    "y_{n}\n",
    "\\end{array}\\right]$=$\\left[\\begin{array}{ccccc}\n",
    "x_{11} & x_{12} & x_{13} & \\cdots & x_{1m}\\\\\n",
    "x_{21} & x_{22} & x_{23} & \\cdots & x_{2m}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{n2} & x_{n3} & \\cdots & x_{nm}\n",
    "\\end{array}\\right]$$\\left[\\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "\\vdots\\\\\n",
    "w_{n}\n",
    "\\end{array}\\right]+\\left[\\begin{array}{c}\n",
    "e_{1}\\\\\n",
    "e_{2}\\\\\n",
    "\\vdots\\\\\n",
    "e_{n}\n",
    "\\end{array}\\right]$\n",
    "* It is equal to $\\boldsymbol{Y}=\\boldsymbol{\\boldsymbol{X}W}+\\boldsymbol{E}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9b252",
   "metadata": {},
   "source": [
    "* regression coefficients are obtained by minimizing the residual squares, so that the normal equation is obtained as follows:<br>\n",
    "$nw_{0}+w_{1}\\sum x_{1j}+w_{2}\\sum x_{2j}+\\cdots+w_{m}\\sum x_{m}=\\sum y_{j}$ <br>\n",
    "$w_{0}\\sum x_{1j}+w_{1}\\sum x_{2j}^{2}+w_{2}\\sum x_{1j}x_{2j}+\\cdots+w_{m}\\sum x_{1j}x_{mj}=\\sum x_{1j}y_{j}$<br>\n",
    "$w_{0}\\sum x_{2j}+w_{1}\\sum x_{1j}x_{2j}+w_{2}\\sum x_{2j}^{2}+\\cdots+w_{m}\\sum x_{2j}x_{mj}=\\sum x_{2j}y_{j}$<br>\n",
    "$\\vdots$<br>\n",
    "$w_{0}\\sum x_{mj}+w_{1}\\sum x_{1j}x_{mj}+w_{2}\\sum x_{2j}x_{mj}+\\cdots+w_{m}\\sum x_{mj}^{2}=\\sum x_{mj}y_{j}$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f560241",
   "metadata": {},
   "source": [
    "* Equations can be written in matrix notation as follows:  \n",
    "$\\left[\\begin{array}{ccccc}\n",
    "n & \\sum x_{1j} & \\sum x_{2j} & \\cdots & \\sum x_{mj}\\\\\n",
    "\\sum x_{1j} & \\sum x_{1j}^{2} & \\sum x_{1j}x_{2j} & \\cdots & \\sum x_{1j}x_{mj}\\\\\n",
    "\\sum x_{2j} & \\sum x_{1j}x_{2j} & \\sum x_{2j}^{2} & \\cdots & \\sum x_{2j}x_{mj}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\sum x_{mj} & \\sum x_{1j}x_{mj} & \\sum x_{2j}x_{mj} & \\cdots & \\sum x_{mj}^{2}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "w_{1}\\\\\n",
    "w_{2}\\\\\n",
    "\\vdots\\\\\n",
    "w_{m}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\sum y_{j}\\\\\n",
    "\\sum x_{1j}y_{j}\\\\\n",
    "\\sum x_{2j}y_{j}\\\\\n",
    "\\vdots\\\\\n",
    "\\sum x_{mj}y_{j}\n",
    "\\end{array}\\right]$\n",
    "* It is equal to  $\\boldsymbol{XW}=\\boldsymbol{Y}$  \n",
    "* where $\\boldsymbol{X}$ matrix is called the covariance matrix."
   ]
  },
  {
   "attachments": {
    "Cholesky.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a3f1edd2",
   "metadata": {},
   "source": [
    "* Cholesky Decomposition:\n",
    "$\\boldsymbol{X}$ is a definite symmetric and positive\n",
    "matrix, $X_{ij}=X_{ji}$, then $\\boldsymbol{X}$ can be written  \n",
    "$\\boldsymbol{X}=\\boldsymbol{L}\\boldsymbol{L}^{T}$  \n",
    "Where $\\boldsymbol{L}$ is the bottom triangle matrix which is defined as follows:  \n",
    "$\\boldsymbol{L}=\\left[\\begin{array}{cccc}\n",
    "l_{11} & 0 & \\cdots & 0\\\\\n",
    "l_{12} & l_{22} & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "l_{n1} & l_{n2} & \\cdots & l_{nn}\n",
    "\\end{array}\\right]$ <br>\n",
    "Where  \n",
    "<img src=\"attachment:Cholesky.png\" width= \"300\"/> </div> <br>\n",
    "\n",
    "* To solve $\\boldsymbol{XW}=\\boldsymbol{Y}$ then solution $\\boldsymbol{W}$ is\n",
    "obtained by: (a) forward substitution: solution $d$ uses $\\boldsymbol{Ld}=\\boldsymbol{W}$, then (b) back substitution:\n",
    "solution $\\boldsymbol{W}$ uses $\\boldsymbol{L^{T}W}=\\boldsymbol{d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76646b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
