{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77132194-4974-4b21-9883-2cd7095439cc",
   "metadata": {},
   "source": [
    "# ECE 364 Lecture 27\n",
    "## Midterm 2 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57adf1-c140-4f72-8023-e5f11a2c1462",
   "metadata": {},
   "source": [
    "## Lectures 13 and 16: Logistic Regression\n",
    "\n",
    "We introduced logistic regression as our first classification model that is trainable by backpropagation. For an input vector $x\\in\\mathbb{R}^N$, logistic fits a parameter vector $w\\in\\mathbb{R}^N$ and a bias term $b\\in\\mathbb{R}$ to perform the binary classification problem where labels $y\\in\\{-1, +1\\}$. The logistic regression function $f_\\theta(x)$ can be seen as fitting a sigmoid function around a linear regression model:\n",
    "\n",
    "$$\n",
    "f_\\theta(x) = \\mathbf{Pr}\\{Y=1|x\\}=\\frac{1}{1+e^{-(w^\\top x + b)}}.\n",
    "$$\n",
    "\n",
    "Above, if the regression score $z= w\\top x+b$ is a larger positive number, the probability of the positive class, i.e. label $+1$, approaches 1. Conversely, if this regression score is small, i.e. very negative, the probability of the positive class will approach 0 as the probability of the negative class, i.e. label $-1$, approaches 1. To train a logistic regression model, we are minimizing the negative log likelihood (similarly, maximizing the log likelihood) of the available labels for the given data in order to learn the parameters $\\theta=\\{w, b\\}\\in\\mathbb{R}^{N+1}$. With respect to a PyTorch implementation, this means we will use binary cross-entropy loss function.\n",
    "\n",
    "Recall that we can also define a multi-class logistic regression model as performing a matrix-vector multiplication with an input vector $x\\in\\mathbb{R}^N$ to produce class scores $z\\in\\mathbb{R}^M$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z &= Wx+b\\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\rule[.6ex]{4ex}{0.75pt} & w_1^\\top & \\rule[.6ex]{4ex}{0.75pt}\\\\\n",
    "\\rule[.6ex]{4ex}{0.75pt} & w_2^\\top & \\rule[.6ex]{4ex}{0.75pt}\\\\\n",
    "& \\vdots & \\\\\n",
    "\\rule[.6ex]{4ex}{0.75pt} & w_M^\\top & \\rule[.6ex]{4ex}{0.75pt}\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\rule[-1ex]{0.5pt}{4ex}\\\\\n",
    "x\\\\\n",
    "\\rule[1ex]{0.5pt}{4ex}\\\\\n",
    "\\end{bmatrix}\n",
    "+\\begin{bmatrix}\n",
    "b_1\\\\\n",
    "b_2\\\\\n",
    "\\vdots\\\\\n",
    "b_M\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "z_1\\\\\n",
    "z_2\\\\\n",
    "\\vdots\\\\\n",
    "z_M\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The resulting class scores are then converted to class probabilities using the softmax function.\n",
    "\n",
    "$$\n",
    "\\textrm{softmax}(z)_k=\\mathbf{Pr}\\{\\textrm{Class }y=k|x\\} = \\frac{e^{z_k}}{\\sum_{j=1}^{M}e^{z_j}}.\n",
    "$$\n",
    "\n",
    "Instead of binary cross-entropy, we will instead now use cross-entropy loss. For label $y_i\\in\\{1, \\ldots, M\\}$, we compute cross-entropy loss across dataset $\\mathcal{D}=\\{(x_i, y_i)\\}_{i=1}^{N}:\n",
    "\n",
    "$$\n",
    "\\ell_{\\textrm{ce}} = -\\sum_{i=1}^{N}\\log\\left(f_{y_i}(x_i; \\theta)\\right),\n",
    "$$\n",
    "\n",
    "where $f_{y_i}(x_i; \\theta)$ computes the probability of class $y_i$ given input $x_i$ according to the softmax outputs of the multi-class logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f89ee-179c-43bc-b8b7-3cda24456951",
   "metadata": {},
   "source": [
    "## Lecture 17 and 18: PyTorch ``nn.Modules``, Optimizers, Datasets, Dataloaders\n",
    "### Learning objectives\n",
    "* Identify the necessary methods and attributes for a PyTorch ``nn.Module`` object.\n",
    "* Implement a PyTorch ``nn.Module`` object for a given parametric model.\n",
    "* Initialize a PyTorch optimizer for training a model described by a ``nn.Module`` object.\n",
    "* State the key steps for using a PyTorch optimizer in a basic machine learning training loop via backpropagation.\n",
    "* Describe the differences between the training set, validation set, and testing set for a machine learning problem.\n",
    "* Create a PyTorch ``Dataset`` class from available data, e.g. from a toy dataset. Specifically, be able to state the necessary methods and attributes for a PyTorch ``Dataset`` class.\n",
    "* Apply the PyTorch ``DataLoader`` class to improve the flexibility and readability of a standard PyTorch training loop. Specifically, understand how ``Dataset`` and ``DataLoader`` objects are related\n",
    "  \n",
    "### ``nn.Modules``\n",
    "\n",
    "The [``nn.Module`` class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is the universal **base class** for neural networks, and more broadly trainable models, in PyTorch found within the ``torch.nn`` package, i.e. ``torch.nn.Module``. We create our own models by **inheriting** this base class and implementing the necessary methods for the class. There are two methods which must be implemented: ``__init__`` and ``forward``.\n",
    "\n",
    "The ``__init__`` method specifies the constructor and thus how every instance of this module must be initialized the ``__init__`` method must first call ``super().__init__()`` to call the constructor of the base ``nn.Module`` class (and thus access all the helpful attributes and methods within). The ``forward`` method implements how the model processes input data for its **forward pass**.\n",
    "\n",
    "As an example ``nn.Module`` class. Consider a class for implementing third-order polynomial regression. In this case, we have\n",
    "$$\n",
    "f_\\theta(x) = ax^3+bx^2+cx+d\n",
    "$$\n",
    "and $\\theta=\\{a, b, c, d\\}$. The corresponding ``nn.Module`` class may be written as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3ffbbf-2396-4f00-b3df-120c50f2d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.4298], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9944], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4005], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2043], requires_grad=True)\n",
      "tensor([2.0291], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ThirdOrderPolynomial(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Specify the learnable parameters: a, b, c, d\n",
    "        '''\n",
    "        super().__init__() # call nn.Module constructor first\n",
    "        self.a = nn.Parameter(torch.rand(1))\n",
    "        self.b = nn.Parameter(torch.rand(1))\n",
    "        self.c = nn.Parameter(torch.rand(1))\n",
    "        self.d = nn.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Implement f(x).\n",
    "        '''\n",
    "        f_x = self.a*x**3 + self.b*x**2 + self.c*x + self.d\n",
    "        return f_x\n",
    "\n",
    "# create ThirdOrderPolynomial object\n",
    "my_model = ThirdOrderPolynomial()\n",
    "# print model parameters\n",
    "print(my_model.a)\n",
    "print(my_model.b)\n",
    "print(my_model.c)\n",
    "print(my_model.d)\n",
    "x_input = torch.tensor([1])\n",
    "prediction = my_model(x_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7094dc-a2bb-4afb-8b2e-d2ca7ba347e9",
   "metadata": {},
   "source": [
    "Note how each method must include the ``self`` parameter to make the class attributes and methods accessible. We also see how the forward function is called my using ``object_instance(input)``. In other words, the ``forward`` method overrides calling the object instance for an intuitive interface of the function/model's forward pass. In addition, model parameters may be specified by wrapping a tensor in the ``nn.Parameter`` class.\n",
    "\n",
    "The ``nn.Module`` base class also implements a highly useful functionality. Recall from our backpropagation lectures how we had to manually collect all of the trainable parameters to perform gradient descent, i.e. specify the update and clearing the gradients for each one. The ``nn.Module`` class automatically collects all learnable paramters in the ``.parameters()`` attribute. This makes scaling our models and learning algorithms dramatically easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c74c7-ff94-4984-84af-0152c8c64af7",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "The [``torch.optim`` package](https://pytorch.org/docs/stable/optim.html) contains many helpful optimizers, i.e. learning algorithms, and other useful interfaces to simplify the parameter updating process. The simplest optimizer in the ``torch.optim`` package is the ``optim.SGD``, which implements **stochastic gradient descent** (SGD). Also known as mini-batch gradient descent, SGD implements gradient descent except only computes the gradient over a random subset, or mini-batch, of the data. Thus, the computed gradient depends on a stochastic sample of the dataset. If the **batch size** for SGD is the entire dataset, then SGD simply becomes ordinary gradient descent.\n",
    "\n",
    "**Pros:**\n",
    "* Faster than gradient descent\n",
    "* Noisier gradients from random subsets can help exit local minima\n",
    "\n",
    "**Cons:**\n",
    "* Can have slower convergence due to noisy gradients\n",
    "\n",
    "### Momentum and Weight Decay\n",
    "\n",
    "The [``optim.SGD`` optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) implements other helpful features aside from basic gradient descent including **momentum** and **weight decay**. As mentioned above, the gradients from SGD may be noisy and susceptible to random outliers. Momentum is one way to smooth out gradient descent updates by incorporating a moving average of previous gradients. Let $\\mu\\in[0, 1)$ be the momentum parameter, $g^{(k)}$ be the gradient over the batch in iteration $k$, and $\\theta^{(k)}$ be the model parameters as iteration $k$. The learning algorithm for SGD with momentum and learning rate $\\alpha$ is then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G^{(k)} &= g^{(k)} + \\mu G^{(k-1)}\\\\\n",
    "\\theta^{(k+1)} &= \\theta^{(k)} - \\alpha G^{(k)}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus, the gradients become as exponential moving average of previous gradients where all previous gradients contribute a weight of $\\mu^{k-m}$ from step $m$. Popular choices of $\\mu$ are larger values such as $0.9$ and $0.99$ so that the exponential moving average does not vanish too quickly.\n",
    "\n",
    "The weight decay parameter implements the same $L_2$ regularization we saw with linear regression in lectures 11 and 12. For loss function $\\ell(f_\\theta(x), y)$ and weight decay parameter $\\lambda$, the effective total loss function used for backpropagation is given by\n",
    "\n",
    "$$\n",
    "\\ell_{\\textrm{total}} = \\ell(f_\\theta(x), y)+\\frac{\\lambda}{2}\\lVert\\theta\\rVert_2^2.\n",
    "$$\n",
    "Thus, an additional component of $\\lambda\\theta$ is added to the gradient. The purpose of weight decay is to encourage model parameters to not be too large and thus prone to overfitting and dangerous outlier behavior. Common choices of weight decay vary by model type and size, however, a small value like $10^{-5}$ is often a safe starting point. Other parameters exist within the ``optim.SGD`` class, but these two are the most important to explain.\n",
    "\n",
    "The ``torch.optim`` package contains [many other popular optimizers](https://pytorch.org/docs/stable/optim.html#algorithms), such as Adagrad and Adam, which are popular **adaptive gradient** methods where per-layer learning rates are automatically tuned during training. For now, we will focus on applying SGD.\n",
    "\n",
    "To initialize the SGD optimizer, and most other optimizers, we need to give the optimizer:\n",
    "* A generator instance for the model parameters, i.e. call ``model.parameters()`` from our ``nn.Module`` object instance.\n",
    "* Learning rate\n",
    "* Momentum parameter (optional)\n",
    "* Weight decay (optional)\n",
    "* and other optional parameters for fancier learning algorithms\n",
    "\n",
    "#### A PyTorch training loop\n",
    "\n",
    "Finally, we have all the necessary ingredients to create a PyTorch training loop! This training loop, while basic, forms the core of any model training code within PyTorch. Before executing the training loop, we need to ensure a few things are set:\n",
    "* The model is instantiated/initialized.\n",
    "* The dataset is prepared.\n",
    "* The loss function and optimizer are instantiated.\n",
    "\n",
    "With these in hand, the training loop takes on the following basic cycle for optimizer named ``optimizer`` and loss function named ``criterion``:\n",
    "1. Zero out the gradients using the optimizer using ``optimizer.zero_grad()``.\n",
    "2. Pass the current batch or entire dataset to the model to generate predictions.\n",
    "3. Calculate loss from ``criterion``.\n",
    "4. Backpropagate from loss value and perform gradient descent update by ``optimizer.step()``\n",
    "5. (Optional) Perform any desired logging, e.g. loss values, performance metrics, etc.\n",
    "\n",
    "And that's it! The below code cell shows a general training loop we used throughout our Deep Net lectures this semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e064a691-8fee-4091-9c6f-18c3722d1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, optimizer, n_epochs, train_loader, val_loader):\n",
    "    loss_values, train_accuracies, val_accuracies = [], [], []\n",
    "    for n in tqdm(range(n_epochs)):\n",
    "        epoch_loss, epoch_acc = 0, 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            # zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "            # pass batch to model\n",
    "            predictions = model(x_batch)\n",
    "            # calculate loss\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            # backpropagate and update\n",
    "            loss.backward() # backprop\n",
    "            optimizer.step()\n",
    "            # logging to update epoch_loss (add loss value) and epoch_acc (add current batch accuracy)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += multiclass_model_accuracy(model, x_batch, y_batch)\n",
    "    \n",
    "        loss_values.append(epoch_loss/len(train_loader))\n",
    "        train_accuracies.append(epoch_acc/N_train)\n",
    "        # validation performance\n",
    "        val_acc = 0\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            # don't compute gradients since we are only evaluating the model\n",
    "            with torch.no_grad():\n",
    "                # validation batch accuracy\n",
    "                val_acc += multiclass_model_accuracy(model, x_batch, y_batch)\n",
    "        val_accuracies.append(val_acc/N_val)\n",
    "    return model, loss_values, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9550a6-8242-43c0-bec3-4c3ffa3e2075",
   "metadata": {},
   "source": [
    "### PyTorch Datasets\n",
    "\n",
    "PyTorch offers an abstract base class for creating datasets that simplifies the process of building, manipulating, and sampling datasets, e.g. into training, validation, and testing sets. The ``torch.utils.data.Dataset`` class requires any new class that inherits this base class to implement three methods:\n",
    "* ``__init__``: The ``__init__`` method is the constructor for the new dataset. Unlike the ``nn.Module`` class, the base class constructor does not need to be called, i.e. we do not need to call ``super().__init__()``. The constructor is most commonly used to establish the data for the dataset or the necessary information to assign attributes that will assist the data retrieval process in the ``__getitem__`` method.\n",
    "\n",
    "* ``__len__``: The ``__len__`` method overrides the ``len()`` function in Python to determine the length of the dataset. In other words, for a dataset named ``my_dataset``. The implemented ``__len__`` function will allow ``len(my_dataset)`` to return the length of the dataset.\n",
    "\n",
    "* ``__getitem__``: The ``__getitem__`` method overloads the use of brackets to index items in a dataset. For example, a dataset named ``my_dataset`` will call the ``__getitem__`` method when we use ``my_dataset[i]`` and the index ``i`` is an input to the ``__getitem__`` method.\n",
    "\n",
    "Let's take a look at an example dataset by implementing a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e41c6a7-11d0-41b4-a710-d8c84d8fab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 100 points\n",
      "Dataset point with index 2 is at x=tensor([-1.3538, -0.2662]) and label y=1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TwoClassDataset(Dataset):\n",
    "    # don't forget the self identifier!\n",
    "    def __init__(self, N, sigma):\n",
    "        self.N = N # number of data points per class\n",
    "        self.sigma = sigma # standard deviation of each class cluster\n",
    "        self.plus_class = self.sigma*torch.randn(N, 2) + torch.tensor([-1, 1])\n",
    "        self.negative_class = self.sigma*torch.randn(N, 2) + torch.tensor([1, -1])\n",
    "        self.data = torch.cat((self.plus_class, self.negative_class), dim=0)\n",
    "        self.labels = torch.cat((torch.ones(self.N), torch.zeros(self.N)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x, y # return input and output pair\n",
    "\n",
    "N = 50\n",
    "sigma = 1.5\n",
    "dataset = TwoClassDataset(N, sigma)\n",
    "\n",
    "plus_data = dataset.plus_class\n",
    "negative_data = dataset.negative_class\n",
    "print('Dataset has {} points'.format(len(dataset)))\n",
    "idx = 2\n",
    "x, y = dataset[idx]\n",
    "print('Dataset point with index {} is at x={} and label y={}'.format(idx, x, y))\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(plus_data[:, 0].numpy(), plus_data[:, 1].numpy(), color='tomato', s=50, edgecolor='black')\n",
    "plt.scatter(negative_data[:, 0].numpy(), negative_data[:, 1].numpy(), color='cornflowerblue', s=50, edgecolor='black')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539fb16-ff54-4bd2-a514-79d076a0f001",
   "metadata": {},
   "source": [
    "### PyTorch Dataloaders\n",
    "\n",
    "With a PyTorch ``Dataset`` class in hand, we may take advantage of the ``torch.utils.data.DataLoader`` interface that will simplify the process of sampling batches of data; shuffling the dataset; partitioning into training set, validation set, testing set; and more! A ``DataLoader`` does not need to be implemented like a ``Dataset`` or ``nn.Module`` class. Instead, we only need to provide a ``Dataset`` object as input alongside several optional inputs:\n",
    "* ``batch_size``: number of examples in each batch or call to the dataloader\n",
    "\n",
    "* ``shuffle``: Boolean option to shuffle dataset each pass or **epoch** through the dataset\n",
    "\n",
    "* ``sampler``: ``Sampler`` object that specifies how data will be extracted from the dataset. For example, the ``SubsetRandomSampler`` allows us to specify indices within the larger dataset to sample at random. This is an easy way to create training, validation, and testing sets!\n",
    "\n",
    "* Plenty other options that [may be explored here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7368191-b1ce-4e92-a339-48db237e611d",
   "metadata": {},
   "source": [
    "## Lecture 19: Multilayer Perceptrons (MLP)\n",
    "\n",
    "### Learning Objectives\n",
    "* Explain how non-linear activation functions allow the creation of more complex multi-layer machine learning models.\n",
    "* Describe the construction of multi-layer perceptrons and the representation of intermediate features.\n",
    "* Create multi-layer perceptrons in PyTorch and train these models on a provided dataset.\n",
    "\n",
    "The first deep net architecture we discussed was the multi-layer perceptron (MLP). We motivated the MLP by demonstrating how logistic regression models can only model linear class boundaries. Simple datasets, e.g. the Two Moons dataset in Lecture 19, with non-linear boundaries are failure cases for logistic regression. To enrich the logistic regression model, we introduced **non-linear activation functions** or **non-linearities** to give us the ability to learn multiple weight or parameter matrices.\n",
    "\n",
    "One individual weight matrix that transforms an input vector to another vector is commonly referred to as a **perceptron**. The concatenation of multiple perceptrons separated by non-linear activation functions is known as a **multi-layer perceptron** (MLP) or **fully-connected network** (we will skip the abbreviation since FCN is commonly used for something else in machine learning).\n",
    "\n",
    "As mentioned earlier, we may stack arbitrarily many perceptrons and non-linearities to form deeper neural nets. Each **layer** has an input and output dimension that is a **hyperparameter** of the **model architecture**. Every layer that is followed by an activation function is referred to as a **hidden layer** as it separates the inputs from the outputs of the model. The below figure depicts a three-layer MLP with two hidden layers. Each arrow represents a weight multiplying one entry of an input vector. The result of this multiplication is passed to a node in the next layer where the result at each node is the summation of all incoming arrows followed by an activation function.\n",
    "\n",
    "<div>\n",
    "<center><img src=\"mlp-figure.jpg\" width=\"800\"/> </center>\n",
    "</div>\n",
    "\n",
    "For arbitrary non-linear element-wise function $\\sigma(z)$, we may stack many **layers** of parameter matrices.\n",
    "$$\n",
    "f_\\theta(x) = W_K\\sigma(W_{K-1}\\sigma(W_{K-2} \\cdots W_2\\sigma(W_1x))).\n",
    "$$\n",
    "$$\n",
    "\\theta = \\{W_1, W_2, W_3, \\ldots, W_K\\}\n",
    "$$\n",
    "\n",
    "Above, we have $K-1$ hidden layers and a final parameter matrix $W_K$ at the last layer to produce class scores for our model. In other words, for an $M$-class classification problem $W_K\\in\\mathbb{R}^{d_{K-1}\\times M}$ where $d_{K-1}$ is the output dimension of the last hidden layer parameter matrix $W_{K-1}$\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Examples of activation functions are as follows:\n",
    "\n",
    "* [Rectified Linear Unit (ReLU)](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU):\n",
    "$$\n",
    "\\sigma(z) = \\begin{cases}\n",
    "z,~&z\\geq 0\\\\\n",
    "0,~&z<0\n",
    "\\end{cases}=\\max\\{0, z\\}\n",
    "$$\n",
    "* [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid):\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "* [Hyperbolic Tangent (Tanh)](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh):\n",
    "$$\n",
    "\\sigma(z) = \\frac{e^z-e^{-z}}{e^{z}+e^{-z}}\n",
    "$$\n",
    "* [Leaky ReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU)\n",
    "$$\n",
    "\\sigma_\\tau(z) = \\begin{cases}\n",
    "z,~&z\\geq 0\\\\\n",
    "-\\tau z~&z < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "* and many more may be [found here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f7db0-e402-411f-b1bb-211098b09898",
   "metadata": {},
   "source": [
    "## Lectures 20, 21, and 22: Convolutional Neural Networks (CNN)\n",
    "\n",
    "### Learning Objectives\n",
    "* Compare convolutional neural networks (CNN) and multi-layer perceptrons (MLP).\n",
    "* Describe the relevant parameters of convolutional layers including stride, padding, and kernel size.\n",
    "* Explain advantages of CNNs with respect to number of parameters and spatial information within data.\n",
    "* Understand how fully convolutional networks (FCN) remove the need for fully-connected layers and enable richer machine learning problems than ordinary image classification or regression.\n",
    "* Give examples of valid image augmentations for training CNNs.\n",
    "* Explain the role of image augmentaitons, batch norm, and dropout in training CNNs. MLPs have two clear shortcomings:\n",
    "\n",
    "1. Spatial information is not considered as all entries of the input are processed with no respect for which entries are closer to other. In other words, MLPs are **permutation-invariant** as we may shuffle input data and obtain the same result of model training. This is particularly problematic with image data when shuffling pixels destroys the spatial information in the data.\n",
    "2. They are parameter inefficient and quickly require enormous numbers of trainable parameters for larger inputs or deeper models.\n",
    "\n",
    "### Convolutional Neural Networks (CNN)\n",
    "In Lecture 20, we briefly reviewed the convolution operation. In doing so, we made two key observations. First, the size of a filter $H$ does not need to scale with the size of an input image $X$. Second, the use of convolution promotes spatial coherence in the resulting output. Thus, each entry in the output represents spatially meaningful information about the previous input. This suggests that we may consider replacing the fully-connected operators of multi-layer perceptrons with convolution operators for learning parameters in a machine learning model. As such, we may define a new type of model, referred to as a **convolutional neural network** denoted by\n",
    "\n",
    "$$\n",
    "f_\\theta(x) = H_K*\\sigma(H_{K-1}*\\sigma(H_{K-2} \\cdots H_2*\\sigma(H_1*x)))\n",
    "$$\n",
    "$$\n",
    "\\theta = \\{H_1, H_2, H_3, \\ldots, H_K\\},\n",
    "$$\n",
    "where each convolution operator $H_k\\in\\mathbb{R}^{C_{\\textrm{out}}\\times K_1\\times K_2\\times C_{\\textrm{in}}}$ has a specified number of input channels, output channels, and kernel dimensions in both height and width given by $K_1$ and $K_2$, respectively. All these different dimensions can be confusing, so let's unpack what we mean by input and output channels as well as how we convolve along input channels in a given input image or **feature map**. The below figure depicts a convolutional layer with 3 input channels, 2 output channels, and kernel size of $3\\times 3$. For each convolutional kernel, we have a stack of 3 *independent and unique* $3\\times 3$ filters. Convolving each kernel with the $3$-channel input image produces one 2D feature map. We also have 2 independent convolutional kernels; thus, the result of the entire convolutional layer has 2 equal-sized feature maps or **feature channels**.\n",
    "\n",
    "<div>\n",
    "<center><img src=\"convolution-with-multiple-filters2.png\" width=\"800\"/></center>\n",
    "</div>\n",
    "\n",
    "Altogether, a convolutional layer (ignoring bias terms) has $C_{\\textrm{out}}\\times K_1\\times K_2\\times C_{\\textrm{in}}$ learnable parameters. Typical choices of $K_1$ and $K_2$ are relatively small odd numbers, e.g. 3, 5, and 7, and we often keep kernels square, i.e. $K_1=K_2$. The number of input and output channels is a design choice we may make at intermediate layers, while the input layer will have the nubmer of input channels determined by the image modality, e.g. 3 input channels for RGB images and 1 input channel for grayscale images.\n",
    "\n",
    "Before creating a CNN and applying it to the FashionMNIST dataset, we should define some other key terminology and practices for CNNs. First, each convolutional layer has multiple parameters that must be set. The most important of these parameters are stride, padding, and kernel size.\n",
    "* **stride**: The stride of a convolutional layer determines how much the filter shifts between subsequent each elementwise product. The default choice would simply be one as we define in basic convolution. However, we may choose a stride of two, three, etc. in order to reduce the size of the resulting feature map. For example, increasing the stride from one to two will result in a feature map that is half as tall and half as wide as the input.\n",
    "\n",
    "* **padding**: The above examples demonstrate no padding as the convolutional kernels are not allowed to hang off the edge of the input image. The padding value determines how many zero-valued pixels are padded around the edge of the input image. We commonly choose a padding size of $\\frac{K-1}{2}$ for odd-sized square kernels of size $K$ so that output feature maps match the size of input feature maps. This choice results in the convolutional kernel as being effectively \"centered\" at each location in the input image to produce the result at the same location in the output feature maps.\n",
    "\n",
    "* **kernel size**: As previously mentioned, we may choose the size of the convolutional kernels at each layer and the size may be rectangular or square. The most common choices are relatively small, square, and odd-sized kernels, e.g. $3\\times 3$, $5\\times 5$, $7\\times 7$.\n",
    "\n",
    "For an input image $X\\in\\mathbb{R}^{C_{\\textrm{in}}\\times H_{\\textrm{in}} \\times W_\\textrm{in}}$ and convolutional layer with stride $s$, padding $p$, and kernel size $K_H\\times K_W$, the resulting height $H_\\textrm{out}$ and width $W_\\textrm{out}$ of the output feature maps will be\n",
    "$$\n",
    "\\begin{align}\n",
    "H_\\textrm{out} &= \\left\\lfloor\\frac{H_{\\textrm{in}}+2p-(K_H-1)-1}{s}+1\\right\\rfloor\\\\\n",
    "W_\\textrm{out} &= \\left\\lfloor\\frac{W_{\\textrm{in}}+2p-(K_W-1)-1}{s}+1\\right\\rfloor.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Pooling Layers\n",
    "Setting the stride and padding parameters is one way to reduce the size of subsequent features maps. Reducing the size of feature maps makes CNNs less computationally expensive as adjacent values in feature maps are often redundant. Another popular technique is the use of **pooling layers**. A pooling layer has its own kernel size and stride parameters used to mask out windows of feature maps. Within each of these windows, a simple function is applied to the elements of each feature map within the window. The kernel size and stride are commonly set to be equal; thus, the pooling operation tiles each feature map into non-overlapping segments. \n",
    "\n",
    "The two most common forms of pooling layers are **mean pooling** and **max pooling**. As the names suggest, mean pooling computes the mean of each window while max pooling takes the maximum value within each window. The below example depicts $2\\times 2$ max pooling for a single feature channel.\n",
    "\n",
    "<div>\n",
    "    <center><img src=\"MaxpoolSample2.png\", width=\"600\"/></center>\n",
    "</div>\n",
    "\n",
    "### Classification Heads\n",
    "Finally, after all of the convolutional layers in a CNN, we still need some mechanism for computing final class scores (respectively other scores for other machine learning tasks, e.g. regression). Suppose $Z_K\\in\\mathbb{R}^{C\\times H\\times W}$ is the final set of feature maps in a CNN with $C$ channels and size $H\\times W$. One popular approach is to apply **global average pooling** or **global max pooling** to reduce each feature channel to a single value. Thus,\n",
    "$$\n",
    "\\textrm{GlobalPooling}(Z_k)\\in\\mathbb{R}^{C}\n",
    "$$\n",
    "produces a length-$C$ vector. From here, a simple fully-connected layer with weight matrix $W\\in\\mathbb{R}^{M\\times C}$ (effectively a logistic regression model) may produce class scores for the $M$ classes.\n",
    "\n",
    "### Example Common CNN Structure for Classification\n",
    "In summary, a CNN for classification tasks is typically composed of convolution layers that are followed by non-linear activation functions just like we saw with MLP models. We commonly stack some number of convolution layers before appling a poolying layer, e.g. max pooling. After some number of blocks alternating convolution layers and pooling layers, we apply a global pooling operation, e.g. global max pooling, to reduce the feature maps at the last layer to a single value and thus produce a vector of feature values. Finally, we may produce class scores by applying a fully-connected layer to the feature values from the global pooling layer\n",
    "\n",
    "For example, we may construct a 5-layer CNN as follows for input RGB images of size $H\\times W$ as follows:\n",
    "\n",
    "<div>\n",
    "    <center><img src=\"five-layer-cnn.png\" width=\"800\"/></center>\n",
    "</div>\n",
    "\n",
    "### Fully Convolutional Networks\n",
    "Convolutional neural networks may be applied to many different tasks beyond simple image classification. Consider the tasks of image denoising, image super-resolution (improving resolution of images), semantic segmentation (labeling the class of every pixel in an image), and object detection (placing bounding boxes and classifiying objects in an image). The first three tasks, in particular, require producing another image. Consider the below figure depicting a semantic segmentation example.\n",
    "\n",
    "<div>\n",
    "    <center><img src=\"semantic-segmentation-example.png\" width=\"600\"></center>\n",
    "</div>\n",
    "\n",
    "Each pixel in this image is assigned its own label, like how we provide an image-level label for image classification. To accomplish such a task, we often use **encoder-decoder** CNN model architectures also known as **autoencoders**. The encoder stage proceeds like a normal CNN where successive convolutional layers and pooling layers reduce the spatial resolution of feature maps. The encoder and decoder stages meet at a bottleneck where the decoder stage begins upsampling the feature maps back towards the desired output resolution. Where the encoder stage applies pooling or strided convolution, the decoder stage performs **upsampling** or **transposed convolution** to increase the spatial resolution of feature maps. Finally, once the desired resolution is reached, we may use a $1\\times 1$ 2D convolution layer to combine the feature maps in the last layer to provide final class scores at each pixel like how a fully connected layer does for regular image classification.\n",
    "\n",
    "Such CNNs have no fully-connected layers (and in fact may avoid all pooling layers as well) and thus we refer to them as **fully convolutional networks** (FCN). An example autoencoder model is shown below.\n",
    "\n",
    "<div>\n",
    "    <center><img src=\"autoencoder-example.png\" width=\"600\"></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32d72e-9daf-4834-8db2-dfb2eb98f437",
   "metadata": {},
   "source": [
    "### Improving Model Generalization\n",
    "Thus far, we have discussed the need for machine learning models to generalize to unseen validation or testing data after being fit to provided training data. We have only considered how parameters of the learning algorithm, e.g. learning rate, weight decay, momentum, or more recently how the model architecture, e.g. number of layers, width of layers, choice of pooling, etc., may be selected or tuned to search for improved model generalization. None of these changes are inherently aimed at improving generalization, but rather design choices we may iterate in hopes of gains in validation/testing performance. Next, we consider two techniques and one more common deep learning layer that help with generalization.\n",
    "\n",
    "#### Image Augmentations\n",
    "One potential bottleneck that degrades model generalization is the availability of data. Deep learning models learn by recognizing common patterns in training data. The hope is that these patterns translate well to new, unseen data. With scarce data, this is quite challenging while abundant data helps capture the general distribution of information the model is being trained and thus evaluated on.\n",
    "\n",
    "A highly popular technique for improving model generalization and robustness is the use of **augmentations**, more specifically **image augmentations** for training CNNs and other vision models. Image augmentations apply simple transformations to input data that do not affect the underlying ground-truth of the data. For example, for the task of image classification, flipping an image of a cat will not change that the image is of a cat. However, shuffling all the pixels randomly makes the image of a cat indiscernible to humans and thus not a useful image augmentation. Some common image augmentations are described below.\n",
    "\n",
    "* **Random flipping**: Typically, this is a horizontal (left-right) flip of an image with some probability, e.g. 0.5. Some applications, for example recognizing digits or text, cannot use random flipping.\n",
    "* **Random Cropping**: For some range of cropping sizes and aspect ratios, pick a random rectangle of image coordinates to crop out.\n",
    "* **Blurring**: Various blurring filters may be applied to an image to distort edges or smooth information.\n",
    "* **Contrast/Color Adjustments**: Many different adjustments may be made to the brightness, hue, contrast, or saturation, as well as different color adjustments, e.g. gamma correction, to change the color appearance of images.\n",
    "* **Affine Transformations**: Any [translation, rotation, or shearing](https://www.mathworks.com/discovery/affine-transformation.html) of an image may be represented by an affine transformation matrix.\n",
    "* **...and plenty more**: [See the PyTorch and torchvision transformations here](https://pytorch.org/vision/main/transforms.html).\n",
    "\n",
    "Example image augmentations are depicted below.\n",
    "\n",
    "<div>\n",
    "    <center><img src=\"augmentation-examples.jpg\" width=\"800\"></center>\n",
    "</div>\n",
    "\n",
    "#### Dropout\n",
    "Deep learning models are often seen as over-parameterized, meaning that they have many more parameters than example data to train on. Informally speaking, this can give models a tendency to **overfit** to training data and memorize patterns in the data. The technique of **dropout** seeks to remove this dependency on memorization during training. For a given probability $p\\in(0, 1)$, a dropout layer will mute or set to zero each entry in an input features map with probability $p$. Dropout seeks to reduce a model's dependency on very specific features for certain training examples and encourage more robust and generalizable feature representations.\n",
    "\n",
    "<div>\n",
    "    <center><img src=\"dropout-example.png\" width=\"800\"></center>\n",
    "</div>\n",
    "\n",
    "#### Batch Norm\n",
    "The activations following non-linearities in a deep net are not guaranteed to follow any \"nice\" distribution of values. For example, a deep net with ReLU activations will only produce positive values that may be any arbitrarily large positive number. **Batch normalization** layers attempt to rectify the distribution of activations by applying a standard normalization. During training, the activations of a given layer $Z\\in\\mathbb{R}^{N\\times C\\times H \\times W}$ for $N$ examples in a batch with $C$ feature channels each of size $H\\times W$ are normalized by computing the per-channel mean and standard deviation within the batch. For feature channel $c$, let the mean and standard deviation be denoted by $\\mu_c$ and $\\sigma_c$. We apply [batch normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) elementwise for feature map $Z_c\\in\\mathbb{R}^{N\\times H\\times W}$ by\n",
    "\n",
    "$$\n",
    "\\textrm{BatchNorm}(Z_c) = \\frac{Z_c-\\mu_c}{\\sigma_c}*\\gamma_c+\\beta_c.\n",
    "$$\n",
    "\n",
    "Above, $\\gamma_c$ and $\\beta_c$ are learnable parameters for channel $c$ to skew the mean and standard deviation of the activations from a standard normal distribution as necessary during training. Thus, each channel has its own parameters and a batch norm layer will have $2C$ learnable parameters. During training, the model uses the batch mean and standard deviation to apply the normalization while also keeping running statistics for the dataset mean and standard deviation. At evaluation/testing time, we use the overall running statistics instead of the mini-batch statistics, which are susceptible to stochastic batch effects. In practice, batch norma has been shown to give strong improvements to CNNs and other deep net architectures, in particular for natural image datasets.\n",
    "\n",
    "#### The ``.eval()`` and ``.train()`` Methods\n",
    "The above dropout and batch norm layers clearly have behavior that is unique to training versus model evaluation and testing. For example, we do not want to randomly remove a fraction of features at a particular layer when evaluating our model; this is only helpful during model training. The ``nn.Module`` class provides a simple interface to toggle our deep nets between training and evaluation model. For a deep net model named ``my_model``, we may set all layers into training mode, e.g. activate dropout, by calling ``my_model.train()``. Then, when evaluating the model, we simply call ``my_model.eval()``. We alternate between these modes when training by backpropagation and when evaluating a deep net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e17b942-4c04-4c75-8496-01b8c77c530f",
   "metadata": {},
   "source": [
    "## Lecture 22: Recurrent Neural Networks (RNN)\n",
    "### Learning Objectives\n",
    "* Explain how recurrent neural networks (RNN) and convolutional neural networks differ.\n",
    "* Identify the equations that represent a long short-term memory (LSTM) RNN and use PyTorch to create an LSTM RNN.\n",
    "  \n",
    "Both MLPs and CNNs are referred to as **feed-forward** neural networks since inputs are passed to a deep net and the computation simply proceeds by a single forward pass through the model. Thus, the model has no memory or state from the previous input when receiving the next input. This may be undesirable when processing sequence data wherein the result at one time step clearly depends on previous time steps. As such, we would like a neural network architecture that has some notion of memory/state as processing from one time step may influence processing.\n",
    "\n",
    "Let $x\\in\\mathbb{R}^{T\\times d}$ be an example of sequence data where $x=\\{x_0, x_1, x_2, \\ldots, x_{T-1}\\}$ and $x_t\\in\\mathbb{R}^d$. For each time step, our model $f$ takes input $x_t$ and the **hidden state** of computation from previous time step $t-1$, $h_{t-1}\\in\\mathbb{R}^{H}$. Thus,\n",
    "$$\n",
    "f(x_t, h_{t-1}) = h_t.\n",
    "$$\n",
    "\n",
    "Such a model is referred to as a **recurrent neural network** (RNN) since the model's computation forms a recurrence, i.e. output at each time depends on outputs at previous times.\n",
    "\n",
    "For whatever task of interest, we have a hidden state of the model at each time step that is an $H$-dimensional vector. We may then apply a fully-connected layer to these each hidden state vector for an appropriate regression or classification problem. Consider, for example, a collection of sensors at a traffic intersection. At each time step, we can have several sensor readings, e.g. in-roadway induction loop, radar sensors, data from cameras, etc. An RNN will produce some **latent representation** $h_t\\in\\mathbb{R}^H$ at each step that we may use to estimate the speed of a vehicle (regression problem) or classify what kind of vehicle is passing through the intersection (classification problem).\n",
    "\n",
    "### Long Short-Term Memory Networks\n",
    "There has been much development over the years of various fundamental layers/units that form the $f(x_t, h_{t-1})$ shown above for RNNs. The most popular such architecture is the **long short-term memory** (LSTM) RNN. There are many pieces of computation that constitute one LSTM layer or **cell**: hidden state, cell state, input gate, forget gate, cell gate, and output gate.\n",
    "\n",
    "At time step $t$, the LSTM has input $x_t$, previous hidden state $h_{t-1}$, and cell state at previous time step $c_{t-1}$. The LSTM cell computes the following non-linear activation values:\n",
    "$$\n",
    "\\begin{align}\n",
    "    i_t &= \\sigma(W_{ii}x_t +W_{hi}h_{t-1})\\in[0, 1],\\quad&\\textrm{Input Gate Activation}\\\\\n",
    "    f_t &= \\sigma(W_{if}x_t+W_{hf}h_{t-1})\\in[0, 1],\\quad&\\textrm{Forget Gate Activation}\\\\\n",
    "    g_t &= \\textrm{tanh}(W_{ig}x_t+W_{hg}h_{t-1})\\in[-1, 1],\\quad&\\textrm{Cell Gate Activation}\\\\\n",
    "    o_t &= \\sigma(W_{io}x_t + W_{ho}h_{t-1})\\in[0, 1],\\quad&\\textrm{Output Gate Activation}\n",
    "\\end{align}\n",
    "$$\n",
    "Above, $\\sigma$ denotes the sigmoid function and $\\textrm{tanh}$ is the hyperbolic tangent function. The matrices $W_{ii}, W_{hi}, W_{if}, W_{hf}, W_{ig}, W_{hg}, W_{io}, W_{ho}$ represent the learnable weights in the LSTM cell. These respective gate activations control how the cell state and hidden state at the current time-step are updated. We obtain the present cell state and hidden state via:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    c_t &= f_t\\odot c_{t-1}+i_t\\odot g_t\\\\\n",
    "    h_t &= o_t\\odot\\textrm{tanh}(c_t)\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\odot$ represents the element-wise (Hadamard) product. Intuitively, the forget gate $f_t$ allows us to remove or keep values in the cell state when entries are close to 0 or 1, respectively. The input gate activation $i_t$ represents an encoding of which entries in the cell state we may modify and to what degree since these values are between 0 and 1. Together with the cell gate activation $g_t$, which has values from -1 to 1 due to the hyperbolic tangent activation, the element-wise product of $i_t\\odot g_t$ allows us to increase or decrease each value in the cell state. Altogether, these mechanisms allow us to retain, forget, or modify information in the cell state may occur from short-term or long-term dependencies in the data. Lastly, the hidden state $h_t$ uses the cell state projected between -1 to 1 by another hyperbolic tangent with one more gate activation $o_t$ to again modify or rescale the information in the cell state. It is important to note that the same weight matrices are used at each time-step.\n",
    "\n",
    "Finally, multiple LSTM cells may be stacked to form a multi-layer or **stacked LSTM** where the cell state and hidden state of each layer is passed to the next layer at a fixed time-step. Thus, an $L$-layer LSTM with data that has $T$ time-steps will produce $T\\times L$ hidden state vectors. For more discussion of LSTMs, [the following GitHub](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) contains an excellent tutorial. PyTorch also has an easy interface for implementing LSTM models using the [nn.LSTM module](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).\n",
    "\n",
    "The below diagram visualizes the computation of an LSTM cell. In each of the orange sigmoid or hyperbolic tangent layers, the appropriate weight matrices are multiplied into the input or hidden state vectors.\n",
    "\n",
    "<div>\n",
    " <center><img src=\"lstm-cell.png\" width=\"600\"></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c813464-766f-4c32-a822-fef3100880da",
   "metadata": {},
   "source": [
    "## Lecture 23: Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is an unsupervised method for dimensionality reduction. Reducing the dimensionality of data can often be helpful in machine learning applications, for example to make models more computationally efficient or more explainable. Consider a data matrix $X\\in\\mathbb{R}^{M\\times N}$ where we have $N$ data points and each column in $X$ is an $M$-dimensional data point or vector. PCA identifies $M$ orthogonal (thus, uncorrelated) vectors that point in the directions of maximum variance in the data. These $M$ vectors are referred to as **principal components** are each each $M$-dimensional vectors\n",
    "\n",
    "We may stack the principal components as rows in a square $M\\times M$ matrix. Let this PCA matrix be referred to as $P$. If we multiply the data matrix by $P$, we will have $Y=PX\\in\\mathbb{R}^{M\\times N}$. This transformation via PCA will rotate the data along the principal component directions and thus make each entry in each column of $Y$ be uncorrelated. To perform dimensionality reduction, we may instead only keep the first $L$ rows of $Y$ to form $Y_L\\in\\mathbb{R}^{L\\times M}$. Applying $Y_L$ to $X$ now will perform the same rotation as before, but now the discarded dimensions have been removed starting with the principal components in the data with the least variance or energy. Thus, we may reduce the dimensionality of the data while removing little important information.\n",
    "\n",
    "The below figure shows the principal components of 2D data. If we take the principal components as our new \"x\" and \"y\" axes, we see that the data is mostly explained or separated along the first principal component. Thus, we may discard the second principal component and retain most information in the data. This idea becomes even more powerful at higher dimensions. [Link to PCA lecture notes](https://courses.grainger.illinois.edu/ece364/fa2024/secure/LectureNotes/Lecture23.pdf).\n",
    "<div>\n",
    "    <center><img src=\"pca.png\" width=\"500\"></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546c9a9-c094-4306-b8d4-06d1d76e7ee3",
   "metadata": {},
   "source": [
    "## Lecture 24: K-Means Clustering and Gaussian Mixture Models (GMM)\n",
    "\n",
    "### K-Means Clustering\n",
    "\n",
    "The K-Means clustering algorithm is another unsupervised machine learning model that groups or clusters data into $K$ distinct clusters. Mathematically, the solution to K-Means clustering will produce $K$ cluster centers $\\{\\mu_k\\}_{k=1}^{K}$. Each data point is assigned to the nearest cluster center with respect to some distance metric, typically L2 distance/Euclidean distance. Such an assignment will minimize the sum of squared distances between each point and its cluster center.\n",
    "\n",
    "In summary, the K-Means algorithm (with L2 distance) iterates the following two steps assuming some initialization for the cluster centers.\n",
    "\n",
    "1. Assign each data point to the nearest cluster center.\n",
    "2. Update each cluster center by taking the mean of all data points currently assigned to each cluster center.\n",
    "\n",
    "<div>\n",
    "    <center><img src=\"k-means-example.png\" width=\"500\"></center>\n",
    "</div>\n",
    "\n",
    "[Link to lecture notes.](https://courses.grainger.illinois.edu/ece364/fa2024/secure/LectureNotes/Lecture24.pdf)\n",
    "\n",
    "### Gaussian Mixture Models\n",
    "\n",
    "Gaussian Mixture Models (GMM) are yet another unsupervised learning algorithm that provide another method of clustering data. Contrary to K-Means, GMMs produces a soft assignment of data points to each cluster. This means that points may have fractional assignment to each Gaussian distribution in the mixture where the sum of each points assignments equals one. Furthermore, GMMs express clusters as Gaussians that may have different variance in each dimension; thus, clusters need not be spherical like in K-Means. Instead they may be elliptical.\n",
    "\n",
    "To fit a Gaussian Mixture Model, we see to maximize the log likelihood of the data with respect to the distribution expressed by the mixture of Gaussians. In other words, the GMM represents a probabilitiy distribution for every possible point in the input data space, e.g. $\\mathbb{R}^2$ for 2D data points. Thus, the solution of a GMM maximizes the sum of log probabilities of each point according to this probability distribution. For more precise mathematical expressions, [please refer to the lecture notes](https://courses.grainger.illinois.edu/ece364/fa2024/secure/LectureNotes/Lecture24.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916cd90a-a972-47e2-b751-5318a82cb7bd",
   "metadata": {},
   "source": [
    "## Lecture 25: Generative Adversarial Networks (GAN)\n",
    "\n",
    "Generative Adversarial Networks are a generative deep learning model that is composed of two deep nets: a generator and a discriminator. The generator draws a random vector, e.g. an $N$-dimensional vector from some probability distribution, and produces an output data example that attempts to mimic examples in some training dataset. Most commonly, GANs are used to produce synthetic images; thus, the generator is a CNN that converts this random latent vector to an image. The discriminator takes input example, e.g. input images, and predicts if the input is a real data example from the training dataset or a fake, generated example from the generator.\n",
    "\n",
    "To train a GAN, a sort of \"game\" is played between the generator and the discriminator. The generator attempts to produce examples to fool the discirminator, i.e. maximize the loss of the discriminator, while the discriminator seeks to minimize its loss function by correctly identifying synthetic data examples. While difficult to train, GANs have been shown to be highly effective in creating novel, synthetic data that strongly resembles the datasets they are trained on.\n",
    "\n",
    "[Link to lecture notes.](https://courses.grainger.illinois.edu/ece364/fa2024/secure/LectureNotes/Lecture25.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
