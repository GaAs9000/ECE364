{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847696bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ECE 364 Lecture 8\n",
    "## Automatic Differentiation II: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb01c63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture goals\n",
    "After this lecture, you should be able to:\n",
    "* Depict the computational graph of toy multivariable functions.\n",
    "* Trace the forward pass of a function's computational graph.\n",
    "* Perform the backward pass of a computational graph by hand to compute the partial derivatives of all variables.\n",
    "* Use the ``torch.autograd``engine to perform backpropagation through a computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea2e91-bbee-45f5-a7c8-bf2e126704cd",
   "metadata": {},
   "source": [
    "## Computational Graphs\n",
    "\n",
    "Recall from previous lectures that we may break apart complicated functions as a composition of simple functions. For example,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x, y) &= x^2\\cos(xy)+1\\\\\n",
    "f(x, y) &= f_1(f_2(f_3(x,y),f_4(f_5(x, y)))))\\\\\n",
    "f_5(x, y) &= xy\\\\\n",
    "f_4(f_5) &= \\cos(f_5)\\\\\n",
    "f_3(x, y) &= x^2\\\\\n",
    "f_2(f_3, f_4) &= f_3f_4\\\\\n",
    "f_1(f_2) &= f_2+1\\\\\n",
    "f(x, y) &= f_1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This decomposition was helpful to systematically compute (partial) derivatives via chain rule. In this lecture, we will see how such a depiction of multivariable functions will allow us to efficiently and exactly compute partial derivatives (and by definition gradients). The above $f(x, y)$ may be represented as a **directed acyclic graph** (DAG) also referred to as a computational graph. Each node in the graph represents intermediate results of computation or values, while edges define functions applied to the values at each node.\n",
    "\n",
    "For the above $f(x, y)$, we obtain the following computational graph where each $w_i$ gives the result of each intermediate operation, including the input of variables $x=w_1$ and $y=w_2$.\n",
    "\n",
    "<div>\n",
    "<center><img src=\"computational_graph.png\" width=\"800\"/> </center>\n",
    "</div>\n",
    "\n",
    "According to this graph, we have defined\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_1 &=x\\\\\n",
    "w_2 &=y\\\\\n",
    "w_3 &= w_1^2\\\\\n",
    "w_4 &= w_1w_2\\\\\n",
    "w_5 &= \\cos(w_4)\\\\\n",
    "w_6 &= w_3w_5\\\\\n",
    "w_7 &= w_6+1.\n",
    "\\end{align*}\n",
    "$$\n",
    "We can double-check our equations from the computational graph by plugging back in starting from $f(x,y)=w_7$.\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x, y) &= w_7\\\\\n",
    "&= w_6+1\\\\\n",
    "&= w_3w_5+1\\\\\n",
    "&= w_1^2\\cos(w_4)+1\\\\\n",
    "&= x^2\\cos(w_1w_2)+1\\\\\n",
    "&= x^2\\cos(xy)+1.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22fffa3-bd3c-47e5-b005-5ca0106b34fe",
   "metadata": {},
   "source": [
    "## Lecture Exercise (Part 1):\n",
    "Consider the function\n",
    "$$\n",
    "g(x, y) = xye^{xy}. \n",
    "$$\n",
    "Sketch the computational graph of $g(x, y)$.\n",
    "\n",
    "**Hint**: Think of this as $g(x, y)= h(x,y)e^{h(x, y)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd83eb6-bc52-4571-b189-91e0db8c977a",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Using computational graphs, we would like to develop a procedure for exact and efficient automatic differentiation. The procedure we will examine is known as **backpropagation**. Backpropagation is an example of auto-differentation by **reverse accumulation** wherein gradients are accumulated by traversing the computational graph backwards from every seed node. This accumulation is accomplished by two traversals or *passes* through the graph: a forward pass and a backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b27a1b1-8bb2-4b0f-bb23-39fad2391a92",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "The forward pass through a computational graph is the direct calculation of applying the represented function to the provided inputs. Starting from every **input node**, $w_1$ and $w_2$ in our example, we follow each directed edge to the next node and perform the indicated operation to the available input(s) to generate the intermediate value at this next node. The next node then transmits this result to any of its **successor nodes** and so on until we reach any node(s) that have no successors. For the purposes of backpropagation, we refer to these end nodes with no successors as **seed nodes**. Below, we depict the forward pass through $f(x, y) = x^2\\cos(xy)+1$ for $(x, y)=(2, \\frac{\\pi}{2})$.\n",
    "\n",
    "<div>\n",
    "<center><img src=\"forward_pass.png\" width=\"800\"/> </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1368d-8968-4d21-b298-7d7d734e3c30",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "After completing the forward pass, we now have exact values of the computation from each node. We also know the operation performed along each edge as each successor node may store the operation performed to obtain its result and the nodes for which it acts as the successor. For example, node $w_6$ performs the multiplication operation from nodes $w_3$ and $w_5$, i.e. $w_6=w_3w_5$. In summary, we have values at each node, operations at each node, and a directed acyclic graph structure which may be traversed backwards from each seed node.\n",
    "\n",
    "Starting from a seed node, backpropagation works backwards to each **predecessor node** and transmits the partial derivative with respect to that predecessor node according to the stored function values and known gradient function. For example, a node that multiplies two inputs has the stored values and can easily define a gradient function with respect to each input as being the other input, i.e. $\\frac{\\partial w_iw_j}{\\partial w_i}=w_j$. More concretely, a successor node $w_i$ will propagate the partial derivative with respect to each of its predecessor nodes to that node. Above, this could be $w_4$ transmits $\\frac{\\partial w_4}{\\partial w_1}$ to $w_1$ and $w_4$ transmits $\\frac{\\partial w_4}{\\partial w_2}$ to $w_2$. Again, each node has a defined forward operation, backward operation (gradient function), and stored values from the forward pass. Below, we depict each of the backpropagated partial derivatives.\n",
    "\n",
    "<div>\n",
    "<center><img src=\"backprop_partial.png\" width=\"800\"/> </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566d550-7d65-4383-a82b-0675b6f10c90",
   "metadata": {},
   "source": [
    "## Lecture Exercise (Part 2):\n",
    "Return to your computational graph from Part 1 and label all the partial derivatives moving backwards from each node to its predecessors, i.e. like in the above example for $f(x ,y)$. For convenience, recall that \n",
    "$$\n",
    "g(x, y) = xye^{xy}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f73cbf-20bf-4768-b848-c6a036729d4d",
   "metadata": {},
   "source": [
    "## Accumulating Gradients\n",
    "Propagating these partial derivatives and accumulating more complicated derivatives by chain rule at each node, however, still requires some coordination. We refer to these accumulated partial derivatives along the backward pass as the **adjoint** at each node and we denote the adjoint at node $i$ as $\\bar{w}_i$. The **adjoint** at each node is calculated by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\bar{w}_i &= \\frac{\\partial f}{\\partial w_i}\\\\\n",
    "    \\bar{w}_i &= \\sum_{j\\in\\textrm{successors}(w_i)}\\bar{w}_j\\frac{\\partial w_j}{\\partial w_i}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's compute the adjoint values for the above computational graph to gain some intuition for the above equations. We first have $\\bar{w}_7$ as the \"base case\" since $f(x, y)=w_7$; thus,\n",
    "\\begin{align*}\n",
    "    \\bar{w}_7 &= \\frac{\\partial f}{\\partial w_7}\\\\\n",
    "             &=1.\n",
    "\\end{align*}\n",
    "\n",
    "Next, $w_7$ backpropagates to its predecessor $w_6$:\n",
    "\\begin{align*}\n",
    "    \\bar{w}_6 &= \\bar{w}_7\\frac{\\partial w_7}{\\partial w_6}\\\\\n",
    "    &= \\frac{\\partial f}{\\partial w_7}\\frac{\\partial w_7}{\\partial w_6}\\\\\n",
    "    &= 1\\\\\n",
    "\\end{align*}\n",
    "Then, $w_6$ backpropagates to its predecessors $w_5$ and $w_3$:\n",
    "\\begin{align*}\n",
    "    \\bar{w}_5 &= \\bar{w}_6\\frac{\\partial w_6}{\\partial w_5}\\\\\n",
    "    &= \\frac{\\partial f}{\\partial w_6}\\frac{\\partial w_6}{\\partial w_5}\\\\\n",
    "    &= w_3\\\\\n",
    "    \\bar{w}_3 &= \\bar{w}_6\\frac{\\partial w_6}{\\partial w_3}\\\\\n",
    "     &= \\frac{\\partial f}{\\partial w_6}\\frac{\\partial w_6}{\\partial w_3}\\\\\n",
    "     &= w_5.\n",
    "\\end{align*}\n",
    "And so on! The below figure shows all of these adjoint quantities in green.\n",
    "\n",
    "<div>\n",
    "    <center><img src=\"backprop_adjoint.png\" width=\"800\"/></center>\n",
    "</div>\n",
    "\n",
    "Of note, we see that\n",
    "\\begin{align*}\n",
    "    \\bar{w}_1 &= \\bar{w}_3\\frac{\\partial w_3}{\\partial w_1} + \\bar{w}_4\\frac{\\partial w_4}{\\partial w_1}\\\\\n",
    "    &= \\frac{\\partial f}{\\partial w_3}\\frac{\\partial w_3}{\\partial w_1}+\\frac{\\partial f}{\\partial w_4}\\frac{\\partial w_4}{\\partial w_1}\\\\\n",
    "    &= 2w_1w_5-w_2w_3\\sin(w_4)\n",
    "\\end{align*}\n",
    "since $w_1$ has two successor nodes and thus $\\frac{\\partial f}{\\partial w_1}$ backpropagates through two separate paths that depend on $w_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7bfc5-d028-4152-870a-339a0a655660",
   "metadata": {},
   "source": [
    "## Lecture Exercise (Part 3):\n",
    "Perform backpropagation by computing and labeling the adjoints, i.e. each of the $\\bar{w}_i=\\frac{\\partial g}{\\partial w_i}$ values, for your computational graph of $g(x,y)$ from Parts 1 and 2.\n",
    "\n",
    "**Hint**: Assuming we have $w_1=x$ and $w_2=y$, we know that $\\bar{w}_1=\\frac{\\partial g}{\\partial x}$ and $\\bar{w}_2=\\frac{\\partial g}{\\partial y}$. You can verify your results by making sure $\\bar{w}_1$ and $\\bar{w}_2$ match the below hand-calculated partial derivatives:\n",
    "$$\n",
    "\\begin{align*}\n",
    "g(x,y) &= xye^{xy}\\\\\n",
    "\\frac{\\partial g}{\\partial x} &= ye^{xy}+xy^2e^{xy}\\\\\n",
    "\\frac{\\partial g}{\\partial y} &= xe^{xy}+x^2ye^{xy}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4631dd3c-78cc-4ebe-9380-24af1155c64c",
   "metadata": {},
   "source": [
    "## And finally (drumroll)...\n",
    "Altogether, we arrive at our final backpropagation results and final partial derivatives at the input nodes $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$.\n",
    "\n",
    "<div>\n",
    "    <center><img src=\"backprop_full.png\" width=\"800\"/></center>\n",
    "</div>\n",
    "\n",
    "The entire procedure of backpropagation only requires one forward pass through the computational graph to establish the values at each node and one backward pass to accumulate gradients from seed nodes back through the entire graph. The backward pass is made significantly more efficient by the computed adjoint values that represent the accumulated gradients up to that node via chain rule. Each predecessor node re-uses the adjoint values of its successors and accumulates the partial derivatives of its successors with respect to itself. Thus, backpropagation may be seen as a form of **dynamic programming** as we recursively re-use previous computation for the next iteration or step of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39ad52-12de-40b7-8982-660bdb3bd6e6",
   "metadata": {},
   "source": [
    "### Is this the only way to do gradient accumulation?\n",
    "The answer is no! Backpropagation is an example of reverse accumulation of gradients and there is, in fact, forward accumulation versions that also utilize chain rule to compute gradients in computational graphs. PyTorch and any other deep learning framework implement backpropagation for automatic differentiation; however, the interested reader can refer to [this Wikipedia article](https://en.wikipedia.org/wiki/Automatic_differentiation#Automatic_differentiation_using_dual_numbers) for information on how dual numbers are used for forward-mode accumulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f030e-70e0-4a89-ba50-d8b59d5c2524",
   "metadata": {},
   "source": [
    "## Comparing to the ``torch.autograd`` Engine\n",
    "\n",
    "We explored the use of PyTorch to automatically compute derivatives in an earlier lecture, but now we can more precisely inspect the partial derivatives at each node in the computational graph using our knowledge of backpropagation. The below code implements every step of the computational graph of $f(x, y)=x^2\\cos(xy)+1$ and compares our hand-calculated adjoint values to the backpropagation values generated by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6054f8f-4dff-495e-9a15-1718aaebba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing our calculations to PyTorch Autograd:\n",
      "w1: Manual = tensor([-0.0931]), PyTorch = tensor([-0.0931])\n",
      "w2: Manual = tensor([-0.8660]), PyTorch = tensor([-0.8660])\n",
      "w3: Manual = tensor([0.5000], grad_fn=<CosBackward0>), PyTorch = tensor([0.5000])\n",
      "w4: Manual = tensor([0.8660]), PyTorch = tensor([0.8660])\n",
      "w5: Manual = tensor([1.], grad_fn=<PowBackward0>), PyTorch = tensor([1.])\n",
      "w6: Manual = 1, PyTorch = tensor([1.])\n",
      "w7: Manual = 1, PyTorch = tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.tensor([float(-1)]) # make sure gradients are computed when backpropagation is called\n",
    "y = torch.tensor([np.pi/3])\n",
    "x.requires_grad = True\n",
    "y.requires_grad = True\n",
    "\n",
    "w1 = x\n",
    "w2 = y\n",
    "w3 = w1**2\n",
    "w4 = w1*w2\n",
    "w5 = torch.cos(w4)\n",
    "w6 = w3*w5\n",
    "w7 = w6+1\n",
    "f = w7\n",
    "\n",
    "# manual gradients\n",
    "with torch.no_grad():\n",
    "    # adjoints\n",
    "    w7bar = 1\n",
    "    w6bar = 1\n",
    "    w5bar = w3\n",
    "    w4bar = -w3*torch.sin(w4)\n",
    "    w3bar = w5\n",
    "    w2bar = -w1*w3*torch.sin(w4)\n",
    "    w1bar = 2*w1*w5 - w2*w3*torch.sin(w4)\n",
    "    \n",
    "# automatic gradients via backpropagation\n",
    "w3.retain_grad(), w4.retain_grad(), w5.retain_grad(), w6.retain_grad(), w7.retain_grad() # making sure PyTorch populates all gradients\n",
    "f.backward() # initiate backpropagation from f as the seed node\n",
    "\n",
    "print('Comparing our calculations to PyTorch Autograd:')\n",
    "print('w1: Manual = {}, PyTorch = {}'.format(w1bar, w1.grad))\n",
    "print('w2: Manual = {}, PyTorch = {}'.format(w2bar, w2.grad))\n",
    "print('w3: Manual = {}, PyTorch = {}'.format(w3bar, w3.grad))\n",
    "print('w4: Manual = {}, PyTorch = {}'.format(w4bar, w4.grad))\n",
    "print('w5: Manual = {}, PyTorch = {}'.format(w5bar, w5.grad))\n",
    "print('w6: Manual = {}, PyTorch = {}'.format(w6bar, w6.grad))\n",
    "print('w7: Manual = {}, PyTorch = {}'.format(w7bar, w7.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d40b9-1f69-4cda-b7dc-52452a4b54bb",
   "metadata": {},
   "source": [
    "## Lecture Exercise (Part 4):\n",
    "Use the ``torch.autograd`` engine like above to verify the adjoints and final gradient results for $h(x, y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20622181-2e1b-4be9-998f-30527b813ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.tensor([1.], requires_grad=True) # make sure gradients are computed when backpropagation is called\n",
    "y = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "# create each w_i node in your graph\n",
    "w_1 = # fill in these lines\n",
    "w_2 =\n",
    "w_3 = \n",
    "w_4 = \n",
    "w_5 = \n",
    "h = w_5\n",
    "# manual gradients\n",
    "with torch.no_grad():\n",
    "    # fill in your adjoints Part 3\n",
    "    w1bar = # fill in these lines\n",
    "    w2bar = \n",
    "    w3bar = \n",
    "    w4bar = \n",
    "    w5bar = \n",
    "# automatic gradients via backpropagation\n",
    "w3.retain_grad(), w4.retain_grad(), w5.retain_grad()\n",
    "h.backward()\n",
    "\n",
    "# print comparisons\n",
    "print('Comparing our calculations to PyTorch Autograd:')\n",
    "print('w1: Manual = {}, PyTorch = {}'.format(w1bar, w1.grad))\n",
    "print('w2: Manual = {}, PyTorch = {}'.format(w2bar, w2.grad))\n",
    "print('w3: Manual = {}, PyTorch = {}'.format(w3bar, w3.grad))\n",
    "print('w4: Manual = {}, PyTorch = {}'.format(w4bar, w4.grad))\n",
    "print('w5: Manual = {}, PyTorch = {}'.format(w5bar, w5.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6561f17-8606-4e05-a100-efe1c2da073c",
   "metadata": {},
   "source": [
    "## Next time: Utilizing the ``torch.autograd`` engine for optimization\n",
    "\n",
    "In part 3 of our auto-differentation lectures, we will connect our knowledge from part 1 about gradient descent and function optimization with the auto-differentiation of this lecture to let PyTorch automatically perform optimization problems like finding local minima of a function and even perform function approximation from toy datasets. There is much much more to be said and utilized from the ``torch.autograd`` and ``torch.optim`` modules. We will explore some next time and more later in this course!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
