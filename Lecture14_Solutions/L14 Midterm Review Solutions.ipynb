{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14597222-1e44-4cf5-8ea8-faf19aea5384",
   "metadata": {},
   "source": [
    "# ECE 364 Lecture 14: Midterm 1 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c57c5bd-2e8d-43bc-b3c6-52df4bba3724",
   "metadata": {},
   "source": [
    "# Midterm Logistics\n",
    "## When?\n",
    "Tuesday, October 15th, 9:30-10:50am (in class)\n",
    "## Where?\n",
    "ECEB 3081\n",
    "## What?\n",
    "* Homeworks 1-4\n",
    "    * Homework 1: PyTorch basics, tensors, storage and memory, indexing and slicing, derivatives and chain rule\n",
    "    * Homework 2: Linear algebra (on paper, in PyTorch), more differentiation and chain rule, derivatives with matrices and vectors\n",
    "    * Homework 3: Gradient descent, computational graphs, backpropagation\n",
    "    * Homework 4: Linear regression: one-dimensional, multi-dimensional, transformations of variables (i.e. non-linear function approximation like HW4 P2)\n",
    "* Lectures 2-12\n",
    "## How?\n",
    "* Mixture of free-response, fill-in-the-blank (complete the code), multiple choice\n",
    "* Less focus on code syntax, more on the concepts of what code is trying to accomplish or the mathematical concepts\n",
    "## Why?\n",
    "Because grades :(\n",
    "## Who?\n",
    "All of you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a58aa-69f2-4d2b-8f66-ad168c1bda66",
   "metadata": {},
   "source": [
    "# Homework 1 Content\n",
    "## PyTorch basics, tensors, storage and memory, indexing and slicing, derivatives and chain rule\n",
    "* Tensors are the primary class we use for storing and acting on data in PyTorch.\n",
    "    * Choosing the appropriate data type and shape for a tensor is important to efficiently store data.\n",
    "* Operations on tensors can either create a new tensor or create a **view** of a Tensor.\n",
    "    * For example operations with an underscore, e.g. ``torch.cos_(x)``, operate **in-place** and thus do not allocate new memory. \n",
    "* Tensor views share the same underlying memory and thus point to the same locations in memory.\n",
    "* Tensor/array slicing are efficient ways to access or collect values according to certain conditions from a tensor/array\n",
    "    * We may use slicing to extract only a particular column or every other element in a row, for example.\n",
    "    * We may use Boolean operators to create **truth arrays** to separate elements based on a condition\n",
    "* We also reviewed differentiation and chain rule as they play a core role to auto-differentiation engines like PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b7563-086c-4962-8df5-742e9f1aec5b",
   "metadata": {},
   "source": [
    "## Tensors and storage practice\n",
    "\n",
    "Consider the below code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58430e8-9320-4b0d-b57f-b2dc0a19d8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.zeros(3, 2)\n",
    "b = a.cos_().add_(b)\n",
    "c = a+b\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb42463-b1c2-4f10-869d-22f491b9aba7",
   "metadata": {},
   "source": [
    "a) Does ``b`` share the same pointer as ``a``? Does ``c`` share the same pointer as ``b``?\n",
    "\n",
    "``b`` shares the same pointer as ``a`` due to the in-place cosine function. ``c`` allocates a new tensor because the given addition is not in-place\n",
    "\n",
    "b) What would be printed if we ran ``print(a, b, c)``?\n",
    "\n",
    "``a`` = 3x2 matrix of ones\n",
    "\n",
    "``b`` = 3x2 matrix of ones\n",
    "\n",
    "``c`` = 3x2 matrix of twos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac9b9ad-a72c-4835-94f4-5829c3612437",
   "metadata": {},
   "source": [
    "## Slicing and truth arrays practice\n",
    "Consider the below code snippet which creates a (10, 10) Tensor of random integers between $\\{-99, 99\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a1ff06-7dde-42e4-9edb-88594fec13d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73, -99,  31, -59,  -5, -59, -86,  41, -11, -11],\n",
      "        [-73,   9, -91,  67,  27, -96, -26,  98, -91,   0],\n",
      "        [ 49, -69, -45,  71, -38, -26,  25, -51,  49, -35],\n",
      "        [-40,  25, -57,  -9, -29, -67,  -7,  -6, -33,  12],\n",
      "        [ 41, -58, -48, -80, -84,  20,  -7, -79, -59, -90],\n",
      "        [-16,  22,  -1, -79,  50,   7, -77, -76, -99,  -2],\n",
      "        [-98, -10,  -8, -27,  88, -36, -31, -99,  72, -22],\n",
      "        [ 30,  58,  46,  97,  54,   2, -88, -73, -29,   1],\n",
      "        [-90,  16,  40, -23, -79,  -3, -42,  46, -62,  82],\n",
      "        [-82,  21, -30,  33,  46, -66, -35,  -8,  17, -94]])\n",
      "tensor([[-59, -86,  41, -11, -11],\n",
      "        [-96, -26,  98, -91,   0],\n",
      "        [-26,  25, -51,  49, -35],\n",
      "        [-67,  -7,  -6, -33,  12],\n",
      "        [ 20,  -7, -79, -59, -90]])\n",
      "tensor([-59,  67,  71,  -9, -80, -79, -27,  97, -23,  33])\n",
      "tensor([-80])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randint(low=-99, high=100, size=(10, 10))\n",
    "print(a)\n",
    "d = a[0:5, 5:10]\n",
    "print(d)\n",
    "e = a[:, 3]\n",
    "print(e)\n",
    "h = e[e%4==0]\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f028c-f510-48d1-bf34-bf0809c8d53f",
   "metadata": {},
   "source": [
    "a) We would like for ``d`` to contain the top-right quadrant of ``a``. What should ``b`` and ``c`` be to accomplish this?\n",
    "\n",
    "``b`` = 0:5 or :5\n",
    "\n",
    "``c`` = 5:10 or 5:\n",
    "\n",
    "b) We would like for ``h`` to contain all multiples of 4 in the third column of ``a``. What should ``f``, ``g`` and ``i`` be to accomplish this? Note: you may assume the \"zeroth\" column refers to index 0, \"first\" to index 1, and so on.\n",
    "\n",
    "``f`` = :\n",
    "\n",
    "``g`` = 3\n",
    "\n",
    "``i`` = e%4==0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14063edc-2818-4c1c-8a75-4303a0ffc351",
   "metadata": {},
   "source": [
    "# Homework 2 Content\n",
    "## Linear algebra review, linear algebra with PyTorch, more differentiation and chain rule including matrices and vectors\n",
    "* We reviewed linear algebra basics including dot products, matrix operations: multiplication, element-wise product, transpose, inverse, norms, gradients, and basic eigendecomposition.\n",
    "* PyTorch readily performs linear algebra operations and may help us automate common calculations like solving systems of equations (via matrix inverse), for example.\n",
    "* Multivariable functions have partial deriviatives with respect to each function variable. The collection of partial derivatives is referred to as the gradient.\n",
    "* Scalar or vector-valued functions with vector and matrix arguments may also have partial derivatives with respect to the scalar or matrix arguments.\n",
    "* For vector-valued functions, we have the **Jacobian** which represents the partial derivative of each entry in the output vector with repect to each element in the input vector.\n",
    "* Scalar-valued example (gradient): $x\\in\\mathbb{R}^n,~y\\in\\mathbb{R}^n, A\\in\\mathbb{R}^{n\\times n}$\n",
    "$$\n",
    "f(x, y, A) = y^\\top Ax \\in\\mathbb{R}.\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial  f}{\\partial x} = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2} & \\cdots \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}^{\\top}\n",
    "$$\n",
    "Note that the shape of $\\frac{\\partial  f}{\\partial x}$ matches the shape of $x$!\n",
    "\n",
    "* Vector-valued example (Jacobian): $x\\in\\mathbb{R}^n, A\\in\\mathbb{R}^{m\\times n}$\n",
    "$$\n",
    "\\mathbf{f}(x, A) = Ax \\in\\mathbb{R}^{m}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial  \\mathbf{f}}{\\partial x} = \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\cdots & \\frac{\\partial f_1}{\\partial x_n}\\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} & \\cdots & \\frac{\\partial f_2}{\\partial x_n}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\frac{\\partial f_m}{\\partial x_2} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\\\\\n",
    "\\end{bmatrix}\\in\\mathbb{R}^{m\\times n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e5a68-eb16-4f8b-9ef0-647e0c297e63",
   "metadata": {},
   "source": [
    "## Gradients with vectors and matrices\n",
    "Compute each of the requested partial derivatives for the given functions. You may assume $x\\in\\mathbb{R}^n$ and $A\\in\\mathbb{R}^{m\\times n}$.\n",
    "\n",
    "a) Compute $\\frac{\\partial f}{\\partial x}$ for $f(x) = \\mathrm{Tr}(xx^\\top)$, recall that $\\mathrm{Tr}(A)=\\sum_{i}^{n}A_{ii}$.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "We start by observing that $xx^\\top$ is an $n\\times n$ matrix as follows:\n",
    "\n",
    "$$\n",
    "xx^\\top = \\begin{bmatrix}\n",
    "x_1x_1 & x_1x_2 & \\cdots & x_1x_n\\\\\n",
    "x_2x_1 & x_2x_2 & \\cdots & x_2x_n\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_nx_1 & x_nx_2 & \\cdots & x_nx_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Thus, the trace of $xx^\\top$ will be given by\n",
    "$$\n",
    "\\mathrm{Tr}(xx^\\top) = \\sum_{i=1}^{n}x_i^2.\n",
    "$$\n",
    "The partial derivative of $f(x)$ with respect to each $x_i$ will then be $2x_i$. Therefore, the overall partial derivative will be \n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 2x.\n",
    "$$\n",
    "b) Compute $\\frac{\\partial f}{\\partial x}$ for $f(x) = \\frac{1}{2}\\lVert Ax\\rVert_2^2$.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "We may approach the partial $\\frac{\\partial f}{\\partial x}$ in a couple different ways.\n",
    "\n",
    "**Option 1**:\n",
    "\n",
    "Let $y=Ax$. We may then write\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{1}{2}\\lVert Ax\\rVert_2^2 &= \\frac{1}{2}\\lVert y\\rVert_2^2\\\\\n",
    "    \\frac{\\partial f}{\\partial x} &= \\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial x}.\n",
    "\\end{align*}\n",
    "$$\n",
    "Above, we use chain rule to re-write the desired partial derivative where $\\frac{\\partial y}{\\partial x}\\in\\mathbb{R}^{m\\times n}$ is the Jacobian of $y$ with respect to $x$. Let $J_y\\in \\mathbb{R}^{m\\times n}$ be this Jacobian $\\frac{\\partial y}{\\partial x}$. In matrix form, $J_y$ will look like\n",
    "$$\n",
    "J_y =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}\\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "The partial derivative $\\frac{\\partial f}{\\partial y}$ will be a vector in $\\mathbb{R}^{m}$. Thus, we have two ways to express express $\\frac{\\partial f}{\\partial x}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial f}{\\partial x} &= J_y^\\top\\frac{\\partial f}{\\partial y} = \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_1}\\\\\n",
    "\\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y_1}{\\partial x_n} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial y_1}\\\\\n",
    "\\frac{\\partial f}{\\partial y_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial f}{\\partial y_m}\n",
    "\\end{bmatrix}\\\\\n",
    "    \\frac{\\partial f}{\\partial x} &= \\left(\\frac{\\partial f}{\\partial y}\\right)^\\top J_y = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial y_1} &\n",
    "\\frac{\\partial f}{\\partial y_2} &\n",
    "\\cdots &\n",
    "\\frac{\\partial f}{\\partial y_m}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}\\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "The first solution would give us $\\frac{\\partial f}{\\partial x}\\in\\mathbb{R}^n$ while the second solution is the transpose of this with $\\frac{\\partial f}{\\partial x}\\in\\mathbb{R}^{1\\times n}$.\n",
    "\n",
    "With the application of chain rule set, we may determine the require partial derivatives and obtain the solution.\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(y) &= \\frac{1}{2}\\lVert y\\rVert_2^2\\\\\n",
    "&= \\frac{1}{2}y^\\top y\\\\\n",
    "\\frac{\\partial f}{\\partial y} &= y\n",
    "\\end{align*}\n",
    "$$\n",
    "To find the Jacobian $J_y$, it is helpful to consider the partial derivative of one element in $y$ with respect to $x$. Let $a_i^\\top$ represent the $i$'th row in $A$. Thus, $y_i=a_i^\\top x$. Therefore, $\\frac{\\partial y_i}{\\partial x}$, which is the $i$'th row in $J_y$ will be $a_i^\\top$. In other words, the rows of $A$ become the rows of $J_y$. Therefore, $J_y=A$. Plugging these results into our expression of chain rule, we find\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial x} &= J_y^\\top\\frac{\\partial f}{\\partial y}\\\\\n",
    "&= A^\\top y\\\\\n",
    "&= A^\\top Ax.\n",
    "\\end{align*}\n",
    "**It is important to note that the Jacobian is defined as being $m\\times n$ for functions mapping from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ like in the above example. However, we had to consider transposing the Jacobian and left-multiplying to make sure chain rule was carried out correctly like in the $J_y^\\top y$ equations above. Notice how the matrix-vector multiplication accumulates the partial derivatives of each $x_i$ from each $y_j$**\n",
    "\n",
    "**Option 2**\n",
    "\n",
    "The alternative we may examine involves expanding the L2 norm and using the product rule of differentation.\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x) &= \\frac{1}{2}\\lVert Ax\\rVert_2^2\\\\\n",
    "&= \\frac{1}{2}(Ax)^\\top(Ax)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "Computing the partial derivative:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial x} &= \\frac{1}{2}\\left(\\frac{\\partial (Ax)^\\top}{\\partial x}Ax+(Ax)^\\top\\frac{\\partial (Ax)}{\\partial x}\\right)\\\\\n",
    "&= \\frac{1}{2}\\left(\\left(\\frac{\\partial Ax}{\\partial x}\\right)^\\top Ax+\\left(\\frac{\\partial Ax}{\\partial x}\\right)^\\top Ax\\right)\\\\\n",
    "&= \\frac{1}{2}\\left(\\frac{\\partial (Ax)^\\top}{\\partial x}Ax+\\left(\\frac{\\partial (Ax)^\\top}{\\partial x}\\right) Ax\\right)\\\\\n",
    "&= \\frac{\\partial (Ax)^\\top}{\\partial x}Ax\\\\\n",
    "&= A^\\top Ax.\n",
    "\\end{align*}\n",
    "$$\n",
    "Above, note that we re-distribute the transpose in the second term in the second line so that we may combine like terms, i.e. $(CD)^\\top=D^\\top C^\\top$ where we have $C=(Ax)^\\top$ and $D=\\frac{\\partial Ax}{\\partial x}$. In the last line, we borrow our result from Option 1 where $\\frac{\\partial Ax}{\\partial x}=A$ and thus $\\frac{\\partial (Ax)^\\top}{\\partial x}=A^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad528bb-4dbd-4795-b51a-87aeaae6b680",
   "metadata": {},
   "source": [
    "# Homework 3 Content\n",
    "## Gradient descent, computational graphs, backpropagation\n",
    "* Derivatives and gradients are critical to minimizing or maximizing functions. Function optimization is at the heart of machine learning and data science problems where we seek to minimize a cost function or maximize or a reward function by learning from data.\n",
    "* Most functions and real-world problems cannot be optimized in closed-form by solving for where the derivative is equal to zero. This motivates the use of gradient-based methods like **gradient descent**.\n",
    "* Gradient descent is an iterative algorithm that updates parameters by stepping in the direction of the negative gradient (direction of steepest descent).\n",
    "* The gradient descent equation for function $f(x)$ at iteration $k+1$ is stated as\n",
    "$$\n",
    "x^{(k+1)} = x^{(k)}-\\alpha\\nabla f(x)\n",
    "$$\n",
    "where $\\alpha$ is the **step-size** or **learning rate** to control the size of each update step.\n",
    "* Auto-differentiation engines operate based on **computational graphs** that build up complicated functions from simple mathematical operations using directed acyclic graphs. These graphs give important structure for computation.\n",
    "* Performing gradient descent by hand is intractable for larger or more complicated functions even if we may determine the derivatives using chain rule. Thus, we would like to automate computing gradients. Methods like numeric differentiation or symbolic differentation are possible, however, we focus on auto-differentiation via **backpropagation** as the most scalable method for machine learning.\n",
    "* Backpropagation works in two stages: (1) forward pass and (2) backward pass.\n",
    "    * Forward pass: inputs are passed to the computational graph and intermediate values are stored at each node of computation of the graph\n",
    "    * Backward pass: well-defined gradient functions at each node utilize the values from forward propagation to transmit partial derivatives to each predecessor node. The **adjoint**, partial derivative with respect to each **seed node**, at each node is composed of the adjoints of all successors and the partial derivatives of each successor with respect to the current node:\n",
    " $$\n",
    " \\bar{w}_i = \\sum_{j\\in\\textrm{successors}(i)}\\bar{w}_j\\frac{\\partial w_j}{\\partial w_i}.\n",
    " $$\n",
    " The above equation demonstrates chain rule!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b375d-75d2-46c9-ade8-53b019f20d3a",
   "metadata": {},
   "source": [
    "## Backpropagation practice\n",
    "Consider the following multivariable function\n",
    "$$\n",
    "f(x, y) = 2\\cos^2(xy)+\\ln(\\cos(xy))\n",
    "$$\n",
    "\n",
    "a) Determine each partial derivative $\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial y}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial x} &= -4y\\cos(xy)\\sin(xy)-\\frac{y\\sin(xy)}{\\cos(xy)}\\\\\n",
    "\\frac{\\partial f}{\\partial y} &= -4x\\cos(xy)\\sin(xy)-\\frac{x\\sin(xy)}{\\cos(xy)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The below computational graph depicts $f(x, y)$.\n",
    "\n",
    "<div>\n",
    "<center><img src=\"computational-graph.png\" width=\"800\"/> </center>\n",
    "</div>\n",
    "\n",
    "b) Determine the values of each node $w_i$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    w_1 &= x &w_2&=y\\\\\n",
    "    w_3 &=w_1w_2 &w_4&= \\cos(w_3)\\\\\n",
    "    w_5 &= w_4^2 &w_6&=2w_5\\\\\n",
    "    w_7 &= \\ln(w_4) &w_8&=w_6+w_7\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "c) Determine the partial derivatives for each successor node with respect to its predecessor nodes.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial w_8}{\\partial w_7} &= 1 &\\frac{\\partial w_8}{\\partial w_6} &= 1\\\\\n",
    "    \\frac{\\partial w_7}{\\partial w_4} &= \\frac{1}{w_4} &\\frac{\\partial w_6}{\\partial w_5} &= 2\\\\\n",
    "    \\frac{\\partial w_5}{\\partial w_4} &= 2w_4 &\\frac{\\partial w_4}{\\partial w_3} &= -\\sin(w_3)\\\\\n",
    "    \\frac{\\partial w_3}{\\partial w_2} &= w_1 &\\frac{\\partial w_3}{\\partial w_1} &= w_2\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "d) Compute the adjoints of all nodes. Verify that your expressions for $\\bar{w}_1$ and $\\bar{w}_2$ match your partial derivatives from part (a).\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\bar{w}_8 &= 1 &\\bar{w}_7&=1\\\\\n",
    "    \\bar{w}_6 &= 1 &\\bar{w}_5&=2\\\\\n",
    "    \\bar{w}_4 &= \\frac{1}{w_4} + 4w_4 &\\bar{w}_3&= -\\frac{\\sin(w_3)}{w_4} -4w_4\\sin(w_3)\\\\\n",
    "    \\bar{w}_2 &= -\\frac{w_1\\sin(w_3)}{w_4} -4w_1w_4\\sin(w_3) &\\bar{w}_1 &= -\\frac{w_2\\sin(w_3)}{w_4} -4w_2w_4\\sin(w_3) \n",
    "\\end{align*}\n",
    "$$\n",
    "Recursively plugging in using the expressions from part (b):\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar{w}_1 &= \\frac{\\partial f}{\\partial w_1} = -4y\\cos(xy)\\sin(xy)-\\frac{y\\sin(xy)}{\\cos(xy)} \\\\\n",
    "\\bar{w}_2 &= \\frac{\\partial f}{\\partial w_2} = -4x\\cos(xy)\\sin(xy)-\\frac{x\\sin(xy)}{\\cos(xy)} \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b532751-69db-49e0-85b5-28ec85646c28",
   "metadata": {},
   "source": [
    "# Homework 4 Content\n",
    "## Linear regression: one-dimensional, multi-dimensional, transformations of inputs\n",
    "\n",
    "* Linear regression in one dimension seeks to find a line-of-best-fit for a dataset of $(x, y)$ coordinates.\n",
    "* Consider $\\mathcal{D}=\\{(x_i, y_i)\\}_{i=1}^{N}$. Linear regression minimizes:\n",
    "$$\n",
    "\\min_{w_1,~w_0}~\\frac{1}{2}\\sum_{i=1}^{N}(y_i-w_1x_i-w_0)^2,\n",
    "$$\n",
    "where $w_0$ represents the **bias** term, i.e. y-intercept.\n",
    "* This may also be written in vector form:\n",
    "$$\n",
    "\\min_{w}~\\frac{1}{2}\\lVert \\mathbf{X}^\\top w - y\\rVert_2^2,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{X}^\\top = \\begin{bmatrix}\n",
    "    x_1 & 1\\\\\n",
    "    x_2 & 1\\\\\n",
    "    \\vdots & \\vdots \\\\\n",
    "    x_N & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "* By taking the gradient of the objective functions and setting to zero, we may obtain a closed-form solution for linear regression\n",
    "\n",
    "$$\n",
    "w^*=(\\mathbf{X}\\mathbf{X}^\\top)^{-1}\\mathbf{X}y.\n",
    "$$\n",
    "* We may also have linear regression in higher dimensions, e.g. plane-of-best-fit, hyperplane-of-best-fit.\n",
    "* Alternatively, linear regression may be applied to more complicated regression problems where transformations are applied to input variables, e.g. polynomial regression like third-order polynomial regression below.\n",
    "$$\n",
    "\\min_{w_3,~w_2,~w_1,~w_0}~\\frac{1}{2}\\sum_{i=1}^{N}(y_i-w_3x_i^3-w_2x_i^2-w_1x_i-w_0)^2\n",
    "$$\n",
    "* Where a transformation $\\Phi(\\mathbf{X})$ is applied to input data for linear regression, we may now describe the closed-form solution as\n",
    "$$\n",
    "w^*=(\\mathbf{\\Phi}\\mathbf{\\Phi}^\\top)^{-1}\\mathbf{\\Phi}y.\n",
    "$$\n",
    "* What happens if $\\mathbf{X}^\\top\\in\\mathbb{R}^{N\\times d}$ has $d>N$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befdbe9-8cf0-4857-a7a1-de07dc0c29fd",
   "metadata": {},
   "source": [
    "## Linear regression practice\n",
    "a) Determine the $\\mathbf{\\Phi}$ matrix for the above third-order polynomial regression problem.\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Phi}^\\top = \\begin{bmatrix}\n",
    "x_1^3 & x_1^2 & x_1 & 1\\\\\n",
    "x_2^3 & x_2^2 & x_2 & 1\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "x_N^3 & x_N^2 & x_N & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "b) What is the minimum number of data points $N$ such that we may obtain a unique solution according to the closed-form solution?\n",
    "\n",
    "We have four unknowns to solve for in the third-order polynomial regression problem (don't forget this includes the bias term!). Thus, we need $N\\geq 4$ dat apoints to obtain a unique closed-form solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
